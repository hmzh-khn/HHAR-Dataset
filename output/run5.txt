TRAINING INFO - is_lstm? True
-------------
actions ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']
devices ['samsungold_1', 'samsungold_2']
step size 1
num_epochs 1000
batch size 20
feature size (x,y,z) 3
hidden size 20
number recurrent layers 2
number of classes 6
writing pickle to run5_model.p
writing accuracy + pickle to acc_run5_model.p
------------------------------------------------
0	 steps: training loss - 1.84406	, testing loss - 1.79536	
1	 steps: training loss - 1.81611	, testing loss - 1.79538	
2	 steps: training loss - 1.85596	, testing loss - 1.79497	
3	 steps: training loss - 1.82336	, testing loss - 1.79439	
4	 steps: training loss - 1.85335	, testing loss - 1.79373	
5	 steps: training loss - 1.80909	, testing loss - 1.79306	
6	 steps: training loss - 1.82284	, testing loss - 1.79253	
7	 steps: training loss - 1.82091	, testing loss - 1.79213	
8	 steps: training loss - 1.83225	, testing loss - 1.79172	
9	 steps: training loss - 1.89902	, testing loss - 1.79116	
10	 steps: training loss - 1.83105	, testing loss - 1.79058	
11	 steps: training loss - 1.83587	, testing loss - 1.78991	
12	 steps: training loss - 1.82793	, testing loss - 1.78932	
13	 steps: training loss - 1.81424	, testing loss - 1.78884	
14	 steps: training loss - 1.82015	, testing loss - 1.78826	
15	 steps: training loss - 1.82380	, testing loss - 1.78774	
16	 steps: training loss - 1.85308	, testing loss - 1.78718	
17	 steps: training loss - 1.87072	, testing loss - 1.78671	
18	 steps: training loss - 1.82353	, testing loss - 1.78638	
19	 steps: training loss - 1.84515	, testing loss - 1.78613	
20	 steps: training loss - 1.78247	, testing loss - 1.78593	
21	 steps: training loss - 1.81039	, testing loss - 1.78571	
22	 steps: training loss - 1.82398	, testing loss - 1.78551	
23	 steps: training loss - 1.81530	, testing loss - 1.78538	
24	 steps: training loss - 1.81180	, testing loss - 1.78529	
25	 steps: training loss - 1.85371	, testing loss - 1.78526	
26	 steps: training loss - 1.78122	, testing loss - 1.78525	
27	 steps: training loss - 1.85089	, testing loss - 1.78518	
28	 steps: training loss - 1.79176	, testing loss - 1.78516	
29	 steps: training loss - 1.81765	, testing loss - 1.78515	
30	 steps: training loss - 1.80541	, testing loss - 1.78529	
31	 steps: training loss - 1.81035	, testing loss - 1.78547	
32	 steps: training loss - 1.82412	, testing loss - 1.78572	
33	 steps: training loss - 1.81917	, testing loss - 1.78605	
34	 steps: training loss - 1.80331	, testing loss - 1.78635	
35	 steps: training loss - 1.80191	, testing loss - 1.78664	
36	 steps: training loss - 1.78825	, testing loss - 1.78684	
37	 steps: training loss - 1.81084	, testing loss - 1.78714	
38	 steps: training loss - 1.82808	, testing loss - 1.78723	
39	 steps: training loss - 1.82110	, testing loss - 1.78716	
40	 steps: training loss - 1.79204	, testing loss - 1.78701	
41	 steps: training loss - 1.81373	, testing loss - 1.78693	
42	 steps: training loss - 1.79951	, testing loss - 1.78683	
43	 steps: training loss - 1.78727	, testing loss - 1.78674	
44	 steps: training loss - 1.78686	, testing loss - 1.78681	
45	 steps: training loss - 1.83627	, testing loss - 1.78693	
46	 steps: training loss - 1.78964	, testing loss - 1.78687	
47	 steps: training loss - 1.80961	, testing loss - 1.78673	
48	 steps: training loss - 1.79408	, testing loss - 1.78665	
49	 steps: training loss - 1.79781	, testing loss - 1.78657	
50	 steps: training loss - 1.78388	, testing loss - 1.78653	
51	 steps: training loss - 1.80046	, testing loss - 1.78654	
52	 steps: training loss - 1.77318	, testing loss - 1.78640	
53	 steps: training loss - 1.78917	, testing loss - 1.78639	
54	 steps: training loss - 1.78809	, testing loss - 1.78654	
55	 steps: training loss - 1.77347	, testing loss - 1.78678	
56	 steps: training loss - 1.78444	, testing loss - 1.78698	
57	 steps: training loss - 1.80081	, testing loss - 1.78720	
58	 steps: training loss - 1.78934	, testing loss - 1.78738	
59	 steps: training loss - 1.78102	, testing loss - 1.78753	
60	 steps: training loss - 1.77760	, testing loss - 1.78767	
61	 steps: training loss - 1.78849	, testing loss - 1.78780	
62	 steps: training loss - 1.77348	, testing loss - 1.78795	
63	 steps: training loss - 1.77614	, testing loss - 1.78818	
64	 steps: training loss - 1.76606	, testing loss - 1.78844	
65	 steps: training loss - 1.77939	, testing loss - 1.78872	
66	 steps: training loss - 1.75541	, testing loss - 1.78901	
67	 steps: training loss - 1.79028	, testing loss - 1.78936	
68	 steps: training loss - 1.79829	, testing loss - 1.78977	
69	 steps: training loss - 1.74634	, testing loss - 1.79005	
70	 steps: training loss - 1.77200	, testing loss - 1.79043	
71	 steps: training loss - 1.77866	, testing loss - 1.79077	
72	 steps: training loss - 1.76323	, testing loss - 1.79103	
73	 steps: training loss - 1.75898	, testing loss - 1.79122	
74	 steps: training loss - 1.76983	, testing loss - 1.79151	
75	 steps: training loss - 1.76035	, testing loss - 1.79195	
76	 steps: training loss - 1.77227	, testing loss - 1.79239	
77	 steps: training loss - 1.79229	, testing loss - 1.79286	
78	 steps: training loss - 1.76555	, testing loss - 1.79312	
79	 steps: training loss - 1.79625	, testing loss - 1.79340	
80	 steps: training loss - 1.78148	, testing loss - 1.79358	
81	 steps: training loss - 1.74884	, testing loss - 1.79375	
82	 steps: training loss - 1.76314	, testing loss - 1.79394	
83	 steps: training loss - 1.74788	, testing loss - 1.79422	
84	 steps: training loss - 1.76325	, testing loss - 1.79463	
85	 steps: training loss - 1.76181	, testing loss - 1.79510	
86	 steps: training loss - 1.74323	, testing loss - 1.79564	
87	 steps: training loss - 1.75718	, testing loss - 1.79621	
88	 steps: training loss - 1.76594	, testing loss - 1.79684	
89	 steps: training loss - 1.74645	, testing loss - 1.79738	
90	 steps: training loss - 1.74488	, testing loss - 1.79791	
91	 steps: training loss - 1.74127	, testing loss - 1.79852	
92	 steps: training loss - 1.74751	, testing loss - 1.79922	
93	 steps: training loss - 1.77487	, testing loss - 1.79988	
94	 steps: training loss - 1.76839	, testing loss - 1.80036	
95	 steps: training loss - 1.76864	, testing loss - 1.80059	
96	 steps: training loss - 1.76517	, testing loss - 1.80069	
97	 steps: training loss - 1.80367	, testing loss - 1.80083	
98	 steps: training loss - 1.75265	, testing loss - 1.80072	
99	 steps: training loss - 1.75256	, testing loss - 1.80058	
100	 steps: training loss - 1.77117	, testing loss - 1.80054	
101	 steps: training loss - 1.76879	, testing loss - 1.80045	
102	 steps: training loss - 1.77772	, testing loss - 1.80029	
103	 steps: training loss - 1.73635	, testing loss - 1.80004	
104	 steps: training loss - 1.76975	, testing loss - 1.79988	
105	 steps: training loss - 1.78587	, testing loss - 1.79971	
106	 steps: training loss - 1.77336	, testing loss - 1.79938	
107	 steps: training loss - 1.77436	, testing loss - 1.79890	
108	 steps: training loss - 1.70784	, testing loss - 1.79838	
109	 steps: training loss - 1.78455	, testing loss - 1.79817	
110	 steps: training loss - 1.76177	, testing loss - 1.79794	
111	 steps: training loss - 1.77458	, testing loss - 1.79776	
112	 steps: training loss - 1.75994	, testing loss - 1.79760	
113	 steps: training loss - 1.77804	, testing loss - 1.79745	
114	 steps: training loss - 1.73157	, testing loss - 1.79726	
115	 steps: training loss - 1.76517	, testing loss - 1.79725	
116	 steps: training loss - 1.73369	, testing loss - 1.79729	
117	 steps: training loss - 1.73831	, testing loss - 1.79749	
118	 steps: training loss - 1.75653	, testing loss - 1.79781	
119	 steps: training loss - 1.73600	, testing loss - 1.79804	
120	 steps: training loss - 1.75281	, testing loss - 1.79828	
121	 steps: training loss - 1.75256	, testing loss - 1.79845	
122	 steps: training loss - 1.72954	, testing loss - 1.79855	
123	 steps: training loss - 1.78146	, testing loss - 1.79872	
124	 steps: training loss - 1.77485	, testing loss - 1.79873	
125	 steps: training loss - 1.72061	, testing loss - 1.79868	
126	 steps: training loss - 1.75820	, testing loss - 1.79884	
127	 steps: training loss - 1.70667	, testing loss - 1.79903	
128	 steps: training loss - 1.78753	, testing loss - 1.79935	
129	 steps: training loss - 1.75056	, testing loss - 1.79944	
130	 steps: training loss - 1.75896	, testing loss - 1.79949	
131	 steps: training loss - 1.75894	, testing loss - 1.79944	
132	 steps: training loss - 1.72639	, testing loss - 1.79929	
133	 steps: training loss - 1.75245	, testing loss - 1.79918	
134	 steps: training loss - 1.73449	, testing loss - 1.79912	
135	 steps: training loss - 1.69792	, testing loss - 1.79913	
136	 steps: training loss - 1.76316	, testing loss - 1.79936	
137	 steps: training loss - 1.72553	, testing loss - 1.79951	
138	 steps: training loss - 1.75251	, testing loss - 1.79969	
139	 steps: training loss - 1.73778	, testing loss - 1.79990	
140	 steps: training loss - 1.73046	, testing loss - 1.80018	
141	 steps: training loss - 1.75315	, testing loss - 1.80054	
142	 steps: training loss - 1.71297	, testing loss - 1.80096	
143	 steps: training loss - 1.71936	, testing loss - 1.80157	
144	 steps: training loss - 1.73191	, testing loss - 1.80223	
145	 steps: training loss - 1.78283	, testing loss - 1.80287	
146	 steps: training loss - 1.74780	, testing loss - 1.80334	
147	 steps: training loss - 1.74750	, testing loss - 1.80366	
148	 steps: training loss - 1.75038	, testing loss - 1.80394	
149	 steps: training loss - 1.69881	, testing loss - 1.80417	
150	 steps: training loss - 1.79122	, testing loss - 1.80453	
151	 steps: training loss - 1.76731	, testing loss - 1.80467	
152	 steps: training loss - 1.76475	, testing loss - 1.80449	
153	 steps: training loss - 1.71176	, testing loss - 1.80416	
154	 steps: training loss - 1.75256	, testing loss - 1.80395	
155	 steps: training loss - 1.72726	, testing loss - 1.80376	
156	 steps: training loss - 1.76723	, testing loss - 1.80359	
157	 steps: training loss - 1.74724	, testing loss - 1.80341	
158	 steps: training loss - 1.77435	, testing loss - 1.80324	
159	 steps: training loss - 1.74218	, testing loss - 1.80299	
160	 steps: training loss - 1.72918	, testing loss - 1.80265	
161	 steps: training loss - 1.74877	, testing loss - 1.80241	
162	 steps: training loss - 1.73557	, testing loss - 1.80221	
163	 steps: training loss - 1.71859	, testing loss - 1.80204	
164	 steps: training loss - 1.73534	, testing loss - 1.80192	
165	 steps: training loss - 1.73394	, testing loss - 1.80182	
166	 steps: training loss - 1.67617	, testing loss - 1.80181	
167	 steps: training loss - 1.72426	, testing loss - 1.80202	
168	 steps: training loss - 1.74787	, testing loss - 1.80222	
169	 steps: training loss - 1.72778	, testing loss - 1.80227	
170	 steps: training loss - 1.71795	, testing loss - 1.80229	
171	 steps: training loss - 1.75743	, testing loss - 1.80239	
172	 steps: training loss - 1.74952	, testing loss - 1.80248	
173	 steps: training loss - 1.78209	, testing loss - 1.80256	
174	 steps: training loss - 1.67594	, testing loss - 1.80232	
175	 steps: training loss - 1.71695	, testing loss - 1.80232	
176	 steps: training loss - 1.70664	, testing loss - 1.80241	
177	 steps: training loss - 1.73294	, testing loss - 1.80255	
178	 steps: training loss - 1.80219	, testing loss - 1.80281	
179	 steps: training loss - 1.71895	, testing loss - 1.80280	
180	 steps: training loss - 1.71701	, testing loss - 1.80277	
181	 steps: training loss - 1.71571	, testing loss - 1.80282	
182	 steps: training loss - 1.73388	, testing loss - 1.80293	
183	 steps: training loss - 1.69431	, testing loss - 1.80311	
184	 steps: training loss - 1.75701	, testing loss - 1.80348	
185	 steps: training loss - 1.73207	, testing loss - 1.80370	
186	 steps: training loss - 1.73504	, testing loss - 1.80389	
187	 steps: training loss - 1.72118	, testing loss - 1.80396	
188	 steps: training loss - 1.74114	, testing loss - 1.80394	
189	 steps: training loss - 1.68036	, testing loss - 1.80385	
190	 steps: training loss - 1.74063	, testing loss - 1.80390	
191	 steps: training loss - 1.71484	, testing loss - 1.80403	
192	 steps: training loss - 1.74706	, testing loss - 1.80424	
193	 steps: training loss - 1.73352	, testing loss - 1.80435	
194	 steps: training loss - 1.70818	, testing loss - 1.80442	
195	 steps: training loss - 1.76900	, testing loss - 1.80450	
196	 steps: training loss - 1.79592	, testing loss - 1.80437	
197	 steps: training loss - 1.73475	, testing loss - 1.80398	
198	 steps: training loss - 1.68980	, testing loss - 1.80365	
199	 steps: training loss - 1.71868	, testing loss - 1.80348	
200	 steps: training loss - 1.71802	, testing loss - 1.80341	
201	 steps: training loss - 1.68623	, testing loss - 1.80332	
202	 steps: training loss - 1.78646	, testing loss - 1.80332	
203	 steps: training loss - 1.73760	, testing loss - 1.80309	
204	 steps: training loss - 1.72626	, testing loss - 1.80287	
205	 steps: training loss - 1.74795	, testing loss - 1.80258	
206	 steps: training loss - 1.70136	, testing loss - 1.80223	
207	 steps: training loss - 1.74464	, testing loss - 1.80208	
208	 steps: training loss - 1.73515	, testing loss - 1.80188	
209	 steps: training loss - 1.80729	, testing loss - 1.80166	
210	 steps: training loss - 1.77154	, testing loss - 1.80121	
211	 steps: training loss - 1.74469	, testing loss - 1.80061	
212	 steps: training loss - 1.72247	, testing loss - 1.80001	
213	 steps: training loss - 1.69082	, testing loss - 1.79963	
214	 steps: training loss - 1.71896	, testing loss - 1.79958	
215	 steps: training loss - 1.73238	, testing loss - 1.79957	
216	 steps: training loss - 1.79259	, testing loss - 1.79959	
217	 steps: training loss - 1.72743	, testing loss - 1.79939	
218	 steps: training loss - 1.74200	, testing loss - 1.79919	
219	 steps: training loss - 1.73645	, testing loss - 1.79891	
220	 steps: training loss - 1.71196	, testing loss - 1.79856	
221	 steps: training loss - 1.73736	, testing loss - 1.79830	
222	 steps: training loss - 1.72374	, testing loss - 1.79803	
223	 steps: training loss - 1.75386	, testing loss - 1.79782	
224	 steps: training loss - 1.70788	, testing loss - 1.79751	
225	 steps: training loss - 1.79275	, testing loss - 1.79738	
226	 steps: training loss - 1.72042	, testing loss - 1.79707	
227	 steps: training loss - 1.73933	, testing loss - 1.79679	
228	 steps: training loss - 1.78168	, testing loss - 1.79653	
229	 steps: training loss - 1.74450	, testing loss - 1.79598	
230	 steps: training loss - 1.79122	, testing loss - 1.79540	
231	 steps: training loss - 1.73117	, testing loss - 1.79468	
232	 steps: training loss - 1.72384	, testing loss - 1.79388	
233	 steps: training loss - 1.69643	, testing loss - 1.79323	
234	 steps: training loss - 1.73437	, testing loss - 1.79285	
235	 steps: training loss - 1.72235	, testing loss - 1.79263	
236	 steps: training loss - 1.72784	, testing loss - 1.79251	
237	 steps: training loss - 1.68602	, testing loss - 1.79241	
238	 steps: training loss - 1.71694	, testing loss - 1.79237	
239	 steps: training loss - 1.71084	, testing loss - 1.79244	
240	 steps: training loss - 1.72034	, testing loss - 1.79265	
241	 steps: training loss - 1.75376	, testing loss - 1.79282	
242	 steps: training loss - 1.74753	, testing loss - 1.79284	
243	 steps: training loss - 1.75064	, testing loss - 1.79262	
244	 steps: training loss - 1.70623	, testing loss - 1.79224	
245	 steps: training loss - 1.73045	, testing loss - 1.79201	
246	 steps: training loss - 1.75812	, testing loss - 1.79177	
247	 steps: training loss - 1.72237	, testing loss - 1.79148	
248	 steps: training loss - 1.77124	, testing loss - 1.79115	
249	 steps: training loss - 1.73825	, testing loss - 1.79066	
250	 steps: training loss - 1.71494	, testing loss - 1.79011	
251	 steps: training loss - 1.70443	, testing loss - 1.78965	
252	 steps: training loss - 1.66696	, testing loss - 1.78933	
253	 steps: training loss - 1.73054	, testing loss - 1.78925	
254	 steps: training loss - 1.66262	, testing loss - 1.78910	
255	 steps: training loss - 1.72121	, testing loss - 1.78906	
256	 steps: training loss - 1.75667	, testing loss - 1.78905	
257	 steps: training loss - 1.75192	, testing loss - 1.78891	
258	 steps: training loss - 1.68857	, testing loss - 1.78860	
259	 steps: training loss - 1.72044	, testing loss - 1.78840	
260	 steps: training loss - 1.69011	, testing loss - 1.78827	
261	 steps: training loss - 1.67011	, testing loss - 1.78826	
262	 steps: training loss - 1.72803	, testing loss - 1.78850	
263	 steps: training loss - 1.72758	, testing loss - 1.78882	
264	 steps: training loss - 1.72644	, testing loss - 1.78905	
265	 steps: training loss - 1.76038	, testing loss - 1.78918	
266	 steps: training loss - 1.70188	, testing loss - 1.78912	
267	 steps: training loss - 1.75268	, testing loss - 1.78917	
268	 steps: training loss - 1.68332	, testing loss - 1.78919	
269	 steps: training loss - 1.75421	, testing loss - 1.78932	
270	 steps: training loss - 1.74880	, testing loss - 1.78944	
271	 steps: training loss - 1.68973	, testing loss - 1.78922	
272	 steps: training loss - 1.68362	, testing loss - 1.78898	
273	 steps: training loss - 1.66168	, testing loss - 1.78878	
274	 steps: training loss - 1.78047	, testing loss - 1.78876	
275	 steps: training loss - 1.69981	, testing loss - 1.78842	
276	 steps: training loss - 1.78492	, testing loss - 1.78814	
277	 steps: training loss - 1.71036	, testing loss - 1.78764	
278	 steps: training loss - 1.72936	, testing loss - 1.78718	
279	 steps: training loss - 1.77171	, testing loss - 1.78676	
280	 steps: training loss - 1.71250	, testing loss - 1.78619	
281	 steps: training loss - 1.76967	, testing loss - 1.78568	
282	 steps: training loss - 1.72521	, testing loss - 1.78516	
283	 steps: training loss - 1.73871	, testing loss - 1.78463	
284	 steps: training loss - 1.79001	, testing loss - 1.78424	
285	 steps: training loss - 1.77425	, testing loss - 1.78371	
286	 steps: training loss - 1.73249	, testing loss - 1.78299	
287	 steps: training loss - 1.73271	, testing loss - 1.78239	
288	 steps: training loss - 1.72672	, testing loss - 1.78187	
289	 steps: training loss - 1.75998	, testing loss - 1.78134	
290	 steps: training loss - 1.74804	, testing loss - 1.78069	
291	 steps: training loss - 1.72516	, testing loss - 1.77996	
292	 steps: training loss - 1.73426	, testing loss - 1.77927	
293	 steps: training loss - 1.66619	, testing loss - 1.77872	
294	 steps: training loss - 1.71394	, testing loss - 1.77850	
295	 steps: training loss - 1.74538	, testing loss - 1.77847	
296	 steps: training loss - 1.71603	, testing loss - 1.77839	
297	 steps: training loss - 1.74296	, testing loss - 1.77815	
298	 steps: training loss - 1.71191	, testing loss - 1.77784	
299	 steps: training loss - 1.73805	, testing loss - 1.77749	
300	 steps: training loss - 1.68310	, testing loss - 1.77705	
301	 steps: training loss - 1.75043	, testing loss - 1.77673	
302	 steps: training loss - 1.75809	, testing loss - 1.77629	
303	 steps: training loss - 1.74755	, testing loss - 1.77575	
304	 steps: training loss - 1.70279	, testing loss - 1.77523	
305	 steps: training loss - 1.68892	, testing loss - 1.77473	
306	 steps: training loss - 1.74063	, testing loss - 1.77441	
307	 steps: training loss - 1.76961	, testing loss - 1.77407	
308	 steps: training loss - 1.67378	, testing loss - 1.77366	
309	 steps: training loss - 1.70039	, testing loss - 1.77343	
310	 steps: training loss - 1.72995	, testing loss - 1.77333	
311	 steps: training loss - 1.71215	, testing loss - 1.77325	
312	 steps: training loss - 1.67443	, testing loss - 1.77322	
313	 steps: training loss - 1.70079	, testing loss - 1.77335	
314	 steps: training loss - 1.73291	, testing loss - 1.77350	
315	 steps: training loss - 1.74932	, testing loss - 1.77355	
316	 steps: training loss - 1.73908	, testing loss - 1.77345	
317	 steps: training loss - 1.71612	, testing loss - 1.77333	
318	 steps: training loss - 1.68554	, testing loss - 1.77327	
319	 steps: training loss - 1.69691	, testing loss - 1.77343	
320	 steps: training loss - 1.69786	, testing loss - 1.77363	
321	 steps: training loss - 1.72259	, testing loss - 1.77382	
322	 steps: training loss - 1.70189	, testing loss - 1.77382	
323	 steps: training loss - 1.72955	, testing loss - 1.77370	
324	 steps: training loss - 1.75401	, testing loss - 1.77351	
325	 steps: training loss - 1.68977	, testing loss - 1.77300	
326	 steps: training loss - 1.75422	, testing loss - 1.77261	
327	 steps: training loss - 1.68664	, testing loss - 1.77221	
328	 steps: training loss - 1.73968	, testing loss - 1.77193	
329	 steps: training loss - 1.64753	, testing loss - 1.77149	
330	 steps: training loss - 1.68914	, testing loss - 1.77124	
331	 steps: training loss - 1.79256	, testing loss - 1.77118	
332	 steps: training loss - 1.71991	, testing loss - 1.77100	
333	 steps: training loss - 1.72122	, testing loss - 1.77069	
334	 steps: training loss - 1.70765	, testing loss - 1.77041	
335	 steps: training loss - 1.70807	, testing loss - 1.77012	
336	 steps: training loss - 1.73684	, testing loss - 1.76990	
337	 steps: training loss - 1.66959	, testing loss - 1.76977	
338	 steps: training loss - 1.72487	, testing loss - 1.76970	
339	 steps: training loss - 1.70009	, testing loss - 1.76948	
340	 steps: training loss - 1.67830	, testing loss - 1.76928	
341	 steps: training loss - 1.67266	, testing loss - 1.76931	
342	 steps: training loss - 1.75795	, testing loss - 1.76952	
343	 steps: training loss - 1.69431	, testing loss - 1.76938	
344	 steps: training loss - 1.72146	, testing loss - 1.76916	
345	 steps: training loss - 1.76779	, testing loss - 1.76893	
346	 steps: training loss - 1.65317	, testing loss - 1.76858	
347	 steps: training loss - 1.75948	, testing loss - 1.76848	
348	 steps: training loss - 1.75988	, testing loss - 1.76832	
349	 steps: training loss - 1.69259	, testing loss - 1.76801	
350	 steps: training loss - 1.71965	, testing loss - 1.76779	
351	 steps: training loss - 1.68725	, testing loss - 1.76740	
352	 steps: training loss - 1.64850	, testing loss - 1.76716	
353	 steps: training loss - 1.66750	, testing loss - 1.76724	
354	 steps: training loss - 1.68415	, testing loss - 1.76756	
355	 steps: training loss - 1.75463	, testing loss - 1.76797	
356	 steps: training loss - 1.68201	, testing loss - 1.76829	
357	 steps: training loss - 1.69194	, testing loss - 1.76855	
358	 steps: training loss - 1.71387	, testing loss - 1.76876	
359	 steps: training loss - 1.73437	, testing loss - 1.76893	
360	 steps: training loss - 1.68781	, testing loss - 1.76880	
361	 steps: training loss - 1.63436	, testing loss - 1.76859	
362	 steps: training loss - 1.71623	, testing loss - 1.76867	
363	 steps: training loss - 1.67232	, testing loss - 1.76879	
364	 steps: training loss - 1.71880	, testing loss - 1.76902	
365	 steps: training loss - 1.71422	, testing loss - 1.76914	
366	 steps: training loss - 1.68348	, testing loss - 1.76924	
367	 steps: training loss - 1.70315	, testing loss - 1.76945	
368	 steps: training loss - 1.74838	, testing loss - 1.76974	
369	 steps: training loss - 1.63870	, testing loss - 1.76978	
370	 steps: training loss - 1.72204	, testing loss - 1.77004	
371	 steps: training loss - 1.70447	, testing loss - 1.77016	
372	 steps: training loss - 1.67772	, testing loss - 1.77004	
373	 steps: training loss - 1.73549	, testing loss - 1.77003	
374	 steps: training loss - 1.66789	, testing loss - 1.76990	
375	 steps: training loss - 1.63015	, testing loss - 1.76989	
376	 steps: training loss - 1.69652	, testing loss - 1.77023	
377	 steps: training loss - 1.65054	, testing loss - 1.77063	
378	 steps: training loss - 1.74066	, testing loss - 1.77111	
379	 steps: training loss - 1.66294	, testing loss - 1.77139	
380	 steps: training loss - 1.66196	, testing loss - 1.77176	
381	 steps: training loss - 1.69527	, testing loss - 1.77210	
382	 steps: training loss - 1.68949	, testing loss - 1.77243	
383	 steps: training loss - 1.69428	, testing loss - 1.77270	
384	 steps: training loss - 1.71804	, testing loss - 1.77284	
385	 steps: training loss - 1.75243	, testing loss - 1.77270	
386	 steps: training loss - 1.71529	, testing loss - 1.77253	
387	 steps: training loss - 1.74651	, testing loss - 1.77241	
388	 steps: training loss - 1.67315	, testing loss - 1.77228	
389	 steps: training loss - 1.75663	, testing loss - 1.77220	
390	 steps: training loss - 1.67225	, testing loss - 1.77199	
391	 steps: training loss - 1.68338	, testing loss - 1.77173	
392	 steps: training loss - 1.69252	, testing loss - 1.77153	
393	 steps: training loss - 1.68971	, testing loss - 1.77122	
394	 steps: training loss - 1.67993	, testing loss - 1.77099	
395	 steps: training loss - 1.74412	, testing loss - 1.77086	
396	 steps: training loss - 1.73140	, testing loss - 1.77060	
397	 steps: training loss - 1.67896	, testing loss - 1.77018	
398	 steps: training loss - 1.63983	, testing loss - 1.76990	
399	 steps: training loss - 1.59935	, testing loss - 1.76995	
400	 steps: training loss - 1.71230	, testing loss - 1.77025	
401	 steps: training loss - 1.65426	, testing loss - 1.77048	
402	 steps: training loss - 1.67813	, testing loss - 1.77072	
403	 steps: training loss - 1.64447	, testing loss - 1.77113	
404	 steps: training loss - 1.70336	, testing loss - 1.77183	
405	 steps: training loss - 1.71109	, testing loss - 1.77225	
406	 steps: training loss - 1.72744	, testing loss - 1.77236	
407	 steps: training loss - 1.70770	, testing loss - 1.77232	
408	 steps: training loss - 1.76856	, testing loss - 1.77225	
409	 steps: training loss - 1.68451	, testing loss - 1.77202	
410	 steps: training loss - 1.65960	, testing loss - 1.77179	
411	 steps: training loss - 1.69722	, testing loss - 1.77179	
412	 steps: training loss - 1.62000	, testing loss - 1.77175	
413	 steps: training loss - 1.69466	, testing loss - 1.77185	
414	 steps: training loss - 1.63479	, testing loss - 1.77192	
415	 steps: training loss - 1.71866	, testing loss - 1.77212	
416	 steps: training loss - 1.72803	, testing loss - 1.77233	
417	 steps: training loss - 1.62967	, testing loss - 1.77248	
418	 steps: training loss - 1.70766	, testing loss - 1.77282	
419	 steps: training loss - 1.68009	, testing loss - 1.77303	
420	 steps: training loss - 1.72203	, testing loss - 1.77313	
421	 steps: training loss - 1.70594	, testing loss - 1.77313	
422	 steps: training loss - 1.66863	, testing loss - 1.77319	
423	 steps: training loss - 1.68837	, testing loss - 1.77336	
424	 steps: training loss - 1.74361	, testing loss - 1.77353	
425	 steps: training loss - 1.69250	, testing loss - 1.77347	
426	 steps: training loss - 1.70586	, testing loss - 1.77343	
427	 steps: training loss - 1.73481	, testing loss - 1.77332	
428	 steps: training loss - 1.63279	, testing loss - 1.77308	
429	 steps: training loss - 1.64821	, testing loss - 1.77309	
430	 steps: training loss - 1.72540	, testing loss - 1.77323	
431	 steps: training loss - 1.74299	, testing loss - 1.77329	
432	 steps: training loss - 1.71013	, testing loss - 1.77311	
433	 steps: training loss - 1.67877	, testing loss - 1.77287	
434	 steps: training loss - 1.69513	, testing loss - 1.77275	
435	 steps: training loss - 1.77167	, testing loss - 1.77275	
436	 steps: training loss - 1.75938	, testing loss - 1.77247	
437	 steps: training loss - 1.70587	, testing loss - 1.77193	
438	 steps: training loss - 1.63132	, testing loss - 1.77145	
439	 steps: training loss - 1.70210	, testing loss - 1.77125	
440	 steps: training loss - 1.71462	, testing loss - 1.77096	
441	 steps: training loss - 1.69827	, testing loss - 1.77043	
442	 steps: training loss - 1.71779	, testing loss - 1.77004	
443	 steps: training loss - 1.66088	, testing loss - 1.76950	
444	 steps: training loss - 1.65188	, testing loss - 1.76897	
445	 steps: training loss - 1.67781	, testing loss - 1.76879	
446	 steps: training loss - 1.66145	, testing loss - 1.76869	
447	 steps: training loss - 1.67587	, testing loss - 1.76848	
448	 steps: training loss - 1.64343	, testing loss - 1.76830	
449	 steps: training loss - 1.69672	, testing loss - 1.76808	
450	 steps: training loss - 1.69889	, testing loss - 1.76785	
451	 steps: training loss - 1.70956	, testing loss - 1.76747	
452	 steps: training loss - 1.72122	, testing loss - 1.76696	
453	 steps: training loss - 1.72220	, testing loss - 1.76643	
454	 steps: training loss - 1.63225	, testing loss - 1.76587	
455	 steps: training loss - 1.71263	, testing loss - 1.76558	
456	 steps: training loss - 1.69560	, testing loss - 1.76527	
457	 steps: training loss - 1.62398	, testing loss - 1.76482	
458	 steps: training loss - 1.72992	, testing loss - 1.76457	
459	 steps: training loss - 1.69365	, testing loss - 1.76442	
460	 steps: training loss - 1.66960	, testing loss - 1.76416	
461	 steps: training loss - 1.69982	, testing loss - 1.76393	
462	 steps: training loss - 1.74468	, testing loss - 1.76376	
463	 steps: training loss - 1.69077	, testing loss - 1.76335	
464	 steps: training loss - 1.69294	, testing loss - 1.76278	
465	 steps: training loss - 1.70863	, testing loss - 1.76235	
466	 steps: training loss - 1.69729	, testing loss - 1.76200	
467	 steps: training loss - 1.79227	, testing loss - 1.76180	
468	 steps: training loss - 1.66026	, testing loss - 1.76133	
469	 steps: training loss - 1.68220	, testing loss - 1.76089	
470	 steps: training loss - 1.64349	, testing loss - 1.76058	
471	 steps: training loss - 1.69949	, testing loss - 1.76042	
472	 steps: training loss - 1.64429	, testing loss - 1.76024	
473	 steps: training loss - 1.67421	, testing loss - 1.76001	
474	 steps: training loss - 1.68525	, testing loss - 1.75990	
475	 steps: training loss - 1.69610	, testing loss - 1.75987	
476	 steps: training loss - 1.69826	, testing loss - 1.75986	
477	 steps: training loss - 1.71860	, testing loss - 1.75991	
478	 steps: training loss - 1.71100	, testing loss - 1.75991	
479	 steps: training loss - 1.71343	, testing loss - 1.75966	
480	 steps: training loss - 1.74263	, testing loss - 1.75944	
481	 steps: training loss - 1.66962	, testing loss - 1.75902	
482	 steps: training loss - 1.69781	, testing loss - 1.75851	
483	 steps: training loss - 1.69314	, testing loss - 1.75784	
484	 steps: training loss - 1.72805	, testing loss - 1.75707	
485	 steps: training loss - 1.60897	, testing loss - 1.75626	
486	 steps: training loss - 1.65485	, testing loss - 1.75568	
487	 steps: training loss - 1.69300	, testing loss - 1.75528	
488	 steps: training loss - 1.67999	, testing loss - 1.75483	
489	 steps: training loss - 1.71943	, testing loss - 1.75432	
490	 steps: training loss - 1.71008	, testing loss - 1.75368	
491	 steps: training loss - 1.67918	, testing loss - 1.75287	
492	 steps: training loss - 1.66182	, testing loss - 1.75226	
493	 steps: training loss - 1.63181	, testing loss - 1.75192	
494	 steps: training loss - 1.70910	, testing loss - 1.75183	
495	 steps: training loss - 1.64106	, testing loss - 1.75157	
496	 steps: training loss - 1.73838	, testing loss - 1.75155	
497	 steps: training loss - 1.68886	, testing loss - 1.75127	
498	 steps: training loss - 1.66239	, testing loss - 1.75089	
499	 steps: training loss - 1.63230	, testing loss - 1.75040	
500	 steps: training loss - 1.69357	, testing loss - 1.74999	
501	 steps: training loss - 1.65768	, testing loss - 1.74948	
502	 steps: training loss - 1.62504	, testing loss - 1.74909	
503	 steps: training loss - 1.69636	, testing loss - 1.74898	
504	 steps: training loss - 1.71078	, testing loss - 1.74894	
505	 steps: training loss - 1.63449	, testing loss - 1.74882	
506	 steps: training loss - 1.62930	, testing loss - 1.74892	
507	 steps: training loss - 1.67532	, testing loss - 1.74913	
508	 steps: training loss - 1.64830	, testing loss - 1.74924	
509	 steps: training loss - 1.65283	, testing loss - 1.74945	
510	 steps: training loss - 1.65066	, testing loss - 1.74968	
511	 steps: training loss - 1.61674	, testing loss - 1.74996	
512	 steps: training loss - 1.66465	, testing loss - 1.75032	
513	 steps: training loss - 1.71147	, testing loss - 1.75073	
514	 steps: training loss - 1.66663	, testing loss - 1.75102	
515	 steps: training loss - 1.62487	, testing loss - 1.75104	
516	 steps: training loss - 1.74743	, testing loss - 1.75095	
517	 steps: training loss - 1.70871	, testing loss - 1.75061	
518	 steps: training loss - 1.73019	, testing loss - 1.75024	
519	 steps: training loss - 1.71994	, testing loss - 1.74968	
520	 steps: training loss - 1.69689	, testing loss - 1.74908	
521	 steps: training loss - 1.68793	, testing loss - 1.74843	
522	 steps: training loss - 1.63930	, testing loss - 1.74782	
523	 steps: training loss - 1.66564	, testing loss - 1.74739	
524	 steps: training loss - 1.71976	, testing loss - 1.74719	
525	 steps: training loss - 1.67912	, testing loss - 1.74702	
526	 steps: training loss - 1.68130	, testing loss - 1.74706	
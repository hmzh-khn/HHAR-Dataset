
LOOCV TRAINING INFO - is_lstm? True
LOOCV? -  True
-------------
TRAINING USERS -  ['b', 'c', 'e', 'g', 'h', 'i']
LEFT OUT -  a
-------------
actions ['bike', 'sit']
devices ['samsungold_1', 'samsungold_2']
step size 10
num_epochs 1000
batch size 12
feature size (x,y,z) 3
hidden size 20
number recurrent layers 2
number of classes 2
writing pickle to run9_model.p
writing accuracy + pickle to acc_run9_model.p
------------------------------------------------
0	 steps: training loss - 0.65866	, testing loss - 0.68835	
1	 steps: training loss - 0.66926	, testing loss - 0.68831	
2	 steps: training loss - 0.71956	, testing loss - 0.68828	
3	 steps: training loss - 0.65621	, testing loss - 0.68816	
4	 steps: training loss - 0.66177	, testing loss - 0.68806	
5	 steps: training loss - 0.66090	, testing loss - 0.68799	
6	 steps: training loss - 0.65206	, testing loss - 0.68796	
7	 steps: training loss - 0.67860	, testing loss - 0.68798	
8	 steps: training loss - 0.70716	, testing loss - 0.68797	
9	 steps: training loss - 0.65514	, testing loss - 0.68788	
10	 steps: training loss - 0.63200	, testing loss - 0.68779	
11	 steps: training loss - 0.68401	, testing loss - 0.68778	
12	 steps: training loss - 0.63106	, testing loss - 0.68776	
13	 steps: training loss - 0.64321	, testing loss - 0.68779	
14	 steps: training loss - 0.65582	, testing loss - 0.68783	
15	 steps: training loss - 0.64566	, testing loss - 0.68788	
16	 steps: training loss - 0.62804	, testing loss - 0.68797	
17	 steps: training loss - 0.62374	, testing loss - 0.68809	
18	 steps: training loss - 0.57904	, testing loss - 0.68825	
19	 steps: training loss - 0.67479	, testing loss - 0.68848	
20	 steps: training loss - 0.70586	, testing loss - 0.68868	
21	 steps: training loss - 0.65877	, testing loss - 0.68877	
22	 steps: training loss - 0.67008	, testing loss - 0.68881	
23	 steps: training loss - 0.61487	, testing loss - 0.68883	
24	 steps: training loss - 0.62720	, testing loss - 0.68889	
25	 steps: training loss - 0.58336	, testing loss - 0.68900	
26	 steps: training loss - 0.56979	, testing loss - 0.68918	
27	 steps: training loss - 0.58726	, testing loss - 0.68943	
28	 steps: training loss - 0.59783	, testing loss - 0.68974	
29	 steps: training loss - 0.68056	, testing loss - 0.69009	
30	 steps: training loss - 0.61502	, testing loss - 0.69037	
31	 steps: training loss - 0.63418	, testing loss - 0.69066	
32	 steps: training loss - 0.63627	, testing loss - 0.69091	
33	 steps: training loss - 0.64773	, testing loss - 0.69114	
34	 steps: training loss - 0.61823	, testing loss - 0.69133	
35	 steps: training loss - 0.60482	, testing loss - 0.69153	
36	 steps: training loss - 0.63040	, testing loss - 0.69177	
37	 steps: training loss - 0.63432	, testing loss - 0.69196	
38	 steps: training loss - 0.63465	, testing loss - 0.69212	
39	 steps: training loss - 0.56405	, testing loss - 0.69227	
40	 steps: training loss - 0.62960	, testing loss - 0.69252	
41	 steps: training loss - 0.62963	, testing loss - 0.69279	
42	 steps: training loss - 0.61587	, testing loss - 0.69301	
43	 steps: training loss - 0.64364	, testing loss - 0.69325	
44	 steps: training loss - 0.69202	, testing loss - 0.69345	
45	 steps: training loss - 0.61557	, testing loss - 0.69349	
46	 steps: training loss - 0.55066	, testing loss - 0.69353	
47	 steps: training loss - 0.59933	, testing loss - 0.69373	
48	 steps: training loss - 0.58599	, testing loss - 0.69400	
49	 steps: training loss - 0.61646	, testing loss - 0.69433	
50	 steps: training loss - 0.58790	, testing loss - 0.69470	
51	 steps: training loss - 0.60015	, testing loss - 0.69508	
52	 steps: training loss - 0.64646	, testing loss - 0.69545	
53	 steps: training loss - 0.68526	, testing loss - 0.69570	
54	 steps: training loss - 0.63065	, testing loss - 0.69582	
55	 steps: training loss - 0.60297	, testing loss - 0.69591	
56	 steps: training loss - 0.64570	, testing loss - 0.69605	
57	 steps: training loss - 0.63111	, testing loss - 0.69611	
58	 steps: training loss - 0.63795	, testing loss - 0.69614	
59	 steps: training loss - 0.69391	, testing loss - 0.69615	
60	 steps: training loss - 0.64370	, testing loss - 0.69601	
61	 steps: training loss - 0.64013	, testing loss - 0.69585	
62	 steps: training loss - 0.56839	, testing loss - 0.69567	
63	 steps: training loss - 0.65689	, testing loss - 0.69562	
64	 steps: training loss - 0.58551	, testing loss - 0.69557	
65	 steps: training loss - 0.66666	, testing loss - 0.69554	
66	 steps: training loss - 0.58240	, testing loss - 0.69548	
67	 steps: training loss - 0.66045	, testing loss - 0.69547	
68	 steps: training loss - 0.58053	, testing loss - 0.69543	
69	 steps: training loss - 0.71905	, testing loss - 0.69543	
70	 steps: training loss - 0.56625	, testing loss - 0.69530	
71	 steps: training loss - 0.62112	, testing loss - 0.69526	
72	 steps: training loss - 0.59732	, testing loss - 0.69527	
73	 steps: training loss - 0.64359	, testing loss - 0.69531	
74	 steps: training loss - 0.59839	, testing loss - 0.69530	
75	 steps: training loss - 0.61423	, testing loss - 0.69535	
76	 steps: training loss - 0.68945	, testing loss - 0.69543	
77	 steps: training loss - 0.59894	, testing loss - 0.69541	
78	 steps: training loss - 0.65269	, testing loss - 0.69540	
79	 steps: training loss - 0.62586	, testing loss - 0.69537	
80	 steps: training loss - 0.64930	, testing loss - 0.69529	
81	 steps: training loss - 0.56516	, testing loss - 0.69519	
82	 steps: training loss - 0.58233	, testing loss - 0.69522	
83	 steps: training loss - 0.61238	, testing loss - 0.69530	
84	 steps: training loss - 0.55957	, testing loss - 0.69538	
85	 steps: training loss - 0.70860	, testing loss - 0.69555	
86	 steps: training loss - 0.55962	, testing loss - 0.69557	
87	 steps: training loss - 0.59657	, testing loss - 0.69563	
88	 steps: training loss - 0.67351	, testing loss - 0.69571	
89	 steps: training loss - 0.59071	, testing loss - 0.69573	
90	 steps: training loss - 0.59145	, testing loss - 0.69575	
91	 steps: training loss - 0.58910	, testing loss - 0.69580	
92	 steps: training loss - 0.65420	, testing loss - 0.69587	
93	 steps: training loss - 0.61455	, testing loss - 0.69589	
94	 steps: training loss - 0.57332	, testing loss - 0.69590	
95	 steps: training loss - 0.62847	, testing loss - 0.69598	
96	 steps: training loss - 0.55506	, testing loss - 0.69606	
97	 steps: training loss - 0.58742	, testing loss - 0.69619	
98	 steps: training loss - 0.61359	, testing loss - 0.69637	
99	 steps: training loss - 0.59556	, testing loss - 0.69656	
100	 steps: training loss - 0.58810	, testing loss - 0.69674	
101	 steps: training loss - 0.58987	, testing loss - 0.69692	
102	 steps: training loss - 0.60387	, testing loss - 0.69712	
103	 steps: training loss - 0.54207	, testing loss - 0.69730	
104	 steps: training loss - 0.54939	, testing loss - 0.69754	
105	 steps: training loss - 0.57267	, testing loss - 0.69782	
106	 steps: training loss - 0.57848	, testing loss - 0.69809	
107	 steps: training loss - 0.56257	, testing loss - 0.69837	
108	 steps: training loss - 0.62380	, testing loss - 0.69866	
109	 steps: training loss - 0.57665	, testing loss - 0.69889	
110	 steps: training loss - 0.57654	, testing loss - 0.69907	
111	 steps: training loss - 0.60093	, testing loss - 0.69931	
112	 steps: training loss - 0.59361	, testing loss - 0.69953	
113	 steps: training loss - 0.56593	, testing loss - 0.69970	
114	 steps: training loss - 0.54733	, testing loss - 0.69984	
115	 steps: training loss - 0.58383	, testing loss - 0.70001	
116	 steps: training loss - 0.60785	, testing loss - 0.70020	
117	 steps: training loss - 0.61889	, testing loss - 0.70036	
118	 steps: training loss - 0.57986	, testing loss - 0.70045	
119	 steps: training loss - 0.64129	, testing loss - 0.70053	
120	 steps: training loss - 0.53797	, testing loss - 0.70053	
121	 steps: training loss - 0.60826	, testing loss - 0.70060	
122	 steps: training loss - 0.61043	, testing loss - 0.70071	
123	 steps: training loss - 0.52366	, testing loss - 0.70078	
124	 steps: training loss - 0.58257	, testing loss - 0.70089	
125	 steps: training loss - 0.55632	, testing loss - 0.70106	
126	 steps: training loss - 0.56548	, testing loss - 0.70127	
127	 steps: training loss - 0.55466	, testing loss - 0.70147	
128	 steps: training loss - 0.60595	, testing loss - 0.70170	
129	 steps: training loss - 0.59425	, testing loss - 0.70188	
130	 steps: training loss - 0.55927	, testing loss - 0.70205	
131	 steps: training loss - 0.61041	, testing loss - 0.70222	
132	 steps: training loss - 0.59353	, testing loss - 0.70235	
133	 steps: training loss - 0.62746	, testing loss - 0.70244	
134	 steps: training loss - 0.55620	, testing loss - 0.70244	
135	 steps: training loss - 0.53361	, testing loss - 0.70248	
136	 steps: training loss - 0.57846	, testing loss - 0.70255	
137	 steps: training loss - 0.54825	, testing loss - 0.70267	
138	 steps: training loss - 0.56811	, testing loss - 0.70281	
139	 steps: training loss - 0.52040	, testing loss - 0.70299	
140	 steps: training loss - 0.59850	, testing loss - 0.70327	
141	 steps: training loss - 0.53734	, testing loss - 0.70349	
142	 steps: training loss - 0.61200	, testing loss - 0.70369	
143	 steps: training loss - 0.52401	, testing loss - 0.70384	
144	 steps: training loss - 0.58152	, testing loss - 0.70400	
145	 steps: training loss - 0.52096	, testing loss - 0.70419	
146	 steps: training loss - 0.62246	, testing loss - 0.70445	
147	 steps: training loss - 0.57229	, testing loss - 0.70465	
148	 steps: training loss - 0.58870	, testing loss - 0.70479	
149	 steps: training loss - 0.56157	, testing loss - 0.70492	
150	 steps: training loss - 0.51761	, testing loss - 0.70514	
151	 steps: training loss - 0.58067	, testing loss - 0.70544	
152	 steps: training loss - 0.57644	, testing loss - 0.70578	
153	 steps: training loss - 0.62438	, testing loss - 0.70605	
154	 steps: training loss - 0.54133	, testing loss - 0.70619	
155	 steps: training loss - 0.52689	, testing loss - 0.70644	
156	 steps: training loss - 0.56080	, testing loss - 0.70675	
157	 steps: training loss - 0.56077	, testing loss - 0.70706	
158	 steps: training loss - 0.55845	, testing loss - 0.70739	
159	 steps: training loss - 0.58738	, testing loss - 0.70773	
160	 steps: training loss - 0.62764	, testing loss - 0.70810	
161	 steps: training loss - 0.57272	, testing loss - 0.70825	
162	 steps: training loss - 0.55228	, testing loss - 0.70838	
163	 steps: training loss - 0.55935	, testing loss - 0.70857	
164	 steps: training loss - 0.50928	, testing loss - 0.70876	
165	 steps: training loss - 0.61234	, testing loss - 0.70904	
166	 steps: training loss - 0.61324	, testing loss - 0.70933	
167	 steps: training loss - 0.51063	, testing loss - 0.70951	
168	 steps: training loss - 0.58705	, testing loss - 0.70971	
169	 steps: training loss - 0.54580	, testing loss - 0.70991	
170	 steps: training loss - 0.56589	, testing loss - 0.71012	
171	 steps: training loss - 0.52021	, testing loss - 0.71031	
172	 steps: training loss - 0.59962	, testing loss - 0.71055	
173	 steps: training loss - 0.55206	, testing loss - 0.71078	
174	 steps: training loss - 0.52214	, testing loss - 0.71101	
175	 steps: training loss - 0.51657	, testing loss - 0.71128	
176	 steps: training loss - 0.64084	, testing loss - 0.71160	
177	 steps: training loss - 0.54848	, testing loss - 0.71174	
178	 steps: training loss - 0.53692	, testing loss - 0.71185	
179	 steps: training loss - 0.58694	, testing loss - 0.71201	
180	 steps: training loss - 0.49080	, testing loss - 0.71215	
181	 steps: training loss - 0.57469	, testing loss - 0.71238	
182	 steps: training loss - 0.57052	, testing loss - 0.71257	
183	 steps: training loss - 0.49350	, testing loss - 0.71278	
184	 steps: training loss - 0.53337	, testing loss - 0.71316	
185	 steps: training loss - 0.52314	, testing loss - 0.71358	
186	 steps: training loss - 0.56024	, testing loss - 0.71405	
187	 steps: training loss - 0.53858	, testing loss - 0.71451	
188	 steps: training loss - 0.51692	, testing loss - 0.71491	
189	 steps: training loss - 0.59163	, testing loss - 0.71531	
190	 steps: training loss - 0.56965	, testing loss - 0.71561	
191	 steps: training loss - 0.50767	, testing loss - 0.71576	
192	 steps: training loss - 0.55503	, testing loss - 0.71594	
193	 steps: training loss - 0.46990	, testing loss - 0.71615	
194	 steps: training loss - 0.55826	, testing loss - 0.71656	
195	 steps: training loss - 0.62599	, testing loss - 0.71694	
196	 steps: training loss - 0.50033	, testing loss - 0.71718	
197	 steps: training loss - 0.49163	, testing loss - 0.71734	
198	 steps: training loss - 0.56488	, testing loss - 0.71754	
199	 steps: training loss - 0.57867	, testing loss - 0.71781	
200	 steps: training loss - 0.58336	, testing loss - 0.71808	
201	 steps: training loss - 0.54988	, testing loss - 0.71830	
202	 steps: training loss - 0.64815	, testing loss - 0.71848	
203	 steps: training loss - 0.57886	, testing loss - 0.71848	
204	 steps: training loss - 0.45954	, testing loss - 0.71837	
205	 steps: training loss - 0.57293	, testing loss - 0.71838	
206	 steps: training loss - 0.59096	, testing loss - 0.71845	
207	 steps: training loss - 0.52479	, testing loss - 0.71849	
208	 steps: training loss - 0.52890	, testing loss - 0.71854	
209	 steps: training loss - 0.57313	, testing loss - 0.71858	
210	 steps: training loss - 0.54844	, testing loss - 0.71856	
211	 steps: training loss - 0.54482	, testing loss - 0.71847	
212	 steps: training loss - 0.53335	, testing loss - 0.71844	
213	 steps: training loss - 0.52548	, testing loss - 0.71850	
214	 steps: training loss - 0.48309	, testing loss - 0.71861	
215	 steps: training loss - 0.50296	, testing loss - 0.71874	
216	 steps: training loss - 0.54229	, testing loss - 0.71889	
217	 steps: training loss - 0.44942	, testing loss - 0.71906	
218	 steps: training loss - 0.57372	, testing loss - 0.71932	
219	 steps: training loss - 0.57653	, testing loss - 0.71951	
220	 steps: training loss - 0.61180	, testing loss - 0.71968	
221	 steps: training loss - 0.59345	, testing loss - 0.71974	
222	 steps: training loss - 0.56272	, testing loss - 0.71974	
223	 steps: training loss - 0.65392	, testing loss - 0.71978	
224	 steps: training loss - 0.48723	, testing loss - 0.71976	
225	 steps: training loss - 0.55218	, testing loss - 0.71981	
226	 steps: training loss - 0.52352	, testing loss - 0.71988	
227	 steps: training loss - 0.52390	, testing loss - 0.71999	
228	 steps: training loss - 0.54888	, testing loss - 0.72007	
229	 steps: training loss - 0.55109	, testing loss - 0.72020	
230	 steps: training loss - 0.48291	, testing loss - 0.72034	
231	 steps: training loss - 0.53910	, testing loss - 0.72052	
232	 steps: training loss - 0.48023	, testing loss - 0.72078	
233	 steps: training loss - 0.53384	, testing loss - 0.72108	
234	 steps: training loss - 0.56290	, testing loss - 0.72139	
235	 steps: training loss - 0.47947	, testing loss - 0.72155	
236	 steps: training loss - 0.54206	, testing loss - 0.72178	
237	 steps: training loss - 0.61916	, testing loss - 0.72201	
238	 steps: training loss - 0.57123	, testing loss - 0.72212	
239	 steps: training loss - 0.57650	, testing loss - 0.72222	
240	 steps: training loss - 0.49799	, testing loss - 0.72237	
241	 steps: training loss - 0.50897	, testing loss - 0.72259	
242	 steps: training loss - 0.56327	, testing loss - 0.72285	
243	 steps: training loss - 0.46637	, testing loss - 0.72305	
244	 steps: training loss - 0.51842	, testing loss - 0.72334	
245	 steps: training loss - 0.51673	, testing loss - 0.72364	
246	 steps: training loss - 0.40955	, testing loss - 0.72389	
247	 steps: training loss - 0.51201	, testing loss - 0.72420	
248	 steps: training loss - 0.51798	, testing loss - 0.72457	
249	 steps: training loss - 0.57404	, testing loss - 0.72498	
250	 steps: training loss - 0.51612	, testing loss - 0.72528	
251	 steps: training loss - 0.48134	, testing loss - 0.72557	
252	 steps: training loss - 0.46305	, testing loss - 0.72586	
253	 steps: training loss - 0.51171	, testing loss - 0.72628	
254	 steps: training loss - 0.57016	, testing loss - 0.72674	
255	 steps: training loss - 0.55348	, testing loss - 0.72713	
256	 steps: training loss - 0.58191	, testing loss - 0.72740	
257	 steps: training loss - 0.58520	, testing loss - 0.72760	
258	 steps: training loss - 0.55665	, testing loss - 0.72763	
259	 steps: training loss - 0.52810	, testing loss - 0.72759	
260	 steps: training loss - 0.43240	, testing loss - 0.72760	
261	 steps: training loss - 0.55071	, testing loss - 0.72780	
262	 steps: training loss - 0.47747	, testing loss - 0.72793	
263	 steps: training loss - 0.58476	, testing loss - 0.72805	
264	 steps: training loss - 0.62062	, testing loss - 0.72819	
265	 steps: training loss - 0.54998	, testing loss - 0.72830	
266	 steps: training loss - 0.51924	, testing loss - 0.72836	
267	 steps: training loss - 0.55653	, testing loss - 0.72851	
268	 steps: training loss - 0.51803	, testing loss - 0.72863	
269	 steps: training loss - 0.53745	, testing loss - 0.72874	
270	 steps: training loss - 0.55945	, testing loss - 0.72891	
271	 steps: training loss - 0.42410	, testing loss - 0.72904	
272	 steps: training loss - 0.46695	, testing loss - 0.72926	
273	 steps: training loss - 0.42823	, testing loss - 0.72952	
274	 steps: training loss - 0.56015	, testing loss - 0.72987	
275	 steps: training loss - 0.51509	, testing loss - 0.73022	
276	 steps: training loss - 0.44759	, testing loss - 0.73060	
277	 steps: training loss - 0.44480	, testing loss - 0.73104	
278	 steps: training loss - 0.47528	, testing loss - 0.73151	
279	 steps: training loss - 0.49099	, testing loss - 0.73202	
280	 steps: training loss - 0.49307	, testing loss - 0.73252	
281	 steps: training loss - 0.42774	, testing loss - 0.73309	
282	 steps: training loss - 0.57663	, testing loss - 0.73377	
283	 steps: training loss - 0.44817	, testing loss - 0.73443	
284	 steps: training loss - 0.48963	, testing loss - 0.73514	
285	 steps: training loss - 0.57850	, testing loss - 0.73591	
286	 steps: training loss - 0.49576	, testing loss - 0.73648	
287	 steps: training loss - 0.60496	, testing loss - 0.73697	
288	 steps: training loss - 0.55421	, testing loss - 0.73736	
289	 steps: training loss - 0.61585	, testing loss - 0.73752	
290	 steps: training loss - 0.57885	, testing loss - 0.73751	
291	 steps: training loss - 0.50173	, testing loss - 0.73737	
292	 steps: training loss - 0.50999	, testing loss - 0.73731	
293	 steps: training loss - 0.55394	, testing loss - 0.73740	
294	 steps: training loss - 0.47878	, testing loss - 0.73751	
295	 steps: training loss - 0.48878	, testing loss - 0.73763	
296	 steps: training loss - 0.57714	, testing loss - 0.73779	
297	 steps: training loss - 0.47021	, testing loss - 0.73783	
298	 steps: training loss - 0.50061	, testing loss - 0.73788	
299	 steps: training loss - 0.48634	, testing loss - 0.73806	
300	 steps: training loss - 0.51982	, testing loss - 0.73830	
301	 steps: training loss - 0.55716	, testing loss - 0.73849	
302	 steps: training loss - 0.51281	, testing loss - 0.73872	
303	 steps: training loss - 0.46122	, testing loss - 0.73887	
304	 steps: training loss - 0.55241	, testing loss - 0.73908	
305	 steps: training loss - 0.54453	, testing loss - 0.73928	
306	 steps: training loss - 0.49875	, testing loss - 0.73941	
307	 steps: training loss - 0.44471	, testing loss - 0.73962	
308	 steps: training loss - 0.49626	, testing loss - 0.73985	
309	 steps: training loss - 0.51801	, testing loss - 0.74000	
310	 steps: training loss - 0.43493	, testing loss - 0.74017	
311	 steps: training loss - 0.53600	, testing loss - 0.74042	
312	 steps: training loss - 0.39925	, testing loss - 0.74063	
313	 steps: training loss - 0.56201	, testing loss - 0.74093	
314	 steps: training loss - 0.48316	, testing loss - 0.74120	
315	 steps: training loss - 0.58086	, testing loss - 0.74148	
316	 steps: training loss - 0.45471	, testing loss - 0.74164	
317	 steps: training loss - 0.47027	, testing loss - 0.74184	
318	 steps: training loss - 0.47699	, testing loss - 0.74200	
319	 steps: training loss - 0.45263	, testing loss - 0.74212	
320	 steps: training loss - 0.51028	, testing loss - 0.74225	
321	 steps: training loss - 0.52059	, testing loss - 0.74237	
322	 steps: training loss - 0.44094	, testing loss - 0.74249	
323	 steps: training loss - 0.58182	, testing loss - 0.74263	
324	 steps: training loss - 0.54435	, testing loss - 0.74270	
325	 steps: training loss - 0.47046	, testing loss - 0.74279	
326	 steps: training loss - 0.45881	, testing loss - 0.74296	
327	 steps: training loss - 0.53271	, testing loss - 0.74308	
328	 steps: training loss - 0.54374	, testing loss - 0.74313	
329	 steps: training loss - 0.45144	, testing loss - 0.74302	
330	 steps: training loss - 0.44070	, testing loss - 0.74296	
331	 steps: training loss - 0.46021	, testing loss - 0.74303	
332	 steps: training loss - 0.43178	, testing loss - 0.74330	
333	 steps: training loss - 0.45209	, testing loss - 0.74367	
334	 steps: training loss - 0.56262	, testing loss - 0.74395	
335	 steps: training loss - 0.48159	, testing loss - 0.74422	
336	 steps: training loss - 0.49989	, testing loss - 0.74441	
337	 steps: training loss - 0.57597	, testing loss - 0.74460	
338	 steps: training loss - 0.45859	, testing loss - 0.74479	
339	 steps: training loss - 0.46775	, testing loss - 0.74504	
340	 steps: training loss - 0.56321	, testing loss - 0.74527	
341	 steps: training loss - 0.51972	, testing loss - 0.74551	
342	 steps: training loss - 0.44841	, testing loss - 0.74570	
343	 steps: training loss - 0.42338	, testing loss - 0.74591	
344	 steps: training loss - 0.49514	, testing loss - 0.74617	
345	 steps: training loss - 0.50287	, testing loss - 0.74647	
346	 steps: training loss - 0.43709	, testing loss - 0.74673	
347	 steps: training loss - 0.45450	, testing loss - 0.74700	
348	 steps: training loss - 0.51786	, testing loss - 0.74736	
349	 steps: training loss - 0.42768	, testing loss - 0.74768	
350	 steps: training loss - 0.52904	, testing loss - 0.74798	
351	 steps: training loss - 0.57851	, testing loss - 0.74821	
352	 steps: training loss - 0.53546	, testing loss - 0.74842	
353	 steps: training loss - 0.51891	, testing loss - 0.74864	
354	 steps: training loss - 0.47402	, testing loss - 0.74890	
355	 steps: training loss - 0.42991	, testing loss - 0.74910	
356	 steps: training loss - 0.56337	, testing loss - 0.74934	
357	 steps: training loss - 0.47052	, testing loss - 0.74951	
358	 steps: training loss - 0.55589	, testing loss - 0.74971	
359	 steps: training loss - 0.47079	, testing loss - 0.74996	
360	 steps: training loss - 0.53436	, testing loss - 0.75024	
361	 steps: training loss - 0.43141	, testing loss - 0.75043	
362	 steps: training loss - 0.53592	, testing loss - 0.75064	
363	 steps: training loss - 0.42198	, testing loss - 0.75087	
364	 steps: training loss - 0.51629	, testing loss - 0.75123	
365	 steps: training loss - 0.49054	, testing loss - 0.75164	
366	 steps: training loss - 0.49350	, testing loss - 0.75202	
367	 steps: training loss - 0.45912	, testing loss - 0.75241	
368	 steps: training loss - 0.53492	, testing loss - 0.75286	
369	 steps: training loss - 0.49782	, testing loss - 0.75328	
370	 steps: training loss - 0.51690	, testing loss - 0.75367	
371	 steps: training loss - 0.43386	, testing loss - 0.75404	
372	 steps: training loss - 0.42392	, testing loss - 0.75430	
373	 steps: training loss - 0.53354	, testing loss - 0.75458	
374	 steps: training loss - 0.49055	, testing loss - 0.75493	
375	 steps: training loss - 0.52403	, testing loss - 0.75529	
376	 steps: training loss - 0.42814	, testing loss - 0.75560	
377	 steps: training loss - 0.50013	, testing loss - 0.75590	
378	 steps: training loss - 0.39157	, testing loss - 0.75616	
379	 steps: training loss - 0.46653	, testing loss - 0.75639	
380	 steps: training loss - 0.54627	, testing loss - 0.75654	
381	 steps: training loss - 0.45874	, testing loss - 0.75664	
382	 steps: training loss - 0.54573	, testing loss - 0.75679	
383	 steps: training loss - 0.42759	, testing loss - 0.75680	
384	 steps: training loss - 0.49247	, testing loss - 0.75683	
385	 steps: training loss - 0.55502	, testing loss - 0.75689	
386	 steps: training loss - 0.51240	, testing loss - 0.75695	
387	 steps: training loss - 0.54593	, testing loss - 0.75700	
388	 steps: training loss - 0.57768	, testing loss - 0.75694	
389	 steps: training loss - 0.48570	, testing loss - 0.75680	
390	 steps: training loss - 0.48078	, testing loss - 0.75670	
391	 steps: training loss - 0.45872	, testing loss - 0.75672	
392	 steps: training loss - 0.43254	, testing loss - 0.75685	
393	 steps: training loss - 0.50241	, testing loss - 0.75711	
394	 steps: training loss - 0.42207	, testing loss - 0.75742	
395	 steps: training loss - 0.47536	, testing loss - 0.75774	
396	 steps: training loss - 0.52892	, testing loss - 0.75807	
397	 steps: training loss - 0.48528	, testing loss - 0.75839	
398	 steps: training loss - 0.48512	, testing loss - 0.75876	
399	 steps: training loss - 0.42827	, testing loss - 0.75915	
400	 steps: training loss - 0.49347	, testing loss - 0.75961	
401	 steps: training loss - 0.43315	, testing loss - 0.76009	
402	 steps: training loss - 0.46381	, testing loss - 0.76056	
403	 steps: training loss - 0.48068	, testing loss - 0.76094	
404	 steps: training loss - 0.40423	, testing loss - 0.76134	
405	 steps: training loss - 0.40746	, testing loss - 0.76177	
406	 steps: training loss - 0.54173	, testing loss - 0.76216	
407	 steps: training loss - 0.36398	, testing loss - 0.76259	
408	 steps: training loss - 0.45150	, testing loss - 0.76306	
409	 steps: training loss - 0.48783	, testing loss - 0.76357	
410	 steps: training loss - 0.52385	, testing loss - 0.76409	
411	 steps: training loss - 0.52077	, testing loss - 0.76455	
412	 steps: training loss - 0.53888	, testing loss - 0.76488	
413	 steps: training loss - 0.49865	, testing loss - 0.76511	
414	 steps: training loss - 0.48315	, testing loss - 0.76539	
415	 steps: training loss - 0.48715	, testing loss - 0.76564	
416	 steps: training loss - 0.40094	, testing loss - 0.76596	
417	 steps: training loss - 0.52392	, testing loss - 0.76624	
418	 steps: training loss - 0.44981	, testing loss - 0.76651	
419	 steps: training loss - 0.44117	, testing loss - 0.76681	
420	 steps: training loss - 0.50654	, testing loss - 0.76706	
421	 steps: training loss - 0.36595	, testing loss - 0.76729	
422	 steps: training loss - 0.47932	, testing loss - 0.76752	
423	 steps: training loss - 0.56041	, testing loss - 0.76776	
424	 steps: training loss - 0.40901	, testing loss - 0.76794	
425	 steps: training loss - 0.50218	, testing loss - 0.76812	
426	 steps: training loss - 0.54352	, testing loss - 0.76824	
427	 steps: training loss - 0.54269	, testing loss - 0.76841	
428	 steps: training loss - 0.45645	, testing loss - 0.76856	
429	 steps: training loss - 0.51185	, testing loss - 0.76873	
430	 steps: training loss - 0.49717	, testing loss - 0.76886	
431	 steps: training loss - 0.37865	, testing loss - 0.76881	
432	 steps: training loss - 0.54753	, testing loss - 0.76878	
433	 steps: training loss - 0.45982	, testing loss - 0.76863	
434	 steps: training loss - 0.53075	, testing loss - 0.76850	
435	 steps: training loss - 0.44536	, testing loss - 0.76837	
436	 steps: training loss - 0.42223	, testing loss - 0.76835	
437	 steps: training loss - 0.46999	, testing loss - 0.76842	
438	 steps: training loss - 0.51205	, testing loss - 0.76862	
439	 steps: training loss - 0.52767	, testing loss - 0.76878	
440	 steps: training loss - 0.53123	, testing loss - 0.76894	
441	 steps: training loss - 0.53003	, testing loss - 0.76911	
442	 steps: training loss - 0.41439	, testing loss - 0.76928	
443	 steps: training loss - 0.41824	, testing loss - 0.76947	
444	 steps: training loss - 0.36977	, testing loss - 0.76976	
445	 steps: training loss - 0.39838	, testing loss - 0.77006	
446	 steps: training loss - 0.39180	, testing loss - 0.77034	
447	 steps: training loss - 0.36154	, testing loss - 0.77069	
448	 steps: training loss - 0.45277	, testing loss - 0.77113	
449	 steps: training loss - 0.43748	, testing loss - 0.77157	
450	 steps: training loss - 0.59245	, testing loss - 0.77205	
451	 steps: training loss - 0.38970	, testing loss - 0.77247	
452	 steps: training loss - 0.45022	, testing loss - 0.77293	
453	 steps: training loss - 0.42987	, testing loss - 0.77342	
454	 steps: training loss - 0.37787	, testing loss - 0.77396	
455	 steps: training loss - 0.56715	, testing loss - 0.77453	
456	 steps: training loss - 0.37691	, testing loss - 0.77507	
457	 steps: training loss - 0.44851	, testing loss - 0.77553	
458	 steps: training loss - 0.34717	, testing loss - 0.77599	
459	 steps: training loss - 0.49656	, testing loss - 0.77643	
460	 steps: training loss - 0.49053	, testing loss - 0.77687	
461	 steps: training loss - 0.37116	, testing loss - 0.77725	
462	 steps: training loss - 0.45981	, testing loss - 0.77767	
463	 steps: training loss - 0.54463	, testing loss - 0.77811	
464	 steps: training loss - 0.52838	, testing loss - 0.77855	
465	 steps: training loss - 0.56538	, testing loss - 0.77885	
466	 steps: training loss - 0.41612	, testing loss - 0.77908	
467	 steps: training loss - 0.44005	, testing loss - 0.77930	
468	 steps: training loss - 0.35122	, testing loss - 0.77954	
469	 steps: training loss - 0.34604	, testing loss - 0.77990	
470	 steps: training loss - 0.46862	, testing loss - 0.78041	
471	 steps: training loss - 0.50587	, testing loss - 0.78101	
472	 steps: training loss - 0.44412	, testing loss - 0.78158	
473	 steps: training loss - 0.45993	, testing loss - 0.78212	
474	 steps: training loss - 0.32065	, testing loss - 0.78266	
475	 steps: training loss - 0.44672	, testing loss - 0.78321	
476	 steps: training loss - 0.39160	, testing loss - 0.78361	
477	 steps: training loss - 0.45451	, testing loss - 0.78401	
478	 steps: training loss - 0.50648	, testing loss - 0.78445	
479	 steps: training loss - 0.38362	, testing loss - 0.78485	
480	 steps: training loss - 0.34482	, testing loss - 0.78530	
481	 steps: training loss - 0.46142	, testing loss - 0.78576	
482	 steps: training loss - 0.49722	, testing loss - 0.78619	
483	 steps: training loss - 0.39114	, testing loss - 0.78645	
484	 steps: training loss - 0.42244	, testing loss - 0.78669	
485	 steps: training loss - 0.47194	, testing loss - 0.78697	
486	 steps: training loss - 0.54577	, testing loss - 0.78722	
487	 steps: training loss - 0.45619	, testing loss - 0.78749	
488	 steps: training loss - 0.46586	, testing loss - 0.78787	
489	 steps: training loss - 0.50794	, testing loss - 0.78832	
490	 steps: training loss - 0.39600	, testing loss - 0.78860	
491	 steps: training loss - 0.37978	, testing loss - 0.78878	
492	 steps: training loss - 0.50863	, testing loss - 0.78896	
493	 steps: training loss - 0.44385	, testing loss - 0.78915	
494	 steps: training loss - 0.40719	, testing loss - 0.78939	
495	 steps: training loss - 0.49864	, testing loss - 0.78959	
496	 steps: training loss - 0.41522	, testing loss - 0.78966	
497	 steps: training loss - 0.49848	, testing loss - 0.78982	
498	 steps: training loss - 0.42726	, testing loss - 0.79007	
499	 steps: training loss - 0.46270	, testing loss - 0.79037	
500	 steps: training loss - 0.50008	, testing loss - 0.79052	
501	 steps: training loss - 0.45871	, testing loss - 0.79062	
502	 steps: training loss - 0.44269	, testing loss - 0.79071	
503	 steps: training loss - 0.38098	, testing loss - 0.79095	
504	 steps: training loss - 0.48253	, testing loss - 0.79138	
505	 steps: training loss - 0.49195	, testing loss - 0.79180	
506	 steps: training loss - 0.38289	, testing loss - 0.79214	
507	 steps: training loss - 0.49967	, testing loss - 0.79253	
508	 steps: training loss - 0.40750	, testing loss - 0.79292	
509	 steps: training loss - 0.38586	, testing loss - 0.79331	
510	 steps: training loss - 0.46719	, testing loss - 0.79361	
511	 steps: training loss - 0.47281	, testing loss - 0.79385	
512	 steps: training loss - 0.41517	, testing loss - 0.79413	
513	 steps: training loss - 0.38136	, testing loss - 0.79448	
514	 steps: training loss - 0.46091	, testing loss - 0.79478	
515	 steps: training loss - 0.47110	, testing loss - 0.79499	
516	 steps: training loss - 0.35135	, testing loss - 0.79521	
517	 steps: training loss - 0.40758	, testing loss - 0.79543	
518	 steps: training loss - 0.33507	, testing loss - 0.79562	
519	 steps: training loss - 0.55172	, testing loss - 0.79594	
520	 steps: training loss - 0.52211	, testing loss - 0.79608	
521	 steps: training loss - 0.33929	, testing loss - 0.79615	
522	 steps: training loss - 0.34774	, testing loss - 0.79631	
523	 steps: training loss - 0.44140	, testing loss - 0.79646	
524	 steps: training loss - 0.48898	, testing loss - 0.79649	
525	 steps: training loss - 0.37528	, testing loss - 0.79655	
526	 steps: training loss - 0.46593	, testing loss - 0.79664	
527	 steps: training loss - 0.32930	, testing loss - 0.79680	
528	 steps: training loss - 0.34803	, testing loss - 0.79706	
529	 steps: training loss - 0.41125	, testing loss - 0.79732	
530	 steps: training loss - 0.39133	, testing loss - 0.79764	
531	 steps: training loss - 0.49097	, testing loss - 0.79805	
532	 steps: training loss - 0.33393	, testing loss - 0.79848	
533	 steps: training loss - 0.50529	, testing loss - 0.79887	
534	 steps: training loss - 0.39845	, testing loss - 0.79923	
535	 steps: training loss - 0.42539	, testing loss - 0.79954	
536	 steps: training loss - 0.52229	, testing loss - 0.79986	
537	 steps: training loss - 0.36630	, testing loss - 0.80025	
538	 steps: training loss - 0.55853	, testing loss - 0.80067	
539	 steps: training loss - 0.43570	, testing loss - 0.80106	
540	 steps: training loss - 0.44703	, testing loss - 0.80142	
541	 steps: training loss - 0.48409	, testing loss - 0.80177	
542	 steps: training loss - 0.49587	, testing loss - 0.80206	
543	 steps: training loss - 0.50079	, testing loss - 0.80228	
544	 steps: training loss - 0.49333	, testing loss - 0.80236	
545	 steps: training loss - 0.46467	, testing loss - 0.80246	
546	 steps: training loss - 0.39699	, testing loss - 0.80254	
547	 steps: training loss - 0.52864	, testing loss - 0.80259	
548	 steps: training loss - 0.40175	, testing loss - 0.80261	
549	 steps: training loss - 0.44849	, testing loss - 0.80260	
550	 steps: training loss - 0.37397	, testing loss - 0.80265	
551	 steps: training loss - 0.44313	, testing loss - 0.80279	
552	 steps: training loss - 0.47202	, testing loss - 0.80293	
553	 steps: training loss - 0.44678	, testing loss - 0.80310	
554	 steps: training loss - 0.42050	, testing loss - 0.80338	
555	 steps: training loss - 0.46900	, testing loss - 0.80371	
556	 steps: training loss - 0.45417	, testing loss - 0.80403	
557	 steps: training loss - 0.45492	, testing loss - 0.80440	
558	 steps: training loss - 0.43340	, testing loss - 0.80488	
559	 steps: training loss - 0.45715	, testing loss - 0.80536	
560	 steps: training loss - 0.49465	, testing loss - 0.80589	
561	 steps: training loss - 0.40502	, testing loss - 0.80641	
562	 steps: training loss - 0.43506	, testing loss - 0.80684	
563	 steps: training loss - 0.37348	, testing loss - 0.80736	
564	 steps: training loss - 0.48836	, testing loss - 0.80805	
565	 steps: training loss - 0.42566	, testing loss - 0.80875	
566	 steps: training loss - 0.39030	, testing loss - 0.80943	
567	 steps: training loss - 0.46309	, testing loss - 0.81012	
568	 steps: training loss - 0.35951	, testing loss - 0.81072	
569	 steps: training loss - 0.52437	, testing loss - 0.81111	
570	 steps: training loss - 0.40852	, testing loss - 0.81126	
571	 steps: training loss - 0.39326	, testing loss - 0.81130	
572	 steps: training loss - 0.39745	, testing loss - 0.81149	
573	 steps: training loss - 0.36475	, testing loss - 0.81171	
574	 steps: training loss - 0.50524	, testing loss - 0.81192	
575	 steps: training loss - 0.41982	, testing loss - 0.81207	
576	 steps: training loss - 0.46432	, testing loss - 0.81226	
577	 steps: training loss - 0.49516	, testing loss - 0.81237	
578	 steps: training loss - 0.48572	, testing loss - 0.81250	
579	 steps: training loss - 0.35036	, testing loss - 0.81251	
580	 steps: training loss - 0.37197	, testing loss - 0.81266	
581	 steps: training loss - 0.47507	, testing loss - 0.81289	
582	 steps: training loss - 0.32687	, testing loss - 0.81316	
583	 steps: training loss - 0.40450	, testing loss - 0.81354	
584	 steps: training loss - 0.43888	, testing loss - 0.81385	
585	 steps: training loss - 0.39917	, testing loss - 0.81417	
586	 steps: training loss - 0.42107	, testing loss - 0.81451	
587	 steps: training loss - 0.47159	, testing loss - 0.81475	
588	 steps: training loss - 0.36816	, testing loss - 0.81496	
589	 steps: training loss - 0.35315	, testing loss - 0.81522	
590	 steps: training loss - 0.49785	, testing loss - 0.81559	
591	 steps: training loss - 0.48195	, testing loss - 0.81592	
592	 steps: training loss - 0.41120	, testing loss - 0.81625	
593	 steps: training loss - 0.33182	, testing loss - 0.81665	
594	 steps: training loss - 0.45636	, testing loss - 0.81708	
595	 steps: training loss - 0.31612	, testing loss - 0.81742	
596	 steps: training loss - 0.40770	, testing loss - 0.81782	
597	 steps: training loss - 0.48017	, testing loss - 0.81817	
598	 steps: training loss - 0.48601	, testing loss - 0.81850	
599	 steps: training loss - 0.46081	, testing loss - 0.81887	
600	 steps: training loss - 0.43666	, testing loss - 0.81930	
601	 steps: training loss - 0.48840	, testing loss - 0.81971	
602	 steps: training loss - 0.31489	, testing loss - 0.82005	
603	 steps: training loss - 0.40531	, testing loss - 0.82032	
604	 steps: training loss - 0.42323	, testing loss - 0.82061	
605	 steps: training loss - 0.50576	, testing loss - 0.82095	
606	 steps: training loss - 0.52192	, testing loss - 0.82107	
607	 steps: training loss - 0.50365	, testing loss - 0.82095	
608	 steps: training loss - 0.43370	, testing loss - 0.82071	
609	 steps: training loss - 0.37510	, testing loss - 0.82035	
610	 steps: training loss - 0.45176	, testing loss - 0.82017	
611	 steps: training loss - 0.43284	, testing loss - 0.82018	
612	 steps: training loss - 0.51756	, testing loss - 0.82014	
613	 steps: training loss - 0.47723	, testing loss - 0.82007	
614	 steps: training loss - 0.41751	, testing loss - 0.82006	
615	 steps: training loss - 0.42665	, testing loss - 0.82015	
616	 steps: training loss - 0.37628	, testing loss - 0.82034	
617	 steps: training loss - 0.44046	, testing loss - 0.82064	
618	 steps: training loss - 0.46184	, testing loss - 0.82094	
619	 steps: training loss - 0.34937	, testing loss - 0.82122	
620	 steps: training loss - 0.34441	, testing loss - 0.82151	
621	 steps: training loss - 0.27025	, testing loss - 0.82183	
622	 steps: training loss - 0.32343	, testing loss - 0.82219	
623	 steps: training loss - 0.47092	, testing loss - 0.82261	
624	 steps: training loss - 0.53116	, testing loss - 0.82300	
625	 steps: training loss - 0.51919	, testing loss - 0.82329	
626	 steps: training loss - 0.42850	, testing loss - 0.82353	
627	 steps: training loss - 0.41474	, testing loss - 0.82379	
628	 steps: training loss - 0.45283	, testing loss - 0.82420	
629	 steps: training loss - 0.42073	, testing loss - 0.82454	
630	 steps: training loss - 0.46025	, testing loss - 0.82487	
631	 steps: training loss - 0.35983	, testing loss - 0.82522	
632	 steps: training loss - 0.34304	, testing loss - 0.82557	
633	 steps: training loss - 0.43406	, testing loss - 0.82591	
634	 steps: training loss - 0.29618	, testing loss - 0.82625	
635	 steps: training loss - 0.51195	, testing loss - 0.82670	
636	 steps: training loss - 0.44160	, testing loss - 0.82714	
637	 steps: training loss - 0.38249	, testing loss - 0.82770	
638	 steps: training loss - 0.47729	, testing loss - 0.82819	
639	 steps: training loss - 0.42607	, testing loss - 0.82864	
640	 steps: training loss - 0.31528	, testing loss - 0.82891	
641	 steps: training loss - 0.43929	, testing loss - 0.82920	
642	 steps: training loss - 0.40645	, testing loss - 0.82962	
643	 steps: training loss - 0.33358	, testing loss - 0.83008	
644	 steps: training loss - 0.37362	, testing loss - 0.83054	
645	 steps: training loss - 0.40712	, testing loss - 0.83099	
646	 steps: training loss - 0.39801	, testing loss - 0.83129	
647	 steps: training loss - 0.37440	, testing loss - 0.83161	
648	 steps: training loss - 0.41649	, testing loss - 0.83188	
649	 steps: training loss - 0.36579	, testing loss - 0.83212	
650	 steps: training loss - 0.42557	, testing loss - 0.83247	
651	 steps: training loss - 0.39966	, testing loss - 0.83270	
652	 steps: training loss - 0.40660	, testing loss - 0.83297	
653	 steps: training loss - 0.43565	, testing loss - 0.83330	
654	 steps: training loss - 0.41026	, testing loss - 0.83363	
655	 steps: training loss - 0.35442	, testing loss - 0.83399	
656	 steps: training loss - 0.40498	, testing loss - 0.83433	
657	 steps: training loss - 0.41916	, testing loss - 0.83455	
658	 steps: training loss - 0.49347	, testing loss - 0.83487	
659	 steps: training loss - 0.36598	, testing loss - 0.83521	
660	 steps: training loss - 0.47536	, testing loss - 0.83561	
661	 steps: training loss - 0.32713	, testing loss - 0.83606	
662	 steps: training loss - 0.43401	, testing loss - 0.83661	
663	 steps: training loss - 0.47370	, testing loss - 0.83693	
664	 steps: training loss - 0.41314	, testing loss - 0.83724	
665	 steps: training loss - 0.45778	, testing loss - 0.83758	
666	 steps: training loss - 0.36337	, testing loss - 0.83795	
667	 steps: training loss - 0.41188	, testing loss - 0.83838	
668	 steps: training loss - 0.38405	, testing loss - 0.83898	
669	 steps: training loss - 0.36139	, testing loss - 0.83947	
670	 steps: training loss - 0.57957	, testing loss - 0.83990	
671	 steps: training loss - 0.36500	, testing loss - 0.84013	
672	 steps: training loss - 0.49536	, testing loss - 0.84042	
673	 steps: training loss - 0.33393	, testing loss - 0.84070	
674	 steps: training loss - 0.44026	, testing loss - 0.84101	
675	 steps: training loss - 0.40735	, testing loss - 0.84116	
676	 steps: training loss - 0.42510	, testing loss - 0.84132	
677	 steps: training loss - 0.39809	, testing loss - 0.84151	
678	 steps: training loss - 0.33183	, testing loss - 0.84160	
679	 steps: training loss - 0.37299	, testing loss - 0.84170	
680	 steps: training loss - 0.41403	, testing loss - 0.84198	
681	 steps: training loss - 0.44631	, testing loss - 0.84230	
682	 steps: training loss - 0.39340	, testing loss - 0.84237	
683	 steps: training loss - 0.51273	, testing loss - 0.84250	
684	 steps: training loss - 0.37954	, testing loss - 0.84277	
685	 steps: training loss - 0.49557	, testing loss - 0.84301	
686	 steps: training loss - 0.45827	, testing loss - 0.84321	
687	 steps: training loss - 0.46224	, testing loss - 0.84329	
688	 steps: training loss - 0.47422	, testing loss - 0.84311	
689	 steps: training loss - 0.33248	, testing loss - 0.84289	
690	 steps: training loss - 0.45645	, testing loss - 0.84282	
691	 steps: training loss - 0.37858	, testing loss - 0.84285	
692	 steps: training loss - 0.47563	, testing loss - 0.84288	
693	 steps: training loss - 0.49418	, testing loss - 0.84288	
694	 steps: training loss - 0.51415	, testing loss - 0.84292	
695	 steps: training loss - 0.34862	, testing loss - 0.84298	
696	 steps: training loss - 0.41996	, testing loss - 0.84312	
697	 steps: training loss - 0.31686	, testing loss - 0.84332	
698	 steps: training loss - 0.49066	, testing loss - 0.84361	
699	 steps: training loss - 0.43889	, testing loss - 0.84392	
700	 steps: training loss - 0.46100	, testing loss - 0.84418	
701	 steps: training loss - 0.40852	, testing loss - 0.84447	
702	 steps: training loss - 0.40431	, testing loss - 0.84484	
703	 steps: training loss - 0.55136	, testing loss - 0.84523	
704	 steps: training loss - 0.47230	, testing loss - 0.84563	
705	 steps: training loss - 0.36576	, testing loss - 0.84606	
706	 steps: training loss - 0.35465	, testing loss - 0.84655	
707	 steps: training loss - 0.48790	, testing loss - 0.84707	
708	 steps: training loss - 0.39505	, testing loss - 0.84751	
709	 steps: training loss - 0.35486	, testing loss - 0.84798	
710	 steps: training loss - 0.42614	, testing loss - 0.84844	
711	 steps: training loss - 0.43180	, testing loss - 0.84898	
712	 steps: training loss - 0.50440	, testing loss - 0.84952	
713	 steps: training loss - 0.45238	, testing loss - 0.85001	
714	 steps: training loss - 0.41870	, testing loss - 0.85052	
715	 steps: training loss - 0.52805	, testing loss - 0.85095	
716	 steps: training loss - 0.42217	, testing loss - 0.85127	
717	 steps: training loss - 0.44161	, testing loss - 0.85170	
718	 steps: training loss - 0.44157	, testing loss - 0.85220	
719	 steps: training loss - 0.47299	, testing loss - 0.85256	
720	 steps: training loss - 0.48239	, testing loss - 0.85292	
721	 steps: training loss - 0.32133	, testing loss - 0.85327	
722	 steps: training loss - 0.42642	, testing loss - 0.85362	
723	 steps: training loss - 0.39493	, testing loss - 0.85401	
724	 steps: training loss - 0.43316	, testing loss - 0.85429	
725	 steps: training loss - 0.41198	, testing loss - 0.85428	
726	 steps: training loss - 0.33933	, testing loss - 0.85437	
727	 steps: training loss - 0.46002	, testing loss - 0.85463	
728	 steps: training loss - 0.34679	, testing loss - 0.85487	
729	 steps: training loss - 0.43703	, testing loss - 0.85529	
730	 steps: training loss - 0.42954	, testing loss - 0.85568	
731	 steps: training loss - 0.39193	, testing loss - 0.85614	
732	 steps: training loss - 0.27757	, testing loss - 0.85657	
733	 steps: training loss - 0.46387	, testing loss - 0.85709	
734	 steps: training loss - 0.46915	, testing loss - 0.85749	
735	 steps: training loss - 0.40179	, testing loss - 0.85785	
736	 steps: training loss - 0.28298	, testing loss - 0.85822	
737	 steps: training loss - 0.33798	, testing loss - 0.85857	
738	 steps: training loss - 0.40545	, testing loss - 0.85894	
739	 steps: training loss - 0.41183	, testing loss - 0.85941	
740	 steps: training loss - 0.41539	, testing loss - 0.85982	
741	 steps: training loss - 0.35333	, testing loss - 0.86022	
742	 steps: training loss - 0.44189	, testing loss - 0.86075	
743	 steps: training loss - 0.26836	, testing loss - 0.86120	
744	 steps: training loss - 0.40679	, testing loss - 0.86156	
745	 steps: training loss - 0.35594	, testing loss - 0.86192	
746	 steps: training loss - 0.31141	, testing loss - 0.86232	
747	 steps: training loss - 0.48444	, testing loss - 0.86276	
748	 steps: training loss - 0.28661	, testing loss - 0.86316	
749	 steps: training loss - 0.35108	, testing loss - 0.86358	
750	 steps: training loss - 0.48944	, testing loss - 0.86400	
751	 steps: training loss - 0.46962	, testing loss - 0.86431	
752	 steps: training loss - 0.44213	, testing loss - 0.86449	
753	 steps: training loss - 0.34282	, testing loss - 0.86465	
754	 steps: training loss - 0.36046	, testing loss - 0.86493	
755	 steps: training loss - 0.28864	, testing loss - 0.86536	
756	 steps: training loss - 0.41097	, testing loss - 0.86570	
757	 steps: training loss - 0.32407	, testing loss - 0.86615	
758	 steps: training loss - 0.38443	, testing loss - 0.86681	
759	 steps: training loss - 0.31798	, testing loss - 0.86734	
760	 steps: training loss - 0.49782	, testing loss - 0.86779	
761	 steps: training loss - 0.37525	, testing loss - 0.86804	
762	 steps: training loss - 0.41475	, testing loss - 0.86835	
763	 steps: training loss - 0.45042	, testing loss - 0.86866	
764	 steps: training loss - 0.38807	, testing loss - 0.86903	
765	 steps: training loss - 0.35450	, testing loss - 0.86944	
766	 steps: training loss - 0.39903	, testing loss - 0.86998	
767	 steps: training loss - 0.35760	, testing loss - 0.87052	
768	 steps: training loss - 0.38070	, testing loss - 0.87112	
769	 steps: training loss - 0.39214	, testing loss - 0.87175	
770	 steps: training loss - 0.33389	, testing loss - 0.87229	
771	 steps: training loss - 0.37575	, testing loss - 0.87283	
772	 steps: training loss - 0.27068	, testing loss - 0.87345	
773	 steps: training loss - 0.29511	, testing loss - 0.87403	
774	 steps: training loss - 0.30511	, testing loss - 0.87458	
775	 steps: training loss - 0.28281	, testing loss - 0.87522	
776	 steps: training loss - 0.44264	, testing loss - 0.87589	
777	 steps: training loss - 0.36404	, testing loss - 0.87656	
778	 steps: training loss - 0.33859	, testing loss - 0.87726	
779	 steps: training loss - 0.33933	, testing loss - 0.87786	
780	 steps: training loss - 0.32364	, testing loss - 0.87852	
781	 steps: training loss - 0.48924	, testing loss - 0.87928	
782	 steps: training loss - 0.45882	, testing loss - 0.87985	
783	 steps: training loss - 0.47285	, testing loss - 0.87999	
784	 steps: training loss - 0.50550	, testing loss - 0.87983	
785	 steps: training loss - 0.34778	, testing loss - 0.87967	
786	 steps: training loss - 0.30174	, testing loss - 0.87966	
787	 steps: training loss - 0.48807	, testing loss - 0.87985	
788	 steps: training loss - 0.30104	, testing loss - 0.88007	
789	 steps: training loss - 0.46401	, testing loss - 0.88030	
790	 steps: training loss - 0.41169	, testing loss - 0.88038	
791	 steps: training loss - 0.37285	, testing loss - 0.88036	
792	 steps: training loss - 0.27715	, testing loss - 0.88059	
793	 steps: training loss - 0.30336	, testing loss - 0.88084	
794	 steps: training loss - 0.43201	, testing loss - 0.88108	
795	 steps: training loss - 0.39110	, testing loss - 0.88129	
796	 steps: training loss - 0.39846	, testing loss - 0.88151	
797	 steps: training loss - 0.38191	, testing loss - 0.88171	
798	 steps: training loss - 0.45512	, testing loss - 0.88211	
799	 steps: training loss - 0.38088	, testing loss - 0.88255	
800	 steps: training loss - 0.33629	, testing loss - 0.88280	
801	 steps: training loss - 0.42434	, testing loss - 0.88301	
802	 steps: training loss - 0.34471	, testing loss - 0.88314	
803	 steps: training loss - 0.45063	, testing loss - 0.88323	
804	 steps: training loss - 0.47448	, testing loss - 0.88340	
805	 steps: training loss - 0.40214	, testing loss - 0.88361	
806	 steps: training loss - 0.52337	, testing loss - 0.88379	
807	 steps: training loss - 0.41569	, testing loss - 0.88398	
808	 steps: training loss - 0.32565	, testing loss - 0.88429	
809	 steps: training loss - 0.33140	, testing loss - 0.88463	
810	 steps: training loss - 0.38060	, testing loss - 0.88502	
811	 steps: training loss - 0.45728	, testing loss - 0.88538	
812	 steps: training loss - 0.44966	, testing loss - 0.88577	
813	 steps: training loss - 0.40938	, testing loss - 0.88614	
814	 steps: training loss - 0.35076	, testing loss - 0.88649	
815	 steps: training loss - 0.32512	, testing loss - 0.88691	
816	 steps: training loss - 0.50189	, testing loss - 0.88743	
817	 steps: training loss - 0.33488	, testing loss - 0.88788	
818	 steps: training loss - 0.48758	, testing loss - 0.88827	
819	 steps: training loss - 0.35047	, testing loss - 0.88856	
820	 steps: training loss - 0.38768	, testing loss - 0.88907	
821	 steps: training loss - 0.43481	, testing loss - 0.88978	
822	 steps: training loss - 0.34421	, testing loss - 0.89035	
823	 steps: training loss - 0.37490	, testing loss - 0.89107	
824	 steps: training loss - 0.41789	, testing loss - 0.89180	
825	 steps: training loss - 0.40604	, testing loss - 0.89241	
826	 steps: training loss - 0.35453	, testing loss - 0.89297	
827	 steps: training loss - 0.42534	, testing loss - 0.89338	
828	 steps: training loss - 0.38036	, testing loss - 0.89366	
829	 steps: training loss - 0.30371	, testing loss - 0.89408	
830	 steps: training loss - 0.47625	, testing loss - 0.89480	
831	 steps: training loss - 0.51144	, testing loss - 0.89553	
832	 steps: training loss - 0.35983	, testing loss - 0.89610	
833	 steps: training loss - 0.52371	, testing loss - 0.89667	
834	 steps: training loss - 0.38921	, testing loss - 0.89722	
835	 steps: training loss - 0.42410	, testing loss - 0.89767	
836	 steps: training loss - 0.37742	, testing loss - 0.89780	
837	 steps: training loss - 0.37517	, testing loss - 0.89804	
838	 steps: training loss - 0.39598	, testing loss - 0.89835	
839	 steps: training loss - 0.49087	, testing loss - 0.89844	
840	 steps: training loss - 0.53400	, testing loss - 0.89847	
841	 steps: training loss - 0.46960	, testing loss - 0.89854	
842	 steps: training loss - 0.38473	, testing loss - 0.89857	
843	 steps: training loss - 0.40093	, testing loss - 0.89849	
844	 steps: training loss - 0.57210	, testing loss - 0.89842	
845	 steps: training loss - 0.39113	, testing loss - 0.89818	
846	 steps: training loss - 0.42087	, testing loss - 0.89774	
847	 steps: training loss - 0.32813	, testing loss - 0.89735	
848	 steps: training loss - 0.36574	, testing loss - 0.89722	
849	 steps: training loss - 0.39021	, testing loss - 0.89746	
850	 steps: training loss - 0.45920	, testing loss - 0.89759	
851	 steps: training loss - 0.24626	, testing loss - 0.89786	
852	 steps: training loss - 0.34143	, testing loss - 0.89816	
853	 steps: training loss - 0.44633	, testing loss - 0.89848	
854	 steps: training loss - 0.34087	, testing loss - 0.89872	
855	 steps: training loss - 0.38864	, testing loss - 0.89893	
856	 steps: training loss - 0.27355	, testing loss - 0.89922	
857	 steps: training loss - 0.39775	, testing loss - 0.89946	
858	 steps: training loss - 0.28361	, testing loss - 0.89971	
859	 steps: training loss - 0.41985	, testing loss - 0.90003	
860	 steps: training loss - 0.22730	, testing loss - 0.90043	
861	 steps: training loss - 0.39962	, testing loss - 0.90079	
862	 steps: training loss - 0.26502	, testing loss - 0.90087	
863	 steps: training loss - 0.42307	, testing loss - 0.90081	
864	 steps: training loss - 0.45509	, testing loss - 0.90067	
865	 steps: training loss - 0.36195	, testing loss - 0.90047	
866	 steps: training loss - 0.45270	, testing loss - 0.90045	
867	 steps: training loss - 0.26447	, testing loss - 0.90041	
868	 steps: training loss - 0.40588	, testing loss - 0.90065	
869	 steps: training loss - 0.41781	, testing loss - 0.90072	
870	 steps: training loss - 0.46960	, testing loss - 0.90066	
871	 steps: training loss - 0.49073	, testing loss - 0.90048	
872	 steps: training loss - 0.35685	, testing loss - 0.90031	
873	 steps: training loss - 0.40546	, testing loss - 0.90030	
874	 steps: training loss - 0.49425	, testing loss - 0.90031	
875	 steps: training loss - 0.36113	, testing loss - 0.90031	
876	 steps: training loss - 0.45417	, testing loss - 0.90026	
877	 steps: training loss - 0.51524	, testing loss - 0.90022	
878	 steps: training loss - 0.29831	, testing loss - 0.90029	
879	 steps: training loss - 0.29265	, testing loss - 0.90035	
880	 steps: training loss - 0.33691	, testing loss - 0.90049	
881	 steps: training loss - 0.39261	, testing loss - 0.90082	
882	 steps: training loss - 0.38060	, testing loss - 0.90102	
883	 steps: training loss - 0.28374	, testing loss - 0.90124	
884	 steps: training loss - 0.41289	, testing loss - 0.90153	
885	 steps: training loss - 0.54529	, testing loss - 0.90158	
886	 steps: training loss - 0.38795	, testing loss - 0.90147	
887	 steps: training loss - 0.29676	, testing loss - 0.90149	
888	 steps: training loss - 0.29586	, testing loss - 0.90163	
889	 steps: training loss - 0.51864	, testing loss - 0.90189	
890	 steps: training loss - 0.37877	, testing loss - 0.90210	
891	 steps: training loss - 0.35003	, testing loss - 0.90214	
892	 steps: training loss - 0.40551	, testing loss - 0.90231	
893	 steps: training loss - 0.33759	, testing loss - 0.90246	
894	 steps: training loss - 0.35005	, testing loss - 0.90264	
895	 steps: training loss - 0.30907	, testing loss - 0.90298	
896	 steps: training loss - 0.38128	, testing loss - 0.90333	
897	 steps: training loss - 0.41636	, testing loss - 0.90367	
898	 steps: training loss - 0.36979	, testing loss - 0.90404	
899	 steps: training loss - 0.23962	, testing loss - 0.90443	
900	 steps: training loss - 0.41545	, testing loss - 0.90479	
901	 steps: training loss - 0.36556	, testing loss - 0.90485	
902	 steps: training loss - 0.37268	, testing loss - 0.90495	
903	 steps: training loss - 0.32012	, testing loss - 0.90531	
904	 steps: training loss - 0.44666	, testing loss - 0.90581	
905	 steps: training loss - 0.33761	, testing loss - 0.90629	
906	 steps: training loss - 0.45142	, testing loss - 0.90678	
907	 steps: training loss - 0.42094	, testing loss - 0.90719	
908	 steps: training loss - 0.32877	, testing loss - 0.90740	
909	 steps: training loss - 0.41911	, testing loss - 0.90773	
910	 steps: training loss - 0.32692	, testing loss - 0.90815	
911	 steps: training loss - 0.35767	, testing loss - 0.90868	
912	 steps: training loss - 0.50725	, testing loss - 0.90905	
913	 steps: training loss - 0.33749	, testing loss - 0.90931	
914	 steps: training loss - 0.39443	, testing loss - 0.90972	
915	 steps: training loss - 0.47181	, testing loss - 0.91034	
916	 steps: training loss - 0.45102	, testing loss - 0.91072	
917	 steps: training loss - 0.39582	, testing loss - 0.91097	
918	 steps: training loss - 0.24416	, testing loss - 0.91120	
919	 steps: training loss - 0.27667	, testing loss - 0.91155	
920	 steps: training loss - 0.39178	, testing loss - 0.91196	
921	 steps: training loss - 0.27446	, testing loss - 0.91209	
922	 steps: training loss - 0.36686	, testing loss - 0.91222	
923	 steps: training loss - 0.36811	, testing loss - 0.91256	
924	 steps: training loss - 0.41484	, testing loss - 0.91305	
925	 steps: training loss - 0.41332	, testing loss - 0.91359	
926	 steps: training loss - 0.42528	, testing loss - 0.91410	
927	 steps: training loss - 0.35692	, testing loss - 0.91451	
928	 steps: training loss - 0.28286	, testing loss - 0.91511	
929	 steps: training loss - 0.43907	, testing loss - 0.91570	
930	 steps: training loss - 0.43636	, testing loss - 0.91607	
931	 steps: training loss - 0.25512	, testing loss - 0.91625	
932	 steps: training loss - 0.29089	, testing loss - 0.91652	
933	 steps: training loss - 0.49446	, testing loss - 0.91692	
934	 steps: training loss - 0.46722	, testing loss - 0.91732	
935	 steps: training loss - 0.40017	, testing loss - 0.91758	
936	 steps: training loss - 0.26743	, testing loss - 0.91796	
937	 steps: training loss - 0.33197	, testing loss - 0.91849	
938	 steps: training loss - 0.32317	, testing loss - 0.91887	
939	 steps: training loss - 0.28457	, testing loss - 0.91925	
940	 steps: training loss - 0.36411	, testing loss - 0.91976	
941	 steps: training loss - 0.36688	, testing loss - 0.92031	
942	 steps: training loss - 0.36625	, testing loss - 0.92106	
943	 steps: training loss - 0.28328	, testing loss - 0.92160	
944	 steps: training loss - 0.32121	, testing loss - 0.92218	
945	 steps: training loss - 0.34700	, testing loss - 0.92282	
946	 steps: training loss - 0.35730	, testing loss - 0.92351	
947	 steps: training loss - 0.39598	, testing loss - 0.92424	
948	 steps: training loss - 0.31988	, testing loss - 0.92508	
949	 steps: training loss - 0.50246	, testing loss - 0.92598	
950	 steps: training loss - 0.48147	, testing loss - 0.92674	
951	 steps: training loss - 0.39869	, testing loss - 0.92728	
952	 steps: training loss - 0.30905	, testing loss - 0.92761	
953	 steps: training loss - 0.53464	, testing loss - 0.92797	
954	 steps: training loss - 0.37314	, testing loss - 0.92816	
955	 steps: training loss - 0.31730	, testing loss - 0.92837	
956	 steps: training loss - 0.29582	, testing loss - 0.92859	
957	 steps: training loss - 0.43348	, testing loss - 0.92884	
958	 steps: training loss - 0.26674	, testing loss - 0.92896	
959	 steps: training loss - 0.43796	, testing loss - 0.92924	
960	 steps: training loss - 0.34449	, testing loss - 0.92938	
961	 steps: training loss - 0.51989	, testing loss - 0.92960	
962	 steps: training loss - 0.47296	, testing loss - 0.92992	
963	 steps: training loss - 0.41112	, testing loss - 0.93023	
964	 steps: training loss - 0.53685	, testing loss - 0.93029	
965	 steps: training loss - 0.42107	, testing loss - 0.93035	
966	 steps: training loss - 0.38678	, testing loss - 0.93036	
967	 steps: training loss - 0.29058	, testing loss - 0.93043	
968	 steps: training loss - 0.42933	, testing loss - 0.93055	
969	 steps: training loss - 0.51830	, testing loss - 0.93060	
970	 steps: training loss - 0.28551	, testing loss - 0.93073	
971	 steps: training loss - 0.32825	, testing loss - 0.93091	
972	 steps: training loss - 0.26597	, testing loss - 0.93123	
973	 steps: training loss - 0.35684	, testing loss - 0.93181	
974	 steps: training loss - 0.46552	, testing loss - 0.93228	
975	 steps: training loss - 0.28984	, testing loss - 0.93267	
976	 steps: training loss - 0.20390	, testing loss - 0.93300	
977	 steps: training loss - 0.36019	, testing loss - 0.93331	
978	 steps: training loss - 0.31194	, testing loss - 0.93359	
979	 steps: training loss - 0.47689	, testing loss - 0.93392	
980	 steps: training loss - 0.49280	, testing loss - 0.93404	
981	 steps: training loss - 0.23282	, testing loss - 0.93401	
982	 steps: training loss - 0.43164	, testing loss - 0.93395	
983	 steps: training loss - 0.37030	, testing loss - 0.93394	
984	 steps: training loss - 0.31033	, testing loss - 0.93394	
985	 steps: training loss - 0.40696	, testing loss - 0.93397	
986	 steps: training loss - 0.45720	, testing loss - 0.93391	
987	 steps: training loss - 0.43957	, testing loss - 0.93391	
988	 steps: training loss - 0.33630	, testing loss - 0.93393	
989	 steps: training loss - 0.43929	, testing loss - 0.93403	
990	 steps: training loss - 0.41134	, testing loss - 0.93412	
991	 steps: training loss - 0.36504	, testing loss - 0.93427	
992	 steps: training loss - 0.30721	, testing loss - 0.93448	
993	 steps: training loss - 0.47073	, testing loss - 0.93492	
994	 steps: training loss - 0.34158	, testing loss - 0.93540	
995	 steps: training loss - 0.26599	, testing loss - 0.93598	
996	 steps: training loss - 0.39114	, testing loss - 0.93657	
997	 steps: training loss - 0.35535	, testing loss - 0.93685	
998	 steps: training loss - 0.27343	, testing loss - 0.93713	
999	 steps: training loss - 0.39079	, testing loss - 0.93749	
EVALUATION
----------
Test loss: 0.38497
MIMO Accuracies: 0.99900

LOOCV TRAINING INFO - is_lstm? True
LOOCV? -  True
-------------
TRAINING USERS -  ['a', 'c', 'e', 'g', 'h', 'i']
LEFT OUT -  b
-------------
actions ['bike', 'sit']
devices ['samsungold_1', 'samsungold_2']
step size 10
num_epochs 1000
batch size 12
feature size (x,y,z) 3
hidden size 20
number recurrent layers 2
number of classes 2
writing pickle to run9_model.p
writing accuracy + pickle to acc_run9_model.p
------------------------------------------------
0	 steps: training loss - 0.67299	, testing loss - 0.76786	
1	 steps: training loss - 0.75441	, testing loss - 0.76553	
2	 steps: training loss - 0.74116	, testing loss - 0.76252	
3	 steps: training loss - 0.71002	, testing loss - 0.75914	
4	 steps: training loss - 0.67486	, testing loss - 0.75578	
5	 steps: training loss - 0.77090	, testing loss - 0.75286	
6	 steps: training loss - 0.69568	, testing loss - 0.74969	
7	 steps: training loss - 0.66543	, testing loss - 0.74654	
8	 steps: training loss - 0.72780	, testing loss - 0.74386	
9	 steps: training loss - 0.81731	, testing loss - 0.74109	
10	 steps: training loss - 0.75951	, testing loss - 0.73783	
11	 steps: training loss - 0.73360	, testing loss - 0.73435	
12	 steps: training loss - 0.71585	, testing loss - 0.73085	
13	 steps: training loss - 0.70196	, testing loss - 0.72744	
14	 steps: training loss - 0.74352	, testing loss - 0.72423	
15	 steps: training loss - 0.79201	, testing loss - 0.72097	
16	 steps: training loss - 0.71586	, testing loss - 0.71745	
17	 steps: training loss - 0.69021	, testing loss - 0.71396	
18	 steps: training loss - 0.72703	, testing loss - 0.71071	
19	 steps: training loss - 0.71249	, testing loss - 0.70744	
20	 steps: training loss - 0.72820	, testing loss - 0.70428	
21	 steps: training loss - 0.70587	, testing loss - 0.70111	
22	 steps: training loss - 0.70579	, testing loss - 0.69799	
23	 steps: training loss - 0.75091	, testing loss - 0.69499	
24	 steps: training loss - 0.70972	, testing loss - 0.69176	
25	 steps: training loss - 0.68261	, testing loss - 0.68866	
26	 steps: training loss - 0.72628	, testing loss - 0.68589	
27	 steps: training loss - 0.70339	, testing loss - 0.68300	
28	 steps: training loss - 0.70981	, testing loss - 0.68016	
29	 steps: training loss - 0.69065	, testing loss - 0.67736	
30	 steps: training loss - 0.69259	, testing loss - 0.67484	
31	 steps: training loss - 0.70547	, testing loss - 0.67243	
32	 steps: training loss - 0.69807	, testing loss - 0.66996	
33	 steps: training loss - 0.67123	, testing loss - 0.66760	
34	 steps: training loss - 0.68207	, testing loss - 0.66570	
35	 steps: training loss - 0.69061	, testing loss - 0.66405	
36	 steps: training loss - 0.70747	, testing loss - 0.66242	
37	 steps: training loss - 0.67892	, testing loss - 0.66048	
38	 steps: training loss - 0.70348	, testing loss - 0.65865	
39	 steps: training loss - 0.68538	, testing loss - 0.65682	
40	 steps: training loss - 0.70327	, testing loss - 0.65500	
41	 steps: training loss - 0.66889	, testing loss - 0.65282	
42	 steps: training loss - 0.67608	, testing loss - 0.65070	
43	 steps: training loss - 0.68432	, testing loss - 0.64865	
44	 steps: training loss - 0.67246	, testing loss - 0.64670	
45	 steps: training loss - 0.68687	, testing loss - 0.64480	
46	 steps: training loss - 0.68813	, testing loss - 0.64283	
47	 steps: training loss - 0.66858	, testing loss - 0.64095	
48	 steps: training loss - 0.67495	, testing loss - 0.63900	
49	 steps: training loss - 0.67618	, testing loss - 0.63693	
50	 steps: training loss - 0.67787	, testing loss - 0.63489	
51	 steps: training loss - 0.67354	, testing loss - 0.63307	
52	 steps: training loss - 0.67784	, testing loss - 0.63153	
53	 steps: training loss - 0.67048	, testing loss - 0.63023	
54	 steps: training loss - 0.67864	, testing loss - 0.62911	
55	 steps: training loss - 0.67675	, testing loss - 0.62810	
56	 steps: training loss - 0.67271	, testing loss - 0.62707	
57	 steps: training loss - 0.66821	, testing loss - 0.62562	
58	 steps: training loss - 0.66608	, testing loss - 0.62436	
59	 steps: training loss - 0.65378	, testing loss - 0.62338	
60	 steps: training loss - 0.67973	, testing loss - 0.62212	
61	 steps: training loss - 0.66950	, testing loss - 0.62065	
62	 steps: training loss - 0.68233	, testing loss - 0.61909	
63	 steps: training loss - 0.66589	, testing loss - 0.61772	
64	 steps: training loss - 0.66105	, testing loss - 0.61626	
65	 steps: training loss - 0.65864	, testing loss - 0.61476	
66	 steps: training loss - 0.65637	, testing loss - 0.61323	
67	 steps: training loss - 0.66465	, testing loss - 0.61184	
68	 steps: training loss - 0.65665	, testing loss - 0.61065	
69	 steps: training loss - 0.66701	, testing loss - 0.60962	
70	 steps: training loss - 0.65513	, testing loss - 0.60870	
71	 steps: training loss - 0.66878	, testing loss - 0.60757	
72	 steps: training loss - 0.65354	, testing loss - 0.60631	
73	 steps: training loss - 0.67125	, testing loss - 0.60503	
74	 steps: training loss - 0.64576	, testing loss - 0.60390	
75	 steps: training loss - 0.63893	, testing loss - 0.60263	
76	 steps: training loss - 0.65374	, testing loss - 0.60106	
77	 steps: training loss - 0.64187	, testing loss - 0.59947	
78	 steps: training loss - 0.65781	, testing loss - 0.59785	
79	 steps: training loss - 0.63911	, testing loss - 0.59637	
80	 steps: training loss - 0.64782	, testing loss - 0.59495	
81	 steps: training loss - 0.63635	, testing loss - 0.59345	
82	 steps: training loss - 0.63476	, testing loss - 0.59186	
83	 steps: training loss - 0.67892	, testing loss - 0.59018	
84	 steps: training loss - 0.63852	, testing loss - 0.58887	
85	 steps: training loss - 0.66948	, testing loss - 0.58765	
86	 steps: training loss - 0.63103	, testing loss - 0.58668	
87	 steps: training loss - 0.66440	, testing loss - 0.58572	
88	 steps: training loss - 0.62266	, testing loss - 0.58466	
89	 steps: training loss - 0.67998	, testing loss - 0.58339	
90	 steps: training loss - 0.65115	, testing loss - 0.58232	
91	 steps: training loss - 0.62570	, testing loss - 0.58135	
92	 steps: training loss - 0.61866	, testing loss - 0.58023	
93	 steps: training loss - 0.64182	, testing loss - 0.57894	
94	 steps: training loss - 0.62786	, testing loss - 0.57761	
95	 steps: training loss - 0.60228	, testing loss - 0.57622	
96	 steps: training loss - 0.66168	, testing loss - 0.57465	
97	 steps: training loss - 0.62169	, testing loss - 0.57316	
98	 steps: training loss - 0.66380	, testing loss - 0.57175	
99	 steps: training loss - 0.64402	, testing loss - 0.57049	
100	 steps: training loss - 0.63434	, testing loss - 0.56931	
101	 steps: training loss - 0.67209	, testing loss - 0.56816	
102	 steps: training loss - 0.61714	, testing loss - 0.56728	
103	 steps: training loss - 0.60160	, testing loss - 0.56634	
104	 steps: training loss - 0.63811	, testing loss - 0.56512	
105	 steps: training loss - 0.66549	, testing loss - 0.56388	
106	 steps: training loss - 0.70913	, testing loss - 0.56288	
107	 steps: training loss - 0.66804	, testing loss - 0.56241	
108	 steps: training loss - 0.63342	, testing loss - 0.56221	
109	 steps: training loss - 0.60489	, testing loss - 0.56193	
110	 steps: training loss - 0.67746	, testing loss - 0.56143	
111	 steps: training loss - 0.63646	, testing loss - 0.56090	
112	 steps: training loss - 0.61069	, testing loss - 0.56040	
113	 steps: training loss - 0.65059	, testing loss - 0.55980	
114	 steps: training loss - 0.64672	, testing loss - 0.55924	
115	 steps: training loss - 0.69792	, testing loss - 0.55871	
116	 steps: training loss - 0.65768	, testing loss - 0.55831	
117	 steps: training loss - 0.59735	, testing loss - 0.55811	
118	 steps: training loss - 0.65195	, testing loss - 0.55762	
119	 steps: training loss - 0.64170	, testing loss - 0.55719	
120	 steps: training loss - 0.63888	, testing loss - 0.55688	
121	 steps: training loss - 0.64488	, testing loss - 0.55653	
122	 steps: training loss - 0.64266	, testing loss - 0.55603	
123	 steps: training loss - 0.64260	, testing loss - 0.55553	
124	 steps: training loss - 0.61404	, testing loss - 0.55513	
125	 steps: training loss - 0.62979	, testing loss - 0.55455	
126	 steps: training loss - 0.63764	, testing loss - 0.55369	
127	 steps: training loss - 0.63828	, testing loss - 0.55276	
128	 steps: training loss - 0.61947	, testing loss - 0.55197	
129	 steps: training loss - 0.62248	, testing loss - 0.55113	
130	 steps: training loss - 0.65939	, testing loss - 0.55015	
131	 steps: training loss - 0.62840	, testing loss - 0.54918	
132	 steps: training loss - 0.59388	, testing loss - 0.54822	
133	 steps: training loss - 0.63934	, testing loss - 0.54713	
134	 steps: training loss - 0.65447	, testing loss - 0.54623	
135	 steps: training loss - 0.62005	, testing loss - 0.54545	
136	 steps: training loss - 0.58797	, testing loss - 0.54458	
137	 steps: training loss - 0.65105	, testing loss - 0.54349	
138	 steps: training loss - 0.60385	, testing loss - 0.54259	
139	 steps: training loss - 0.65658	, testing loss - 0.54160	
140	 steps: training loss - 0.63427	, testing loss - 0.54071	
141	 steps: training loss - 0.63268	, testing loss - 0.54003	
142	 steps: training loss - 0.64420	, testing loss - 0.53940	
143	 steps: training loss - 0.60017	, testing loss - 0.53890	
144	 steps: training loss - 0.59276	, testing loss - 0.53819	
145	 steps: training loss - 0.59502	, testing loss - 0.53734	
146	 steps: training loss - 0.63827	, testing loss - 0.53638	
147	 steps: training loss - 0.59443	, testing loss - 0.53555	
148	 steps: training loss - 0.61794	, testing loss - 0.53467	
149	 steps: training loss - 0.64433	, testing loss - 0.53378	
150	 steps: training loss - 0.63318	, testing loss - 0.53307	
151	 steps: training loss - 0.66589	, testing loss - 0.53260	
152	 steps: training loss - 0.60900	, testing loss - 0.53236	
153	 steps: training loss - 0.59955	, testing loss - 0.53198	
154	 steps: training loss - 0.60298	, testing loss - 0.53132	
155	 steps: training loss - 0.59678	, testing loss - 0.53066	
156	 steps: training loss - 0.63361	, testing loss - 0.52978	
157	 steps: training loss - 0.62868	, testing loss - 0.52886	
158	 steps: training loss - 0.65184	, testing loss - 0.52815	
159	 steps: training loss - 0.62347	, testing loss - 0.52761	
160	 steps: training loss - 0.62436	, testing loss - 0.52718	
161	 steps: training loss - 0.60622	, testing loss - 0.52668	
162	 steps: training loss - 0.56524	, testing loss - 0.52602	
163	 steps: training loss - 0.60182	, testing loss - 0.52518	
164	 steps: training loss - 0.62773	, testing loss - 0.52425	
165	 steps: training loss - 0.62210	, testing loss - 0.52331	
166	 steps: training loss - 0.60076	, testing loss - 0.52241	
167	 steps: training loss - 0.62260	, testing loss - 0.52162	
168	 steps: training loss - 0.60080	, testing loss - 0.52098	
169	 steps: training loss - 0.63121	, testing loss - 0.52035	
170	 steps: training loss - 0.60010	, testing loss - 0.51980	
171	 steps: training loss - 0.66398	, testing loss - 0.51917	
172	 steps: training loss - 0.64699	, testing loss - 0.51868	
173	 steps: training loss - 0.64695	, testing loss - 0.51842	
174	 steps: training loss - 0.60088	, testing loss - 0.51827	
175	 steps: training loss - 0.61562	, testing loss - 0.51804	
176	 steps: training loss - 0.59410	, testing loss - 0.51762	
177	 steps: training loss - 0.61224	, testing loss - 0.51695	
178	 steps: training loss - 0.61500	, testing loss - 0.51615	
179	 steps: training loss - 0.57560	, testing loss - 0.51543	
180	 steps: training loss - 0.58887	, testing loss - 0.51468	
181	 steps: training loss - 0.60780	, testing loss - 0.51378	
182	 steps: training loss - 0.61924	, testing loss - 0.51296	
183	 steps: training loss - 0.59109	, testing loss - 0.51228	
184	 steps: training loss - 0.64915	, testing loss - 0.51164	
185	 steps: training loss - 0.66077	, testing loss - 0.51107	
186	 steps: training loss - 0.57885	, testing loss - 0.51061	
187	 steps: training loss - 0.59778	, testing loss - 0.51002	
188	 steps: training loss - 0.58560	, testing loss - 0.50927	
189	 steps: training loss - 0.61353	, testing loss - 0.50834	
190	 steps: training loss - 0.64264	, testing loss - 0.50741	
191	 steps: training loss - 0.64493	, testing loss - 0.50665	
192	 steps: training loss - 0.60008	, testing loss - 0.50614	
193	 steps: training loss - 0.64659	, testing loss - 0.50565	
194	 steps: training loss - 0.66441	, testing loss - 0.50517	
195	 steps: training loss - 0.64119	, testing loss - 0.50480	
196	 steps: training loss - 0.57493	, testing loss - 0.50462	
197	 steps: training loss - 0.64082	, testing loss - 0.50439	
198	 steps: training loss - 0.62054	, testing loss - 0.50418	
199	 steps: training loss - 0.56868	, testing loss - 0.50398	
200	 steps: training loss - 0.58962	, testing loss - 0.50364	
201	 steps: training loss - 0.62158	, testing loss - 0.50330	
202	 steps: training loss - 0.67413	, testing loss - 0.50298	
203	 steps: training loss - 0.63022	, testing loss - 0.50268	
204	 steps: training loss - 0.69412	, testing loss - 0.50241	
205	 steps: training loss - 0.57607	, testing loss - 0.50232	
206	 steps: training loss - 0.65732	, testing loss - 0.50204	
207	 steps: training loss - 0.58973	, testing loss - 0.50170	
208	 steps: training loss - 0.60027	, testing loss - 0.50126	
209	 steps: training loss - 0.56366	, testing loss - 0.50074	
210	 steps: training loss - 0.61956	, testing loss - 0.50016	
211	 steps: training loss - 0.66838	, testing loss - 0.49950	
212	 steps: training loss - 0.57711	, testing loss - 0.49897	
213	 steps: training loss - 0.59003	, testing loss - 0.49849	
214	 steps: training loss - 0.63203	, testing loss - 0.49799	
215	 steps: training loss - 0.62719	, testing loss - 0.49763	
216	 steps: training loss - 0.64091	, testing loss - 0.49723	
217	 steps: training loss - 0.56500	, testing loss - 0.49684	
218	 steps: training loss - 0.61239	, testing loss - 0.49635	
219	 steps: training loss - 0.55082	, testing loss - 0.49592	
220	 steps: training loss - 0.53152	, testing loss - 0.49528	
221	 steps: training loss - 0.59450	, testing loss - 0.49448	
222	 steps: training loss - 0.56973	, testing loss - 0.49385	
223	 steps: training loss - 0.58541	, testing loss - 0.49331	
224	 steps: training loss - 0.57984	, testing loss - 0.49274	
225	 steps: training loss - 0.61759	, testing loss - 0.49198	
226	 steps: training loss - 0.57990	, testing loss - 0.49116	
227	 steps: training loss - 0.61509	, testing loss - 0.49037	
228	 steps: training loss - 0.62650	, testing loss - 0.48962	
229	 steps: training loss - 0.58234	, testing loss - 0.48903	
230	 steps: training loss - 0.65637	, testing loss - 0.48847	
231	 steps: training loss - 0.56137	, testing loss - 0.48786	
232	 steps: training loss - 0.54142	, testing loss - 0.48718	
233	 steps: training loss - 0.59654	, testing loss - 0.48620	
234	 steps: training loss - 0.62180	, testing loss - 0.48531	
235	 steps: training loss - 0.63120	, testing loss - 0.48462	
236	 steps: training loss - 0.52713	, testing loss - 0.48420	
237	 steps: training loss - 0.63265	, testing loss - 0.48380	
238	 steps: training loss - 0.63143	, testing loss - 0.48345	
239	 steps: training loss - 0.59791	, testing loss - 0.48317	
240	 steps: training loss - 0.54678	, testing loss - 0.48274	
241	 steps: training loss - 0.57307	, testing loss - 0.48208	
242	 steps: training loss - 0.56517	, testing loss - 0.48154	
243	 steps: training loss - 0.62299	, testing loss - 0.48095	
244	 steps: training loss - 0.56713	, testing loss - 0.48031	
245	 steps: training loss - 0.55663	, testing loss - 0.47964	
246	 steps: training loss - 0.65979	, testing loss - 0.47888	
247	 steps: training loss - 0.57741	, testing loss - 0.47822	
248	 steps: training loss - 0.59214	, testing loss - 0.47760	
249	 steps: training loss - 0.61634	, testing loss - 0.47696	
250	 steps: training loss - 0.61723	, testing loss - 0.47636	
251	 steps: training loss - 0.59341	, testing loss - 0.47576	
252	 steps: training loss - 0.62681	, testing loss - 0.47508	
253	 steps: training loss - 0.57070	, testing loss - 0.47448	
254	 steps: training loss - 0.56865	, testing loss - 0.47386	
255	 steps: training loss - 0.60192	, testing loss - 0.47327	
256	 steps: training loss - 0.59369	, testing loss - 0.47275	
257	 steps: training loss - 0.55206	, testing loss - 0.47219	
258	 steps: training loss - 0.57044	, testing loss - 0.47152	
259	 steps: training loss - 0.56217	, testing loss - 0.47084	
260	 steps: training loss - 0.54172	, testing loss - 0.47005	
261	 steps: training loss - 0.57728	, testing loss - 0.46921	
262	 steps: training loss - 0.54642	, testing loss - 0.46832	
263	 steps: training loss - 0.60099	, testing loss - 0.46744	
264	 steps: training loss - 0.58845	, testing loss - 0.46672	
265	 steps: training loss - 0.54928	, testing loss - 0.46601	
266	 steps: training loss - 0.53998	, testing loss - 0.46529	
267	 steps: training loss - 0.56555	, testing loss - 0.46452	
268	 steps: training loss - 0.57841	, testing loss - 0.46373	
269	 steps: training loss - 0.63926	, testing loss - 0.46296	
270	 steps: training loss - 0.60908	, testing loss - 0.46233	
271	 steps: training loss - 0.59170	, testing loss - 0.46173	
272	 steps: training loss - 0.54489	, testing loss - 0.46115	
273	 steps: training loss - 0.70164	, testing loss - 0.46054	
274	 steps: training loss - 0.58554	, testing loss - 0.46009	
275	 steps: training loss - 0.53418	, testing loss - 0.45967	
276	 steps: training loss - 0.54622	, testing loss - 0.45920	
277	 steps: training loss - 0.60506	, testing loss - 0.45881	
278	 steps: training loss - 0.66164	, testing loss - 0.45842	
279	 steps: training loss - 0.55045	, testing loss - 0.45812	
280	 steps: training loss - 0.60839	, testing loss - 0.45775	
281	 steps: training loss - 0.58488	, testing loss - 0.45737	
282	 steps: training loss - 0.58954	, testing loss - 0.45697	
283	 steps: training loss - 0.54660	, testing loss - 0.45659	
284	 steps: training loss - 0.51168	, testing loss - 0.45613	
285	 steps: training loss - 0.56441	, testing loss - 0.45552	
286	 steps: training loss - 0.63105	, testing loss - 0.45492	
287	 steps: training loss - 0.57853	, testing loss - 0.45439	
288	 steps: training loss - 0.53325	, testing loss - 0.45386	
289	 steps: training loss - 0.58337	, testing loss - 0.45331	
290	 steps: training loss - 0.56255	, testing loss - 0.45270	
291	 steps: training loss - 0.64070	, testing loss - 0.45204	
292	 steps: training loss - 0.60569	, testing loss - 0.45156	
293	 steps: training loss - 0.57597	, testing loss - 0.45111	
294	 steps: training loss - 0.59057	, testing loss - 0.45057	
295	 steps: training loss - 0.60162	, testing loss - 0.45005	
296	 steps: training loss - 0.65924	, testing loss - 0.44956	
297	 steps: training loss - 0.55256	, testing loss - 0.44912	
298	 steps: training loss - 0.55777	, testing loss - 0.44870	
299	 steps: training loss - 0.60040	, testing loss - 0.44825	
300	 steps: training loss - 0.59676	, testing loss - 0.44784	
301	 steps: training loss - 0.61850	, testing loss - 0.44746	
302	 steps: training loss - 0.50693	, testing loss - 0.44715	
303	 steps: training loss - 0.69365	, testing loss - 0.44671	
304	 steps: training loss - 0.61409	, testing loss - 0.44643	
305	 steps: training loss - 0.50735	, testing loss - 0.44628	
306	 steps: training loss - 0.59329	, testing loss - 0.44602	
307	 steps: training loss - 0.54965	, testing loss - 0.44570	
308	 steps: training loss - 0.66211	, testing loss - 0.44537	
309	 steps: training loss - 0.53488	, testing loss - 0.44515	
310	 steps: training loss - 0.52517	, testing loss - 0.44484	
311	 steps: training loss - 0.53481	, testing loss - 0.44452	
312	 steps: training loss - 0.50388	, testing loss - 0.44410	
313	 steps: training loss - 0.62227	, testing loss - 0.44351	
314	 steps: training loss - 0.57903	, testing loss - 0.44299	
315	 steps: training loss - 0.60856	, testing loss - 0.44252	
316	 steps: training loss - 0.52629	, testing loss - 0.44202	
317	 steps: training loss - 0.59944	, testing loss - 0.44148	
318	 steps: training loss - 0.57778	, testing loss - 0.44095	
319	 steps: training loss - 0.54636	, testing loss - 0.44037	
320	 steps: training loss - 0.57719	, testing loss - 0.43982	
321	 steps: training loss - 0.62824	, testing loss - 0.43931	
322	 steps: training loss - 0.56211	, testing loss - 0.43890	
323	 steps: training loss - 0.58380	, testing loss - 0.43851	
324	 steps: training loss - 0.53818	, testing loss - 0.43814	
325	 steps: training loss - 0.60236	, testing loss - 0.43769	
326	 steps: training loss - 0.58169	, testing loss - 0.43729	
327	 steps: training loss - 0.60041	, testing loss - 0.43690	
328	 steps: training loss - 0.59910	, testing loss - 0.43654	
329	 steps: training loss - 0.58431	, testing loss - 0.43618	
330	 steps: training loss - 0.55986	, testing loss - 0.43582	
331	 steps: training loss - 0.56558	, testing loss - 0.43546	
332	 steps: training loss - 0.56220	, testing loss - 0.43512	
333	 steps: training loss - 0.54678	, testing loss - 0.43469	
334	 steps: training loss - 0.60743	, testing loss - 0.43413	
335	 steps: training loss - 0.52460	, testing loss - 0.43358	
336	 steps: training loss - 0.54495	, testing loss - 0.43300	
337	 steps: training loss - 0.61822	, testing loss - 0.43233	
338	 steps: training loss - 0.58798	, testing loss - 0.43174	
339	 steps: training loss - 0.57839	, testing loss - 0.43123	
340	 steps: training loss - 0.62380	, testing loss - 0.43069	
341	 steps: training loss - 0.61567	, testing loss - 0.43020	
342	 steps: training loss - 0.62242	, testing loss - 0.42981	
343	 steps: training loss - 0.54486	, testing loss - 0.42955	
344	 steps: training loss - 0.54241	, testing loss - 0.42928	
345	 steps: training loss - 0.57947	, testing loss - 0.42899	
346	 steps: training loss - 0.62760	, testing loss - 0.42868	
347	 steps: training loss - 0.53939	, testing loss - 0.42842	
348	 steps: training loss - 0.50806	, testing loss - 0.42815	
349	 steps: training loss - 0.62623	, testing loss - 0.42779	
350	 steps: training loss - 0.65049	, testing loss - 0.42752	
351	 steps: training loss - 0.59705	, testing loss - 0.42737	
352	 steps: training loss - 0.54435	, testing loss - 0.42722	
353	 steps: training loss - 0.55751	, testing loss - 0.42697	
354	 steps: training loss - 0.49544	, testing loss - 0.42676	
355	 steps: training loss - 0.47443	, testing loss - 0.42653	
356	 steps: training loss - 0.54146	, testing loss - 0.42622	
357	 steps: training loss - 0.56357	, testing loss - 0.42588	
358	 steps: training loss - 0.50894	, testing loss - 0.42546	
359	 steps: training loss - 0.56104	, testing loss - 0.42493	
360	 steps: training loss - 0.58501	, testing loss - 0.42438	
361	 steps: training loss - 0.64752	, testing loss - 0.42387	
362	 steps: training loss - 0.51221	, testing loss - 0.42350	
363	 steps: training loss - 0.52562	, testing loss - 0.42314	
364	 steps: training loss - 0.61080	, testing loss - 0.42274	
365	 steps: training loss - 0.59998	, testing loss - 0.42245	
366	 steps: training loss - 0.58363	, testing loss - 0.42222	
367	 steps: training loss - 0.58167	, testing loss - 0.42195	
368	 steps: training loss - 0.54336	, testing loss - 0.42178	
369	 steps: training loss - 0.58799	, testing loss - 0.42163	
370	 steps: training loss - 0.49565	, testing loss - 0.42148	
371	 steps: training loss - 0.48267	, testing loss - 0.42114	
372	 steps: training loss - 0.51180	, testing loss - 0.42056	
373	 steps: training loss - 0.60321	, testing loss - 0.42000	
374	 steps: training loss - 0.61835	, testing loss - 0.41956	
375	 steps: training loss - 0.61570	, testing loss - 0.41915	
376	 steps: training loss - 0.42872	, testing loss - 0.41881	
377	 steps: training loss - 0.57135	, testing loss - 0.41835	
378	 steps: training loss - 0.66372	, testing loss - 0.41778	
379	 steps: training loss - 0.51294	, testing loss - 0.41727	
380	 steps: training loss - 0.64436	, testing loss - 0.41666	
381	 steps: training loss - 0.52082	, testing loss - 0.41607	
382	 steps: training loss - 0.58459	, testing loss - 0.41557	
383	 steps: training loss - 0.45631	, testing loss - 0.41511	
384	 steps: training loss - 0.55330	, testing loss - 0.41467	
385	 steps: training loss - 0.64921	, testing loss - 0.41419	
386	 steps: training loss - 0.53180	, testing loss - 0.41383	
387	 steps: training loss - 0.48772	, testing loss - 0.41345	
388	 steps: training loss - 0.45264	, testing loss - 0.41298	
389	 steps: training loss - 0.55209	, testing loss - 0.41247	
390	 steps: training loss - 0.53271	, testing loss - 0.41192	
391	 steps: training loss - 0.51868	, testing loss - 0.41132	
392	 steps: training loss - 0.62134	, testing loss - 0.41081	
393	 steps: training loss - 0.56581	, testing loss - 0.41041	
394	 steps: training loss - 0.58240	, testing loss - 0.41002	
395	 steps: training loss - 0.56295	, testing loss - 0.40962	
396	 steps: training loss - 0.61827	, testing loss - 0.40926	
397	 steps: training loss - 0.49091	, testing loss - 0.40893	
398	 steps: training loss - 0.54960	, testing loss - 0.40850	
399	 steps: training loss - 0.53298	, testing loss - 0.40811	
400	 steps: training loss - 0.55714	, testing loss - 0.40782	
401	 steps: training loss - 0.56430	, testing loss - 0.40743	
402	 steps: training loss - 0.59075	, testing loss - 0.40707	
403	 steps: training loss - 0.53411	, testing loss - 0.40669	
404	 steps: training loss - 0.58614	, testing loss - 0.40626	
405	 steps: training loss - 0.51909	, testing loss - 0.40587	
406	 steps: training loss - 0.55963	, testing loss - 0.40544	
407	 steps: training loss - 0.65930	, testing loss - 0.40495	
408	 steps: training loss - 0.54731	, testing loss - 0.40447	
409	 steps: training loss - 0.58861	, testing loss - 0.40400	
410	 steps: training loss - 0.63130	, testing loss - 0.40356	
411	 steps: training loss - 0.55314	, testing loss - 0.40316	
412	 steps: training loss - 0.55367	, testing loss - 0.40275	
413	 steps: training loss - 0.61441	, testing loss - 0.40228	
414	 steps: training loss - 0.56043	, testing loss - 0.40185	
415	 steps: training loss - 0.51328	, testing loss - 0.40141	
416	 steps: training loss - 0.60790	, testing loss - 0.40093	
417	 steps: training loss - 0.57290	, testing loss - 0.40052	
418	 steps: training loss - 0.51136	, testing loss - 0.40012	
419	 steps: training loss - 0.59160	, testing loss - 0.39975	
420	 steps: training loss - 0.60452	, testing loss - 0.39944	
421	 steps: training loss - 0.59701	, testing loss - 0.39915	
422	 steps: training loss - 0.52432	, testing loss - 0.39888	
423	 steps: training loss - 0.52612	, testing loss - 0.39859	
424	 steps: training loss - 0.56328	, testing loss - 0.39822	
425	 steps: training loss - 0.59650	, testing loss - 0.39794	
426	 steps: training loss - 0.51881	, testing loss - 0.39771	
427	 steps: training loss - 0.54693	, testing loss - 0.39749	
428	 steps: training loss - 0.59658	, testing loss - 0.39728	
429	 steps: training loss - 0.55256	, testing loss - 0.39710	
430	 steps: training loss - 0.56285	, testing loss - 0.39697	
431	 steps: training loss - 0.55389	, testing loss - 0.39680	
432	 steps: training loss - 0.59943	, testing loss - 0.39659	
433	 steps: training loss - 0.61462	, testing loss - 0.39634	
434	 steps: training loss - 0.47864	, testing loss - 0.39614	
435	 steps: training loss - 0.51361	, testing loss - 0.39583	
436	 steps: training loss - 0.59808	, testing loss - 0.39542	
437	 steps: training loss - 0.46895	, testing loss - 0.39499	
438	 steps: training loss - 0.57450	, testing loss - 0.39452	
439	 steps: training loss - 0.56987	, testing loss - 0.39405	
440	 steps: training loss - 0.60008	, testing loss - 0.39361	
441	 steps: training loss - 0.62665	, testing loss - 0.39316	
442	 steps: training loss - 0.61607	, testing loss - 0.39276	
443	 steps: training loss - 0.46017	, testing loss - 0.39246	
444	 steps: training loss - 0.52217	, testing loss - 0.39208	
445	 steps: training loss - 0.68150	, testing loss - 0.39166	
446	 steps: training loss - 0.51451	, testing loss - 0.39136	
447	 steps: training loss - 0.49271	, testing loss - 0.39107	
448	 steps: training loss - 0.47644	, testing loss - 0.39079	
449	 steps: training loss - 0.51072	, testing loss - 0.39047	
450	 steps: training loss - 0.66248	, testing loss - 0.39014	
451	 steps: training loss - 0.50930	, testing loss - 0.38991	
452	 steps: training loss - 0.68497	, testing loss - 0.38965	
453	 steps: training loss - 0.52696	, testing loss - 0.38943	
454	 steps: training loss - 0.53402	, testing loss - 0.38918	
455	 steps: training loss - 0.55501	, testing loss - 0.38893	
456	 steps: training loss - 0.57651	, testing loss - 0.38864	
457	 steps: training loss - 0.56328	, testing loss - 0.38838	
458	 steps: training loss - 0.54806	, testing loss - 0.38815	
459	 steps: training loss - 0.69022	, testing loss - 0.38792	
460	 steps: training loss - 0.60756	, testing loss - 0.38779	
461	 steps: training loss - 0.58137	, testing loss - 0.38767	
462	 steps: training loss - 0.62922	, testing loss - 0.38751	
463	 steps: training loss - 0.50389	, testing loss - 0.38737	
464	 steps: training loss - 0.59718	, testing loss - 0.38713	
465	 steps: training loss - 0.54176	, testing loss - 0.38689	
466	 steps: training loss - 0.55960	, testing loss - 0.38662	
467	 steps: training loss - 0.52078	, testing loss - 0.38636	
468	 steps: training loss - 0.50274	, testing loss - 0.38605	
469	 steps: training loss - 0.51199	, testing loss - 0.38571	
470	 steps: training loss - 0.58424	, testing loss - 0.38531	
471	 steps: training loss - 0.59601	, testing loss - 0.38496	
472	 steps: training loss - 0.55240	, testing loss - 0.38464	
473	 steps: training loss - 0.57988	, testing loss - 0.38437	
474	 steps: training loss - 0.54776	, testing loss - 0.38416	
475	 steps: training loss - 0.60933	, testing loss - 0.38392	
476	 steps: training loss - 0.54634	, testing loss - 0.38374	
477	 steps: training loss - 0.46918	, testing loss - 0.38352	
478	 steps: training loss - 0.55882	, testing loss - 0.38329	
479	 steps: training loss - 0.52158	, testing loss - 0.38307	
480	 steps: training loss - 0.52834	, testing loss - 0.38289	
481	 steps: training loss - 0.59296	, testing loss - 0.38266	
482	 steps: training loss - 0.55912	, testing loss - 0.38243	
483	 steps: training loss - 0.56236	, testing loss - 0.38216	
484	 steps: training loss - 0.57269	, testing loss - 0.38190	
485	 steps: training loss - 0.59750	, testing loss - 0.38171	
486	 steps: training loss - 0.57116	, testing loss - 0.38158	
487	 steps: training loss - 0.52623	, testing loss - 0.38141	
488	 steps: training loss - 0.56454	, testing loss - 0.38110	
489	 steps: training loss - 0.59046	, testing loss - 0.38078	
490	 steps: training loss - 0.59247	, testing loss - 0.38042	
491	 steps: training loss - 0.50093	, testing loss - 0.38002	
492	 steps: training loss - 0.53615	, testing loss - 0.37959	
493	 steps: training loss - 0.52440	, testing loss - 0.37917	
494	 steps: training loss - 0.55962	, testing loss - 0.37875	
495	 steps: training loss - 0.57768	, testing loss - 0.37838	
496	 steps: training loss - 0.56015	, testing loss - 0.37807	
497	 steps: training loss - 0.56477	, testing loss - 0.37777	
498	 steps: training loss - 0.48629	, testing loss - 0.37751	
499	 steps: training loss - 0.57707	, testing loss - 0.37718	
500	 steps: training loss - 0.44553	, testing loss - 0.37690	
501	 steps: training loss - 0.52176	, testing loss - 0.37653	
502	 steps: training loss - 0.51105	, testing loss - 0.37617	
503	 steps: training loss - 0.61772	, testing loss - 0.37578	
504	 steps: training loss - 0.43975	, testing loss - 0.37544	
505	 steps: training loss - 0.45285	, testing loss - 0.37509	
506	 steps: training loss - 0.57053	, testing loss - 0.37467	
507	 steps: training loss - 0.49607	, testing loss - 0.37420	
508	 steps: training loss - 0.50481	, testing loss - 0.37378	
509	 steps: training loss - 0.55253	, testing loss - 0.37342	
510	 steps: training loss - 0.57252	, testing loss - 0.37310	
511	 steps: training loss - 0.52555	, testing loss - 0.37281	
512	 steps: training loss - 0.59133	, testing loss - 0.37244	
513	 steps: training loss - 0.53684	, testing loss - 0.37211	
514	 steps: training loss - 0.48929	, testing loss - 0.37183	
515	 steps: training loss - 0.58750	, testing loss - 0.37157	
516	 steps: training loss - 0.58362	, testing loss - 0.37133	
517	 steps: training loss - 0.45003	, testing loss - 0.37113	
518	 steps: training loss - 0.47020	, testing loss - 0.37089	
519	 steps: training loss - 0.57758	, testing loss - 0.37054	
520	 steps: training loss - 0.54692	, testing loss - 0.37027	
521	 steps: training loss - 0.50714	, testing loss - 0.37002	
522	 steps: training loss - 0.56345	, testing loss - 0.36972	
523	 steps: training loss - 0.50949	, testing loss - 0.36948	
524	 steps: training loss - 0.58092	, testing loss - 0.36921	
525	 steps: training loss - 0.53990	, testing loss - 0.36891	
526	 steps: training loss - 0.58888	, testing loss - 0.36862	
527	 steps: training loss - 0.57447	, testing loss - 0.36838	
528	 steps: training loss - 0.61559	, testing loss - 0.36819	
529	 steps: training loss - 0.59146	, testing loss - 0.36810	
530	 steps: training loss - 0.52264	, testing loss - 0.36805	
531	 steps: training loss - 0.60240	, testing loss - 0.36790	
532	 steps: training loss - 0.45012	, testing loss - 0.36774	
533	 steps: training loss - 0.58761	, testing loss - 0.36756	
534	 steps: training loss - 0.59956	, testing loss - 0.36739	
535	 steps: training loss - 0.53840	, testing loss - 0.36726	
536	 steps: training loss - 0.55209	, testing loss - 0.36707	
537	 steps: training loss - 0.56101	, testing loss - 0.36694	
538	 steps: training loss - 0.60057	, testing loss - 0.36681	
539	 steps: training loss - 0.57083	, testing loss - 0.36667	
540	 steps: training loss - 0.52288	, testing loss - 0.36643	
541	 steps: training loss - 0.50713	, testing loss - 0.36616	
542	 steps: training loss - 0.52760	, testing loss - 0.36587	
543	 steps: training loss - 0.56821	, testing loss - 0.36552	
544	 steps: training loss - 0.62165	, testing loss - 0.36518	
545	 steps: training loss - 0.47511	, testing loss - 0.36485	
546	 steps: training loss - 0.58944	, testing loss - 0.36450	
547	 steps: training loss - 0.50744	, testing loss - 0.36416	
548	 steps: training loss - 0.56408	, testing loss - 0.36384	
549	 steps: training loss - 0.58231	, testing loss - 0.36350	
550	 steps: training loss - 0.58454	, testing loss - 0.36320	
551	 steps: training loss - 0.42897	, testing loss - 0.36294	
552	 steps: training loss - 0.48921	, testing loss - 0.36265	
553	 steps: training loss - 0.57235	, testing loss - 0.36232	
554	 steps: training loss - 0.49013	, testing loss - 0.36205	
555	 steps: training loss - 0.58730	, testing loss - 0.36175	
556	 steps: training loss - 0.59591	, testing loss - 0.36149	
557	 steps: training loss - 0.55196	, testing loss - 0.36126	
558	 steps: training loss - 0.48025	, testing loss - 0.36100	
559	 steps: training loss - 0.49795	, testing loss - 0.36071	
560	 steps: training loss - 0.55555	, testing loss - 0.36040	
561	 steps: training loss - 0.42783	, testing loss - 0.36007	
562	 steps: training loss - 0.51045	, testing loss - 0.35972	
563	 steps: training loss - 0.63366	, testing loss - 0.35934	
564	 steps: training loss - 0.63284	, testing loss - 0.35902	
565	 steps: training loss - 0.48411	, testing loss - 0.35877	
566	 steps: training loss - 0.55914	, testing loss - 0.35849	
567	 steps: training loss - 0.55855	, testing loss - 0.35822	
568	 steps: training loss - 0.52244	, testing loss - 0.35796	
569	 steps: training loss - 0.64251	, testing loss - 0.35771	
570	 steps: training loss - 0.59752	, testing loss - 0.35753	
571	 steps: training loss - 0.50019	, testing loss - 0.35737	
572	 steps: training loss - 0.53383	, testing loss - 0.35721	
573	 steps: training loss - 0.54666	, testing loss - 0.35703	
574	 steps: training loss - 0.46757	, testing loss - 0.35682	
575	 steps: training loss - 0.51751	, testing loss - 0.35657	
576	 steps: training loss - 0.56417	, testing loss - 0.35631	
577	 steps: training loss - 0.50182	, testing loss - 0.35607	
578	 steps: training loss - 0.49561	, testing loss - 0.35589	
579	 steps: training loss - 0.48776	, testing loss - 0.35570	
580	 steps: training loss - 0.48910	, testing loss - 0.35544	
581	 steps: training loss - 0.48678	, testing loss - 0.35513	
582	 steps: training loss - 0.53638	, testing loss - 0.35485	
583	 steps: training loss - 0.50514	, testing loss - 0.35459	
584	 steps: training loss - 0.46607	, testing loss - 0.35438	
585	 steps: training loss - 0.58765	, testing loss - 0.35410	
586	 steps: training loss - 0.57276	, testing loss - 0.35381	
587	 steps: training loss - 0.46459	, testing loss - 0.35364	
588	 steps: training loss - 0.53321	, testing loss - 0.35341	
589	 steps: training loss - 0.47832	, testing loss - 0.35319	
590	 steps: training loss - 0.49989	, testing loss - 0.35294	
591	 steps: training loss - 0.58228	, testing loss - 0.35264	
592	 steps: training loss - 0.61031	, testing loss - 0.35230	
593	 steps: training loss - 0.57717	, testing loss - 0.35196	
594	 steps: training loss - 0.51832	, testing loss - 0.35167	
595	 steps: training loss - 0.51818	, testing loss - 0.35133	
596	 steps: training loss - 0.59448	, testing loss - 0.35096	
597	 steps: training loss - 0.41838	, testing loss - 0.35068	
598	 steps: training loss - 0.54080	, testing loss - 0.35038	
599	 steps: training loss - 0.50786	, testing loss - 0.35005	
600	 steps: training loss - 0.61067	, testing loss - 0.34968	
601	 steps: training loss - 0.52047	, testing loss - 0.34933	
602	 steps: training loss - 0.48274	, testing loss - 0.34902	
603	 steps: training loss - 0.59470	, testing loss - 0.34870	
604	 steps: training loss - 0.56337	, testing loss - 0.34844	
605	 steps: training loss - 0.52018	, testing loss - 0.34822	
606	 steps: training loss - 0.45836	, testing loss - 0.34804	
607	 steps: training loss - 0.44351	, testing loss - 0.34782	
608	 steps: training loss - 0.44499	, testing loss - 0.34750	
609	 steps: training loss - 0.54994	, testing loss - 0.34713	
610	 steps: training loss - 0.49224	, testing loss - 0.34681	
611	 steps: training loss - 0.51205	, testing loss - 0.34650	
612	 steps: training loss - 0.57040	, testing loss - 0.34620	
613	 steps: training loss - 0.53195	, testing loss - 0.34593	
614	 steps: training loss - 0.43412	, testing loss - 0.34569	
615	 steps: training loss - 0.53552	, testing loss - 0.34543	
616	 steps: training loss - 0.57758	, testing loss - 0.34516	
617	 steps: training loss - 0.55338	, testing loss - 0.34495	
618	 steps: training loss - 0.57748	, testing loss - 0.34480	
619	 steps: training loss - 0.53621	, testing loss - 0.34462	
620	 steps: training loss - 0.49043	, testing loss - 0.34441	
621	 steps: training loss - 0.57902	, testing loss - 0.34417	
622	 steps: training loss - 0.52303	, testing loss - 0.34398	
623	 steps: training loss - 0.53242	, testing loss - 0.34381	
624	 steps: training loss - 0.67199	, testing loss - 0.34361	
625	 steps: training loss - 0.46018	, testing loss - 0.34345	
626	 steps: training loss - 0.53490	, testing loss - 0.34328	
627	 steps: training loss - 0.52201	, testing loss - 0.34306	
628	 steps: training loss - 0.56814	, testing loss - 0.34278	
629	 steps: training loss - 0.50632	, testing loss - 0.34250	
630	 steps: training loss - 0.57130	, testing loss - 0.34221	
631	 steps: training loss - 0.49764	, testing loss - 0.34195	
632	 steps: training loss - 0.53539	, testing loss - 0.34171	
633	 steps: training loss - 0.43808	, testing loss - 0.34148	
634	 steps: training loss - 0.51957	, testing loss - 0.34121	
635	 steps: training loss - 0.51239	, testing loss - 0.34094	
636	 steps: training loss - 0.42539	, testing loss - 0.34068	
637	 steps: training loss - 0.51886	, testing loss - 0.34038	
638	 steps: training loss - 0.59829	, testing loss - 0.34011	
639	 steps: training loss - 0.45411	, testing loss - 0.33992	
640	 steps: training loss - 0.62304	, testing loss - 0.33968	
641	 steps: training loss - 0.44711	, testing loss - 0.33946	
642	 steps: training loss - 0.38151	, testing loss - 0.33920	
643	 steps: training loss - 0.44289	, testing loss - 0.33888	
644	 steps: training loss - 0.55162	, testing loss - 0.33852	
645	 steps: training loss - 0.53523	, testing loss - 0.33817	
646	 steps: training loss - 0.52273	, testing loss - 0.33782	
647	 steps: training loss - 0.50529	, testing loss - 0.33750	
648	 steps: training loss - 0.45772	, testing loss - 0.33719	
649	 steps: training loss - 0.53432	, testing loss - 0.33692	
650	 steps: training loss - 0.55485	, testing loss - 0.33670	
651	 steps: training loss - 0.42779	, testing loss - 0.33651	
652	 steps: training loss - 0.53876	, testing loss - 0.33624	
653	 steps: training loss - 0.57230	, testing loss - 0.33597	
654	 steps: training loss - 0.45799	, testing loss - 0.33575	
655	 steps: training loss - 0.45398	, testing loss - 0.33552	
656	 steps: training loss - 0.62325	, testing loss - 0.33526	
657	 steps: training loss - 0.50038	, testing loss - 0.33504	
658	 steps: training loss - 0.54691	, testing loss - 0.33480	
659	 steps: training loss - 0.46161	, testing loss - 0.33460	
660	 steps: training loss - 0.54362	, testing loss - 0.33439	
661	 steps: training loss - 0.50589	, testing loss - 0.33418	
662	 steps: training loss - 0.62240	, testing loss - 0.33398	
663	 steps: training loss - 0.43202	, testing loss - 0.33382	
664	 steps: training loss - 0.45809	, testing loss - 0.33362	
665	 steps: training loss - 0.49235	, testing loss - 0.33338	
666	 steps: training loss - 0.51052	, testing loss - 0.33312	
667	 steps: training loss - 0.60392	, testing loss - 0.33292	
668	 steps: training loss - 0.55720	, testing loss - 0.33281	
669	 steps: training loss - 0.53724	, testing loss - 0.33268	
670	 steps: training loss - 0.59117	, testing loss - 0.33252	
671	 steps: training loss - 0.43725	, testing loss - 0.33240	
672	 steps: training loss - 0.68116	, testing loss - 0.33223	
673	 steps: training loss - 0.65656	, testing loss - 0.33207	
674	 steps: training loss - 0.53312	, testing loss - 0.33192	
675	 steps: training loss - 0.55226	, testing loss - 0.33171	
676	 steps: training loss - 0.62213	, testing loss - 0.33152	
677	 steps: training loss - 0.52070	, testing loss - 0.33138	
678	 steps: training loss - 0.61105	, testing loss - 0.33125	
679	 steps: training loss - 0.56518	, testing loss - 0.33115	
680	 steps: training loss - 0.43312	, testing loss - 0.33106	
681	 steps: training loss - 0.36360	, testing loss - 0.33094	
682	 steps: training loss - 0.59626	, testing loss - 0.33072	
683	 steps: training loss - 0.43532	, testing loss - 0.33053	
684	 steps: training loss - 0.52034	, testing loss - 0.33031	
685	 steps: training loss - 0.50382	, testing loss - 0.33005	
686	 steps: training loss - 0.56749	, testing loss - 0.32977	
687	 steps: training loss - 0.46042	, testing loss - 0.32951	
688	 steps: training loss - 0.54390	, testing loss - 0.32924	
689	 steps: training loss - 0.56360	, testing loss - 0.32899	
690	 steps: training loss - 0.47285	, testing loss - 0.32880	
691	 steps: training loss - 0.54993	, testing loss - 0.32860	
692	 steps: training loss - 0.48393	, testing loss - 0.32841	
693	 steps: training loss - 0.55594	, testing loss - 0.32821	
694	 steps: training loss - 0.40720	, testing loss - 0.32799	
695	 steps: training loss - 0.54127	, testing loss - 0.32772	
696	 steps: training loss - 0.50583	, testing loss - 0.32744	
697	 steps: training loss - 0.52160	, testing loss - 0.32714	
698	 steps: training loss - 0.52773	, testing loss - 0.32684	
699	 steps: training loss - 0.45891	, testing loss - 0.32654	
700	 steps: training loss - 0.49789	, testing loss - 0.32622	
701	 steps: training loss - 0.42451	, testing loss - 0.32594	
702	 steps: training loss - 0.66995	, testing loss - 0.32565	
703	 steps: training loss - 0.46684	, testing loss - 0.32542	
704	 steps: training loss - 0.52209	, testing loss - 0.32518	
705	 steps: training loss - 0.54053	, testing loss - 0.32491	
706	 steps: training loss - 0.43625	, testing loss - 0.32465	
707	 steps: training loss - 0.48387	, testing loss - 0.32436	
708	 steps: training loss - 0.68166	, testing loss - 0.32405	
709	 steps: training loss - 0.57285	, testing loss - 0.32385	
710	 steps: training loss - 0.35664	, testing loss - 0.32370	
711	 steps: training loss - 0.59295	, testing loss - 0.32347	
712	 steps: training loss - 0.53054	, testing loss - 0.32323	
713	 steps: training loss - 0.60460	, testing loss - 0.32303	
714	 steps: training loss - 0.49122	, testing loss - 0.32285	
715	 steps: training loss - 0.55336	, testing loss - 0.32267	
716	 steps: training loss - 0.62403	, testing loss - 0.32251	
717	 steps: training loss - 0.49929	, testing loss - 0.32238	
718	 steps: training loss - 0.58653	, testing loss - 0.32227	
719	 steps: training loss - 0.46500	, testing loss - 0.32215	
720	 steps: training loss - 0.47951	, testing loss - 0.32198	
721	 steps: training loss - 0.40448	, testing loss - 0.32178	
722	 steps: training loss - 0.56863	, testing loss - 0.32156	
723	 steps: training loss - 0.54428	, testing loss - 0.32136	
724	 steps: training loss - 0.35649	, testing loss - 0.32122	
725	 steps: training loss - 0.47473	, testing loss - 0.32101	
726	 steps: training loss - 0.50852	, testing loss - 0.32079	
727	 steps: training loss - 0.52181	, testing loss - 0.32060	
728	 steps: training loss - 0.63652	, testing loss - 0.32043	
729	 steps: training loss - 0.50067	, testing loss - 0.32029	
730	 steps: training loss - 0.51862	, testing loss - 0.32017	
731	 steps: training loss - 0.63647	, testing loss - 0.32008	
732	 steps: training loss - 0.52545	, testing loss - 0.32007	
733	 steps: training loss - 0.70150	, testing loss - 0.32005	
734	 steps: training loss - 0.49244	, testing loss - 0.32008	
735	 steps: training loss - 0.50537	, testing loss - 0.32008	
736	 steps: training loss - 0.54231	, testing loss - 0.32005	
737	 steps: training loss - 0.55060	, testing loss - 0.31999	
738	 steps: training loss - 0.65707	, testing loss - 0.31993	
739	 steps: training loss - 0.41258	, testing loss - 0.31989	
740	 steps: training loss - 0.42762	, testing loss - 0.31982	
741	 steps: training loss - 0.43714	, testing loss - 0.31960	
742	 steps: training loss - 0.40991	, testing loss - 0.31930	
743	 steps: training loss - 0.62390	, testing loss - 0.31896	
744	 steps: training loss - 0.62323	, testing loss - 0.31867	
745	 steps: training loss - 0.60071	, testing loss - 0.31845	
746	 steps: training loss - 0.53112	, testing loss - 0.31826	
747	 steps: training loss - 0.49598	, testing loss - 0.31807	
748	 steps: training loss - 0.58227	, testing loss - 0.31787	
749	 steps: training loss - 0.57335	, testing loss - 0.31767	
750	 steps: training loss - 0.52789	, testing loss - 0.31745	
751	 steps: training loss - 0.53480	, testing loss - 0.31721	
752	 steps: training loss - 0.65680	, testing loss - 0.31699	
753	 steps: training loss - 0.34987	, testing loss - 0.31681	
754	 steps: training loss - 0.60070	, testing loss - 0.31655	
755	 steps: training loss - 0.46350	, testing loss - 0.31629	
756	 steps: training loss - 0.48184	, testing loss - 0.31601	
757	 steps: training loss - 0.48133	, testing loss - 0.31571	
758	 steps: training loss - 0.53068	, testing loss - 0.31539	
759	 steps: training loss - 0.52004	, testing loss - 0.31509	
760	 steps: training loss - 0.56568	, testing loss - 0.31480	
761	 steps: training loss - 0.37994	, testing loss - 0.31450	
762	 steps: training loss - 0.54157	, testing loss - 0.31418	
763	 steps: training loss - 0.49111	, testing loss - 0.31388	
764	 steps: training loss - 0.55235	, testing loss - 0.31361	
765	 steps: training loss - 0.43249	, testing loss - 0.31339	
766	 steps: training loss - 0.49197	, testing loss - 0.31317	
767	 steps: training loss - 0.47692	, testing loss - 0.31296	
768	 steps: training loss - 0.49977	, testing loss - 0.31275	
769	 steps: training loss - 0.65187	, testing loss - 0.31253	
770	 steps: training loss - 0.31845	, testing loss - 0.31241	
771	 steps: training loss - 0.28962	, testing loss - 0.31224	
772	 steps: training loss - 0.63655	, testing loss - 0.31199	
773	 steps: training loss - 0.44931	, testing loss - 0.31176	
774	 steps: training loss - 0.54206	, testing loss - 0.31149	
775	 steps: training loss - 0.41129	, testing loss - 0.31122	
776	 steps: training loss - 0.50715	, testing loss - 0.31089	
777	 steps: training loss - 0.46121	, testing loss - 0.31062	
778	 steps: training loss - 0.51620	, testing loss - 0.31038	
779	 steps: training loss - 0.56168	, testing loss - 0.31019	
780	 steps: training loss - 0.58246	, testing loss - 0.30998	
781	 steps: training loss - 0.53652	, testing loss - 0.30980	
782	 steps: training loss - 0.47699	, testing loss - 0.30963	
783	 steps: training loss - 0.52814	, testing loss - 0.30942	
784	 steps: training loss - 0.43221	, testing loss - 0.30922	
785	 steps: training loss - 0.45092	, testing loss - 0.30897	
786	 steps: training loss - 0.46050	, testing loss - 0.30870	
787	 steps: training loss - 0.53550	, testing loss - 0.30842	
788	 steps: training loss - 0.38043	, testing loss - 0.30818	
789	 steps: training loss - 0.45639	, testing loss - 0.30789	
790	 steps: training loss - 0.50658	, testing loss - 0.30760	
791	 steps: training loss - 0.55327	, testing loss - 0.30736	
792	 steps: training loss - 0.51095	, testing loss - 0.30715	
793	 steps: training loss - 0.47453	, testing loss - 0.30698	
794	 steps: training loss - 0.43574	, testing loss - 0.30680	
795	 steps: training loss - 0.39205	, testing loss - 0.30660	
796	 steps: training loss - 0.53446	, testing loss - 0.30637	
797	 steps: training loss - 0.49389	, testing loss - 0.30617	
798	 steps: training loss - 0.44771	, testing loss - 0.30595	
799	 steps: training loss - 0.51534	, testing loss - 0.30574	
800	 steps: training loss - 0.49766	, testing loss - 0.30558	
801	 steps: training loss - 0.44185	, testing loss - 0.30541	
802	 steps: training loss - 0.57064	, testing loss - 0.30522	
803	 steps: training loss - 0.49178	, testing loss - 0.30509	
804	 steps: training loss - 0.53143	, testing loss - 0.30495	
805	 steps: training loss - 0.51474	, testing loss - 0.30482	
806	 steps: training loss - 0.52823	, testing loss - 0.30463	
807	 steps: training loss - 0.45647	, testing loss - 0.30447	
808	 steps: training loss - 0.47189	, testing loss - 0.30432	
809	 steps: training loss - 0.45956	, testing loss - 0.30411	
810	 steps: training loss - 0.46616	, testing loss - 0.30389	
811	 steps: training loss - 0.53065	, testing loss - 0.30368	
812	 steps: training loss - 0.62312	, testing loss - 0.30353	
813	 steps: training loss - 0.44896	, testing loss - 0.30346	
814	 steps: training loss - 0.61387	, testing loss - 0.30342	
815	 steps: training loss - 0.58234	, testing loss - 0.30331	
816	 steps: training loss - 0.65526	, testing loss - 0.30316	
817	 steps: training loss - 0.47834	, testing loss - 0.30305	
818	 steps: training loss - 0.39905	, testing loss - 0.30285	
819	 steps: training loss - 0.59289	, testing loss - 0.30258	
820	 steps: training loss - 0.51999	, testing loss - 0.30232	
821	 steps: training loss - 0.54201	, testing loss - 0.30207	
822	 steps: training loss - 0.51557	, testing loss - 0.30187	
823	 steps: training loss - 0.53511	, testing loss - 0.30173	
824	 steps: training loss - 0.38835	, testing loss - 0.30158	
825	 steps: training loss - 0.60009	, testing loss - 0.30139	
826	 steps: training loss - 0.47973	, testing loss - 0.30121	
827	 steps: training loss - 0.47960	, testing loss - 0.30101	
828	 steps: training loss - 0.51143	, testing loss - 0.30081	
829	 steps: training loss - 0.57264	, testing loss - 0.30067	
830	 steps: training loss - 0.53294	, testing loss - 0.30057	
831	 steps: training loss - 0.44102	, testing loss - 0.30044	
832	 steps: training loss - 0.41632	, testing loss - 0.30028	
833	 steps: training loss - 0.41474	, testing loss - 0.30007	
834	 steps: training loss - 0.57858	, testing loss - 0.29980	
835	 steps: training loss - 0.57051	, testing loss - 0.29956	
836	 steps: training loss - 0.55371	, testing loss - 0.29938	
837	 steps: training loss - 0.48686	, testing loss - 0.29922	
838	 steps: training loss - 0.46895	, testing loss - 0.29906	
839	 steps: training loss - 0.60483	, testing loss - 0.29889	
840	 steps: training loss - 0.42996	, testing loss - 0.29881	
841	 steps: training loss - 0.63353	, testing loss - 0.29869	
842	 steps: training loss - 0.37885	, testing loss - 0.29862	
843	 steps: training loss - 0.42430	, testing loss - 0.29852	
844	 steps: training loss - 0.59534	, testing loss - 0.29835	
845	 steps: training loss - 0.51216	, testing loss - 0.29828	
846	 steps: training loss - 0.49961	, testing loss - 0.29826	
847	 steps: training loss - 0.48793	, testing loss - 0.29822	
848	 steps: training loss - 0.44437	, testing loss - 0.29821	
849	 steps: training loss - 0.63694	, testing loss - 0.29823	
850	 steps: training loss - 0.58347	, testing loss - 0.29830	
851	 steps: training loss - 0.50862	, testing loss - 0.29832	
852	 steps: training loss - 0.45475	, testing loss - 0.29828	
853	 steps: training loss - 0.50113	, testing loss - 0.29821	
854	 steps: training loss - 0.57705	, testing loss - 0.29815	
855	 steps: training loss - 0.49726	, testing loss - 0.29814	
856	 steps: training loss - 0.53222	, testing loss - 0.29806	
857	 steps: training loss - 0.41843	, testing loss - 0.29794	
858	 steps: training loss - 0.56271	, testing loss - 0.29781	
859	 steps: training loss - 0.37823	, testing loss - 0.29770	
860	 steps: training loss - 0.65270	, testing loss - 0.29755	
861	 steps: training loss - 0.49469	, testing loss - 0.29745	
862	 steps: training loss - 0.48836	, testing loss - 0.29738	
863	 steps: training loss - 0.46830	, testing loss - 0.29727	
864	 steps: training loss - 0.57703	, testing loss - 0.29714	
865	 steps: training loss - 0.49162	, testing loss - 0.29699	
866	 steps: training loss - 0.50756	, testing loss - 0.29680	
867	 steps: training loss - 0.45949	, testing loss - 0.29658	
868	 steps: training loss - 0.59191	, testing loss - 0.29638	
869	 steps: training loss - 0.48845	, testing loss - 0.29619	
870	 steps: training loss - 0.46273	, testing loss - 0.29595	
871	 steps: training loss - 0.60936	, testing loss - 0.29565	
872	 steps: training loss - 0.38637	, testing loss - 0.29537	
873	 steps: training loss - 0.47627	, testing loss - 0.29503	
874	 steps: training loss - 0.51476	, testing loss - 0.29463	
875	 steps: training loss - 0.55071	, testing loss - 0.29431	
876	 steps: training loss - 0.51572	, testing loss - 0.29407	
877	 steps: training loss - 0.52500	, testing loss - 0.29381	
878	 steps: training loss - 0.58626	, testing loss - 0.29359	
879	 steps: training loss - 0.41574	, testing loss - 0.29347	
880	 steps: training loss - 0.49388	, testing loss - 0.29337	
881	 steps: training loss - 0.63686	, testing loss - 0.29322	
882	 steps: training loss - 0.68764	, testing loss - 0.29311	
883	 steps: training loss - 0.65793	, testing loss - 0.29305	
884	 steps: training loss - 0.43613	, testing loss - 0.29307	
885	 steps: training loss - 0.61940	, testing loss - 0.29310	
886	 steps: training loss - 0.54583	, testing loss - 0.29312	
887	 steps: training loss - 0.53846	, testing loss - 0.29308	
888	 steps: training loss - 0.39945	, testing loss - 0.29303	
889	 steps: training loss - 0.48010	, testing loss - 0.29294	
890	 steps: training loss - 0.49604	, testing loss - 0.29282	
891	 steps: training loss - 0.73161	, testing loss - 0.29266	
892	 steps: training loss - 0.64827	, testing loss - 0.29254	
893	 steps: training loss - 0.46663	, testing loss - 0.29251	
894	 steps: training loss - 0.64059	, testing loss - 0.29247	
895	 steps: training loss - 0.65810	, testing loss - 0.29247	
896	 steps: training loss - 0.44617	, testing loss - 0.29246	
897	 steps: training loss - 0.45645	, testing loss - 0.29242	
898	 steps: training loss - 0.40842	, testing loss - 0.29230	
899	 steps: training loss - 0.55710	, testing loss - 0.29209	
900	 steps: training loss - 0.50760	, testing loss - 0.29191	
901	 steps: training loss - 0.51927	, testing loss - 0.29175	
902	 steps: training loss - 0.58331	, testing loss - 0.29158	
903	 steps: training loss - 0.56893	, testing loss - 0.29143	
904	 steps: training loss - 0.52194	, testing loss - 0.29130	
905	 steps: training loss - 0.47750	, testing loss - 0.29121	
906	 steps: training loss - 0.35443	, testing loss - 0.29110	
907	 steps: training loss - 0.46123	, testing loss - 0.29095	
908	 steps: training loss - 0.71536	, testing loss - 0.29078	
909	 steps: training loss - 0.35380	, testing loss - 0.29071	
910	 steps: training loss - 0.57296	, testing loss - 0.29062	
911	 steps: training loss - 0.52916	, testing loss - 0.29054	
912	 steps: training loss - 0.62845	, testing loss - 0.29050	
913	 steps: training loss - 0.38803	, testing loss - 0.29053	
914	 steps: training loss - 0.66987	, testing loss - 0.29049	
915	 steps: training loss - 0.40140	, testing loss - 0.29052	
916	 steps: training loss - 0.38268	, testing loss - 0.29049	
917	 steps: training loss - 0.41020	, testing loss - 0.29033	
918	 steps: training loss - 0.62859	, testing loss - 0.29014	
919	 steps: training loss - 0.59933	, testing loss - 0.29006	
920	 steps: training loss - 0.58641	, testing loss - 0.29001	
921	 steps: training loss - 0.50972	, testing loss - 0.28997	
922	 steps: training loss - 0.51356	, testing loss - 0.28988	
923	 steps: training loss - 0.46381	, testing loss - 0.28983	
924	 steps: training loss - 0.52625	, testing loss - 0.28970	
925	 steps: training loss - 0.46877	, testing loss - 0.28959	
926	 steps: training loss - 0.64899	, testing loss - 0.28954	
927	 steps: training loss - 0.50814	, testing loss - 0.28954	
928	 steps: training loss - 0.62357	, testing loss - 0.28955	
929	 steps: training loss - 0.52535	, testing loss - 0.28957	
930	 steps: training loss - 0.49025	, testing loss - 0.28958	
931	 steps: training loss - 0.53188	, testing loss - 0.28955	
932	 steps: training loss - 0.50129	, testing loss - 0.28957	
933	 steps: training loss - 0.41392	, testing loss - 0.28962	
934	 steps: training loss - 0.52573	, testing loss - 0.28964	
935	 steps: training loss - 0.63678	, testing loss - 0.28967	
936	 steps: training loss - 0.56510	, testing loss - 0.28976	
937	 steps: training loss - 0.36085	, testing loss - 0.28981	
938	 steps: training loss - 0.64712	, testing loss - 0.28980	
939	 steps: training loss - 0.40989	, testing loss - 0.28986	
940	 steps: training loss - 0.52552	, testing loss - 0.28985	
941	 steps: training loss - 0.35933	, testing loss - 0.28976	
942	 steps: training loss - 0.46125	, testing loss - 0.28956	
943	 steps: training loss - 0.61410	, testing loss - 0.28934	
944	 steps: training loss - 0.52611	, testing loss - 0.28914	
945	 steps: training loss - 0.54828	, testing loss - 0.28889	
946	 steps: training loss - 0.43260	, testing loss - 0.28868	
947	 steps: training loss - 0.57894	, testing loss - 0.28850	
948	 steps: training loss - 0.52825	, testing loss - 0.28832	
949	 steps: training loss - 0.54035	, testing loss - 0.28819	
950	 steps: training loss - 0.46912	, testing loss - 0.28803	
951	 steps: training loss - 0.55542	, testing loss - 0.28784	
952	 steps: training loss - 0.61930	, testing loss - 0.28770	
953	 steps: training loss - 0.47047	, testing loss - 0.28762	
954	 steps: training loss - 0.60542	, testing loss - 0.28754	
955	 steps: training loss - 0.48142	, testing loss - 0.28751	
956	 steps: training loss - 0.46122	, testing loss - 0.28751	
957	 steps: training loss - 0.50605	, testing loss - 0.28747	
958	 steps: training loss - 0.53189	, testing loss - 0.28740	
959	 steps: training loss - 0.50375	, testing loss - 0.28734	
960	 steps: training loss - 0.55725	, testing loss - 0.28727	
961	 steps: training loss - 0.50299	, testing loss - 0.28721	
962	 steps: training loss - 0.45907	, testing loss - 0.28716	
963	 steps: training loss - 0.37163	, testing loss - 0.28709	
964	 steps: training loss - 0.63928	, testing loss - 0.28696	
965	 steps: training loss - 0.62126	, testing loss - 0.28688	
966	 steps: training loss - 0.49757	, testing loss - 0.28687	
967	 steps: training loss - 0.40745	, testing loss - 0.28686	
968	 steps: training loss - 0.58760	, testing loss - 0.28678	
969	 steps: training loss - 0.60998	, testing loss - 0.28669	
970	 steps: training loss - 0.39789	, testing loss - 0.28667	
971	 steps: training loss - 0.68578	, testing loss - 0.28659	
972	 steps: training loss - 0.51592	, testing loss - 0.28651	
973	 steps: training loss - 0.48417	, testing loss - 0.28641	
974	 steps: training loss - 0.54443	, testing loss - 0.28629	
975	 steps: training loss - 0.57827	, testing loss - 0.28622	
976	 steps: training loss - 0.63911	, testing loss - 0.28616	
977	 steps: training loss - 0.48533	, testing loss - 0.28617	
978	 steps: training loss - 0.54974	, testing loss - 0.28617	
979	 steps: training loss - 0.53648	, testing loss - 0.28610	
980	 steps: training loss - 0.56591	, testing loss - 0.28604	
981	 steps: training loss - 0.58050	, testing loss - 0.28603	
982	 steps: training loss - 0.46453	, testing loss - 0.28604	
983	 steps: training loss - 0.38062	, testing loss - 0.28595	
984	 steps: training loss - 0.42409	, testing loss - 0.28579	
985	 steps: training loss - 0.62218	, testing loss - 0.28557	
986	 steps: training loss - 0.49632	, testing loss - 0.28539	
987	 steps: training loss - 0.55558	, testing loss - 0.28521	
988	 steps: training loss - 0.49806	, testing loss - 0.28510	
989	 steps: training loss - 0.61041	, testing loss - 0.28496	
990	 steps: training loss - 0.54502	, testing loss - 0.28490	
991	 steps: training loss - 0.57635	, testing loss - 0.28490	
992	 steps: training loss - 0.44623	, testing loss - 0.28498	
993	 steps: training loss - 0.56817	, testing loss - 0.28499	
994	 steps: training loss - 0.55531	, testing loss - 0.28490	
995	 steps: training loss - 0.65897	, testing loss - 0.28480	
996	 steps: training loss - 0.50523	, testing loss - 0.28469	
997	 steps: training loss - 0.46735	, testing loss - 0.28457	
998	 steps: training loss - 0.53475	, testing loss - 0.28447	
999	 steps: training loss - 0.42634	, testing loss - 0.28434	
EVALUATION
----------
Test loss: 0.53603
MIMO Accuracies: 0.99971

LOOCV TRAINING INFO - is_lstm? True
LOOCV? -  True
-------------
TRAINING USERS -  ['a', 'b', 'e', 'g', 'h', 'i']
LEFT OUT -  c
-------------
actions ['bike', 'sit']
devices ['samsungold_1', 'samsungold_2']
step size 10
num_epochs 1000
batch size 12
feature size (x,y,z) 3
hidden size 20
number recurrent layers 2
number of classes 2
writing pickle to run9_model.p
writing accuracy + pickle to acc_run9_model.p
------------------------------------------------
0	 steps: training loss - 0.66411	, testing loss - 0.69670	
1	 steps: training loss - 0.64952	, testing loss - 0.69665	
2	 steps: training loss - 0.65509	, testing loss - 0.69667	
3	 steps: training loss - 0.66904	, testing loss - 0.69676	
4	 steps: training loss - 0.65121	, testing loss - 0.69711	
5	 steps: training loss - 0.62808	, testing loss - 0.69755	
6	 steps: training loss - 0.69836	, testing loss - 0.69793	
7	 steps: training loss - 0.66177	, testing loss - 0.69834	
8	 steps: training loss - 0.63855	, testing loss - 0.69876	
9	 steps: training loss - 0.66111	, testing loss - 0.69919	
10	 steps: training loss - 0.65529	, testing loss - 0.69959	
11	 steps: training loss - 0.68033	, testing loss - 0.70000	
12	 steps: training loss - 0.64977	, testing loss - 0.70042	
13	 steps: training loss - 0.65070	, testing loss - 0.70081	
14	 steps: training loss - 0.65709	, testing loss - 0.70119	
15	 steps: training loss - 0.67666	, testing loss - 0.70158	
16	 steps: training loss - 0.63063	, testing loss - 0.70194	
17	 steps: training loss - 0.66596	, testing loss - 0.70232	
18	 steps: training loss - 0.64572	, testing loss - 0.70275	
19	 steps: training loss - 0.67683	, testing loss - 0.70320	
20	 steps: training loss - 0.66302	, testing loss - 0.70366	
21	 steps: training loss - 0.64453	, testing loss - 0.70412	
22	 steps: training loss - 0.60260	, testing loss - 0.70457	
23	 steps: training loss - 0.70600	, testing loss - 0.70503	
24	 steps: training loss - 0.65284	, testing loss - 0.70542	
25	 steps: training loss - 0.65024	, testing loss - 0.70579	
26	 steps: training loss - 0.67552	, testing loss - 0.70616	
27	 steps: training loss - 0.63739	, testing loss - 0.70655	
28	 steps: training loss - 0.63011	, testing loss - 0.70693	
29	 steps: training loss - 0.66575	, testing loss - 0.70732	
30	 steps: training loss - 0.68000	, testing loss - 0.70769	
31	 steps: training loss - 0.63568	, testing loss - 0.70805	
32	 steps: training loss - 0.62303	, testing loss - 0.70844	
33	 steps: training loss - 0.65263	, testing loss - 0.70884	
34	 steps: training loss - 0.65594	, testing loss - 0.70922	
35	 steps: training loss - 0.61648	, testing loss - 0.70959	
36	 steps: training loss - 0.65309	, testing loss - 0.70999	
37	 steps: training loss - 0.61930	, testing loss - 0.71038	
38	 steps: training loss - 0.64552	, testing loss - 0.71077	
39	 steps: training loss - 0.65536	, testing loss - 0.71114	
40	 steps: training loss - 0.65870	, testing loss - 0.71149	
41	 steps: training loss - 0.68063	, testing loss - 0.71184	
42	 steps: training loss - 0.64131	, testing loss - 0.71216	
43	 steps: training loss - 0.61000	, testing loss - 0.71247	
44	 steps: training loss - 0.63074	, testing loss - 0.71282	
45	 steps: training loss - 0.63230	, testing loss - 0.71320	
46	 steps: training loss - 0.60035	, testing loss - 0.71356	
47	 steps: training loss - 0.64231	, testing loss - 0.71391	
48	 steps: training loss - 0.63143	, testing loss - 0.71426	
49	 steps: training loss - 0.63926	, testing loss - 0.71464	
50	 steps: training loss - 0.62037	, testing loss - 0.71504	
51	 steps: training loss - 0.61747	, testing loss - 0.71546	
52	 steps: training loss - 0.63583	, testing loss - 0.71590	
53	 steps: training loss - 0.60655	, testing loss - 0.71632	
54	 steps: training loss - 0.57105	, testing loss - 0.71676	
55	 steps: training loss - 0.62287	, testing loss - 0.71726	
56	 steps: training loss - 0.63108	, testing loss - 0.71780	
57	 steps: training loss - 0.61213	, testing loss - 0.71833	
58	 steps: training loss - 0.65495	, testing loss - 0.71887	
59	 steps: training loss - 0.64256	, testing loss - 0.71936	
60	 steps: training loss - 0.62594	, testing loss - 0.71980	
61	 steps: training loss - 0.62536	, testing loss - 0.72023	
62	 steps: training loss - 0.64093	, testing loss - 0.72066	
63	 steps: training loss - 0.65164	, testing loss - 0.72107	
64	 steps: training loss - 0.60561	, testing loss - 0.72142	
65	 steps: training loss - 0.65483	, testing loss - 0.72179	
66	 steps: training loss - 0.60345	, testing loss - 0.72213	
67	 steps: training loss - 0.61826	, testing loss - 0.72250	
68	 steps: training loss - 0.65712	, testing loss - 0.72290	
69	 steps: training loss - 0.63591	, testing loss - 0.72324	
70	 steps: training loss - 0.63010	, testing loss - 0.72357	
71	 steps: training loss - 0.58640	, testing loss - 0.72391	
72	 steps: training loss - 0.60564	, testing loss - 0.72432	
73	 steps: training loss - 0.66849	, testing loss - 0.72477	
74	 steps: training loss - 0.66611	, testing loss - 0.72517	
75	 steps: training loss - 0.60410	, testing loss - 0.72549	
76	 steps: training loss - 0.60563	, testing loss - 0.72582	
77	 steps: training loss - 0.61380	, testing loss - 0.72618	
78	 steps: training loss - 0.62941	, testing loss - 0.72654	
79	 steps: training loss - 0.61023	, testing loss - 0.72690	
80	 steps: training loss - 0.61771	, testing loss - 0.72726	
81	 steps: training loss - 0.61609	, testing loss - 0.72765	
82	 steps: training loss - 0.62217	, testing loss - 0.72807	
83	 steps: training loss - 0.59839	, testing loss - 0.72849	
84	 steps: training loss - 0.59539	, testing loss - 0.72892	
85	 steps: training loss - 0.60687	, testing loss - 0.72934	
86	 steps: training loss - 0.58667	, testing loss - 0.72978	
87	 steps: training loss - 0.61393	, testing loss - 0.73026	
88	 steps: training loss - 0.58680	, testing loss - 0.73074	
89	 steps: training loss - 0.57680	, testing loss - 0.73124	
90	 steps: training loss - 0.60571	, testing loss - 0.73179	
91	 steps: training loss - 0.60207	, testing loss - 0.73238	
92	 steps: training loss - 0.58660	, testing loss - 0.73295	
93	 steps: training loss - 0.62132	, testing loss - 0.73356	
94	 steps: training loss - 0.58762	, testing loss - 0.73419	
95	 steps: training loss - 0.62249	, testing loss - 0.73482	
96	 steps: training loss - 0.63238	, testing loss - 0.73542	
97	 steps: training loss - 0.61028	, testing loss - 0.73596	
98	 steps: training loss - 0.63458	, testing loss - 0.73646	
99	 steps: training loss - 0.58916	, testing loss - 0.73690	
100	 steps: training loss - 0.61545	, testing loss - 0.73735	
101	 steps: training loss - 0.64356	, testing loss - 0.73773	
102	 steps: training loss - 0.65680	, testing loss - 0.73805	
103	 steps: training loss - 0.57586	, testing loss - 0.73828	
104	 steps: training loss - 0.59091	, testing loss - 0.73858	
105	 steps: training loss - 0.57922	, testing loss - 0.73889	
106	 steps: training loss - 0.60694	, testing loss - 0.73925	
107	 steps: training loss - 0.63555	, testing loss - 0.73962	
108	 steps: training loss - 0.55286	, testing loss - 0.73994	
109	 steps: training loss - 0.63014	, testing loss - 0.74031	
110	 steps: training loss - 0.57710	, testing loss - 0.74063	
111	 steps: training loss - 0.57176	, testing loss - 0.74097	
112	 steps: training loss - 0.59719	, testing loss - 0.74139	
113	 steps: training loss - 0.59265	, testing loss - 0.74181	
114	 steps: training loss - 0.61867	, testing loss - 0.74219	
115	 steps: training loss - 0.55768	, testing loss - 0.74253	
116	 steps: training loss - 0.56466	, testing loss - 0.74294	
117	 steps: training loss - 0.57396	, testing loss - 0.74339	
118	 steps: training loss - 0.56095	, testing loss - 0.74388	
119	 steps: training loss - 0.54865	, testing loss - 0.74443	
120	 steps: training loss - 0.53711	, testing loss - 0.74503	
121	 steps: training loss - 0.60746	, testing loss - 0.74570	
122	 steps: training loss - 0.52805	, testing loss - 0.74634	
123	 steps: training loss - 0.64300	, testing loss - 0.74705	
124	 steps: training loss - 0.59077	, testing loss - 0.74767	
125	 steps: training loss - 0.58141	, testing loss - 0.74821	
126	 steps: training loss - 0.56965	, testing loss - 0.74874	
127	 steps: training loss - 0.58363	, testing loss - 0.74929	
128	 steps: training loss - 0.58114	, testing loss - 0.74986	
129	 steps: training loss - 0.53477	, testing loss - 0.75044	
130	 steps: training loss - 0.52205	, testing loss - 0.75109	
131	 steps: training loss - 0.58338	, testing loss - 0.75186	
132	 steps: training loss - 0.62463	, testing loss - 0.75265	
133	 steps: training loss - 0.62046	, testing loss - 0.75337	
134	 steps: training loss - 0.58944	, testing loss - 0.75389	
135	 steps: training loss - 0.63654	, testing loss - 0.75435	
136	 steps: training loss - 0.54781	, testing loss - 0.75468	
137	 steps: training loss - 0.56002	, testing loss - 0.75500	
138	 steps: training loss - 0.58365	, testing loss - 0.75533	
139	 steps: training loss - 0.57804	, testing loss - 0.75568	
140	 steps: training loss - 0.59445	, testing loss - 0.75605	
141	 steps: training loss - 0.59040	, testing loss - 0.75646	
142	 steps: training loss - 0.57017	, testing loss - 0.75683	
143	 steps: training loss - 0.54810	, testing loss - 0.75719	
144	 steps: training loss - 0.55459	, testing loss - 0.75762	
145	 steps: training loss - 0.61057	, testing loss - 0.75810	
146	 steps: training loss - 0.57820	, testing loss - 0.75852	
147	 steps: training loss - 0.62846	, testing loss - 0.75889	
148	 steps: training loss - 0.62972	, testing loss - 0.75920	
149	 steps: training loss - 0.57283	, testing loss - 0.75942	
150	 steps: training loss - 0.52702	, testing loss - 0.75962	
151	 steps: training loss - 0.52275	, testing loss - 0.75997	
152	 steps: training loss - 0.57672	, testing loss - 0.76045	
153	 steps: training loss - 0.55218	, testing loss - 0.76096	
154	 steps: training loss - 0.60451	, testing loss - 0.76146	
155	 steps: training loss - 0.57877	, testing loss - 0.76186	
156	 steps: training loss - 0.55224	, testing loss - 0.76215	
157	 steps: training loss - 0.61316	, testing loss - 0.76251	
158	 steps: training loss - 0.65123	, testing loss - 0.76279	
159	 steps: training loss - 0.56549	, testing loss - 0.76290	
160	 steps: training loss - 0.57853	, testing loss - 0.76307	
161	 steps: training loss - 0.54849	, testing loss - 0.76331	
162	 steps: training loss - 0.48749	, testing loss - 0.76360	
163	 steps: training loss - 0.58332	, testing loss - 0.76402	
164	 steps: training loss - 0.52165	, testing loss - 0.76443	
165	 steps: training loss - 0.55786	, testing loss - 0.76486	
166	 steps: training loss - 0.54560	, testing loss - 0.76533	
167	 steps: training loss - 0.54196	, testing loss - 0.76581	
168	 steps: training loss - 0.51483	, testing loss - 0.76634	
169	 steps: training loss - 0.61366	, testing loss - 0.76692	
170	 steps: training loss - 0.57011	, testing loss - 0.76739	
171	 steps: training loss - 0.59005	, testing loss - 0.76783	
172	 steps: training loss - 0.57684	, testing loss - 0.76825	
173	 steps: training loss - 0.58765	, testing loss - 0.76866	
174	 steps: training loss - 0.57149	, testing loss - 0.76901	
175	 steps: training loss - 0.65664	, testing loss - 0.76932	
176	 steps: training loss - 0.53239	, testing loss - 0.76955	
177	 steps: training loss - 0.55507	, testing loss - 0.76985	
178	 steps: training loss - 0.54190	, testing loss - 0.77021	
179	 steps: training loss - 0.56709	, testing loss - 0.77058	
180	 steps: training loss - 0.61647	, testing loss - 0.77101	
181	 steps: training loss - 0.56174	, testing loss - 0.77134	
182	 steps: training loss - 0.53259	, testing loss - 0.77168	
183	 steps: training loss - 0.57542	, testing loss - 0.77204	
184	 steps: training loss - 0.54252	, testing loss - 0.77240	
185	 steps: training loss - 0.52298	, testing loss - 0.77282	
186	 steps: training loss - 0.52842	, testing loss - 0.77328	
187	 steps: training loss - 0.54038	, testing loss - 0.77378	
188	 steps: training loss - 0.58552	, testing loss - 0.77428	
189	 steps: training loss - 0.52387	, testing loss - 0.77475	
190	 steps: training loss - 0.53683	, testing loss - 0.77527	
191	 steps: training loss - 0.58572	, testing loss - 0.77582	
192	 steps: training loss - 0.56597	, testing loss - 0.77638	
193	 steps: training loss - 0.52666	, testing loss - 0.77693	
194	 steps: training loss - 0.49549	, testing loss - 0.77747	
195	 steps: training loss - 0.51692	, testing loss - 0.77805	
196	 steps: training loss - 0.51525	, testing loss - 0.77869	
197	 steps: training loss - 0.53780	, testing loss - 0.77939	
198	 steps: training loss - 0.54790	, testing loss - 0.78013	
199	 steps: training loss - 0.56175	, testing loss - 0.78091	
200	 steps: training loss - 0.51418	, testing loss - 0.78173	
201	 steps: training loss - 0.50334	, testing loss - 0.78257	
202	 steps: training loss - 0.51340	, testing loss - 0.78347	
203	 steps: training loss - 0.54849	, testing loss - 0.78449	
204	 steps: training loss - 0.54835	, testing loss - 0.78551	
205	 steps: training loss - 0.50249	, testing loss - 0.78654	
206	 steps: training loss - 0.50811	, testing loss - 0.78757	
207	 steps: training loss - 0.56747	, testing loss - 0.78849	
208	 steps: training loss - 0.54341	, testing loss - 0.78921	
209	 steps: training loss - 0.56814	, testing loss - 0.78978	
210	 steps: training loss - 0.52121	, testing loss - 0.79028	
211	 steps: training loss - 0.53962	, testing loss - 0.79074	
212	 steps: training loss - 0.55082	, testing loss - 0.79111	
213	 steps: training loss - 0.53011	, testing loss - 0.79145	
214	 steps: training loss - 0.51907	, testing loss - 0.79191	
215	 steps: training loss - 0.45590	, testing loss - 0.79232	
216	 steps: training loss - 0.53427	, testing loss - 0.79286	
217	 steps: training loss - 0.52531	, testing loss - 0.79340	
218	 steps: training loss - 0.53892	, testing loss - 0.79402	
219	 steps: training loss - 0.56454	, testing loss - 0.79461	
220	 steps: training loss - 0.47219	, testing loss - 0.79513	
221	 steps: training loss - 0.63571	, testing loss - 0.79572	
222	 steps: training loss - 0.49890	, testing loss - 0.79615	
223	 steps: training loss - 0.51519	, testing loss - 0.79663	
224	 steps: training loss - 0.55675	, testing loss - 0.79712	
225	 steps: training loss - 0.51870	, testing loss - 0.79750	
226	 steps: training loss - 0.56905	, testing loss - 0.79795	
227	 steps: training loss - 0.56788	, testing loss - 0.79837	
228	 steps: training loss - 0.59260	, testing loss - 0.79868	
229	 steps: training loss - 0.60643	, testing loss - 0.79886	
230	 steps: training loss - 0.57905	, testing loss - 0.79886	
231	 steps: training loss - 0.53725	, testing loss - 0.79884	
232	 steps: training loss - 0.52230	, testing loss - 0.79893	
233	 steps: training loss - 0.58568	, testing loss - 0.79907	
234	 steps: training loss - 0.49857	, testing loss - 0.79928	
235	 steps: training loss - 0.55251	, testing loss - 0.79954	
236	 steps: training loss - 0.53180	, testing loss - 0.79979	
237	 steps: training loss - 0.47114	, testing loss - 0.80009	
238	 steps: training loss - 0.51182	, testing loss - 0.80048	
239	 steps: training loss - 0.53462	, testing loss - 0.80096	
240	 steps: training loss - 0.53620	, testing loss - 0.80153	
241	 steps: training loss - 0.53329	, testing loss - 0.80211	
242	 steps: training loss - 0.57216	, testing loss - 0.80267	
243	 steps: training loss - 0.45582	, testing loss - 0.80317	
244	 steps: training loss - 0.47954	, testing loss - 0.80373	
245	 steps: training loss - 0.46213	, testing loss - 0.80443	
246	 steps: training loss - 0.51628	, testing loss - 0.80517	
247	 steps: training loss - 0.50569	, testing loss - 0.80583	
248	 steps: training loss - 0.49698	, testing loss - 0.80644	
249	 steps: training loss - 0.51575	, testing loss - 0.80703	
250	 steps: training loss - 0.53783	, testing loss - 0.80763	
251	 steps: training loss - 0.52515	, testing loss - 0.80828	
252	 steps: training loss - 0.47190	, testing loss - 0.80888	
253	 steps: training loss - 0.53939	, testing loss - 0.80942	
254	 steps: training loss - 0.53180	, testing loss - 0.80990	
255	 steps: training loss - 0.50236	, testing loss - 0.81035	
256	 steps: training loss - 0.56124	, testing loss - 0.81080	
257	 steps: training loss - 0.55795	, testing loss - 0.81124	
258	 steps: training loss - 0.49246	, testing loss - 0.81158	
259	 steps: training loss - 0.49366	, testing loss - 0.81191	
260	 steps: training loss - 0.53848	, testing loss - 0.81238	
261	 steps: training loss - 0.52481	, testing loss - 0.81288	
262	 steps: training loss - 0.49833	, testing loss - 0.81345	
263	 steps: training loss - 0.49491	, testing loss - 0.81414	
264	 steps: training loss - 0.47645	, testing loss - 0.81477	
265	 steps: training loss - 0.52141	, testing loss - 0.81540	
266	 steps: training loss - 0.52940	, testing loss - 0.81612	
267	 steps: training loss - 0.49452	, testing loss - 0.81679	
268	 steps: training loss - 0.59808	, testing loss - 0.81747	
269	 steps: training loss - 0.49233	, testing loss - 0.81801	
270	 steps: training loss - 0.55006	, testing loss - 0.81847	
271	 steps: training loss - 0.51587	, testing loss - 0.81894	
272	 steps: training loss - 0.49823	, testing loss - 0.81939	
273	 steps: training loss - 0.41417	, testing loss - 0.81997	
274	 steps: training loss - 0.46216	, testing loss - 0.82072	
275	 steps: training loss - 0.49669	, testing loss - 0.82161	
276	 steps: training loss - 0.56121	, testing loss - 0.82257	
277	 steps: training loss - 0.51349	, testing loss - 0.82341	
278	 steps: training loss - 0.45870	, testing loss - 0.82419	
279	 steps: training loss - 0.55021	, testing loss - 0.82492	
280	 steps: training loss - 0.50207	, testing loss - 0.82555	
281	 steps: training loss - 0.50270	, testing loss - 0.82609	
282	 steps: training loss - 0.46731	, testing loss - 0.82667	
283	 steps: training loss - 0.45506	, testing loss - 0.82728	
284	 steps: training loss - 0.51734	, testing loss - 0.82792	
285	 steps: training loss - 0.52165	, testing loss - 0.82850	
286	 steps: training loss - 0.47056	, testing loss - 0.82908	
287	 steps: training loss - 0.50002	, testing loss - 0.82967	
288	 steps: training loss - 0.60214	, testing loss - 0.83019	
289	 steps: training loss - 0.45069	, testing loss - 0.83063	
290	 steps: training loss - 0.51624	, testing loss - 0.83101	
291	 steps: training loss - 0.47101	, testing loss - 0.83127	
292	 steps: training loss - 0.51375	, testing loss - 0.83153	
293	 steps: training loss - 0.49453	, testing loss - 0.83174	
294	 steps: training loss - 0.42202	, testing loss - 0.83194	
295	 steps: training loss - 0.47096	, testing loss - 0.83227	
296	 steps: training loss - 0.46961	, testing loss - 0.83269	
297	 steps: training loss - 0.52760	, testing loss - 0.83319	
298	 steps: training loss - 0.51887	, testing loss - 0.83360	
299	 steps: training loss - 0.49275	, testing loss - 0.83399	
300	 steps: training loss - 0.52817	, testing loss - 0.83442	
301	 steps: training loss - 0.50119	, testing loss - 0.83479	
302	 steps: training loss - 0.49893	, testing loss - 0.83509	
303	 steps: training loss - 0.54381	, testing loss - 0.83536	
304	 steps: training loss - 0.46234	, testing loss - 0.83564	
305	 steps: training loss - 0.49632	, testing loss - 0.83609	
306	 steps: training loss - 0.49262	, testing loss - 0.83657	
307	 steps: training loss - 0.51038	, testing loss - 0.83699	
308	 steps: training loss - 0.43752	, testing loss - 0.83736	
309	 steps: training loss - 0.46195	, testing loss - 0.83782	
310	 steps: training loss - 0.49993	, testing loss - 0.83833	
311	 steps: training loss - 0.55065	, testing loss - 0.83884	
312	 steps: training loss - 0.47257	, testing loss - 0.83930	
313	 steps: training loss - 0.55239	, testing loss - 0.83973	
314	 steps: training loss - 0.50077	, testing loss - 0.84006	
315	 steps: training loss - 0.52203	, testing loss - 0.84038	
316	 steps: training loss - 0.40271	, testing loss - 0.84077	
317	 steps: training loss - 0.52599	, testing loss - 0.84125	
318	 steps: training loss - 0.50898	, testing loss - 0.84177	
319	 steps: training loss - 0.50478	, testing loss - 0.84233	
320	 steps: training loss - 0.50007	, testing loss - 0.84291	
321	 steps: training loss - 0.49791	, testing loss - 0.84344	
322	 steps: training loss - 0.50428	, testing loss - 0.84388	
323	 steps: training loss - 0.48228	, testing loss - 0.84437	
324	 steps: training loss - 0.46959	, testing loss - 0.84493	
325	 steps: training loss - 0.47758	, testing loss - 0.84551	
326	 steps: training loss - 0.47522	, testing loss - 0.84610	
327	 steps: training loss - 0.52398	, testing loss - 0.84669	
328	 steps: training loss - 0.52080	, testing loss - 0.84724	
329	 steps: training loss - 0.47323	, testing loss - 0.84776	
330	 steps: training loss - 0.48794	, testing loss - 0.84826	
331	 steps: training loss - 0.47399	, testing loss - 0.84875	
332	 steps: training loss - 0.39265	, testing loss - 0.84909	
333	 steps: training loss - 0.54313	, testing loss - 0.84953	
334	 steps: training loss - 0.47665	, testing loss - 0.84998	
335	 steps: training loss - 0.57114	, testing loss - 0.85059	
336	 steps: training loss - 0.50695	, testing loss - 0.85120	
337	 steps: training loss - 0.45754	, testing loss - 0.85178	
338	 steps: training loss - 0.43655	, testing loss - 0.85247	
339	 steps: training loss - 0.44479	, testing loss - 0.85324	
340	 steps: training loss - 0.48610	, testing loss - 0.85408	
341	 steps: training loss - 0.54923	, testing loss - 0.85494	
342	 steps: training loss - 0.41167	, testing loss - 0.85560	
343	 steps: training loss - 0.47505	, testing loss - 0.85625	
344	 steps: training loss - 0.45561	, testing loss - 0.85691	
345	 steps: training loss - 0.44116	, testing loss - 0.85747	
346	 steps: training loss - 0.39870	, testing loss - 0.85809	
347	 steps: training loss - 0.45520	, testing loss - 0.85883	
348	 steps: training loss - 0.56323	, testing loss - 0.85957	
349	 steps: training loss - 0.42923	, testing loss - 0.86026	
350	 steps: training loss - 0.46520	, testing loss - 0.86108	
351	 steps: training loss - 0.39986	, testing loss - 0.86173	
352	 steps: training loss - 0.51365	, testing loss - 0.86228	
353	 steps: training loss - 0.47336	, testing loss - 0.86290	
354	 steps: training loss - 0.47916	, testing loss - 0.86347	
355	 steps: training loss - 0.51078	, testing loss - 0.86402	
356	 steps: training loss - 0.46405	, testing loss - 0.86441	
357	 steps: training loss - 0.43481	, testing loss - 0.86474	
358	 steps: training loss - 0.45606	, testing loss - 0.86527	
359	 steps: training loss - 0.49880	, testing loss - 0.86587	
360	 steps: training loss - 0.56922	, testing loss - 0.86653	
361	 steps: training loss - 0.41284	, testing loss - 0.86715	
362	 steps: training loss - 0.44734	, testing loss - 0.86783	
363	 steps: training loss - 0.50753	, testing loss - 0.86868	
364	 steps: training loss - 0.45334	, testing loss - 0.86955	
365	 steps: training loss - 0.50864	, testing loss - 0.87057	
366	 steps: training loss - 0.54456	, testing loss - 0.87157	
367	 steps: training loss - 0.48744	, testing loss - 0.87229	
368	 steps: training loss - 0.54052	, testing loss - 0.87305	
369	 steps: training loss - 0.60857	, testing loss - 0.87374	
370	 steps: training loss - 0.57548	, testing loss - 0.87425	
371	 steps: training loss - 0.44913	, testing loss - 0.87455	
372	 steps: training loss - 0.41670	, testing loss - 0.87475	
373	 steps: training loss - 0.41173	, testing loss - 0.87501	
374	 steps: training loss - 0.43105	, testing loss - 0.87532	
375	 steps: training loss - 0.49738	, testing loss - 0.87573	
376	 steps: training loss - 0.41834	, testing loss - 0.87609	
377	 steps: training loss - 0.48044	, testing loss - 0.87655	
378	 steps: training loss - 0.36125	, testing loss - 0.87705	
379	 steps: training loss - 0.42497	, testing loss - 0.87761	
380	 steps: training loss - 0.50604	, testing loss - 0.87809	
381	 steps: training loss - 0.53214	, testing loss - 0.87860	
382	 steps: training loss - 0.47612	, testing loss - 0.87906	
383	 steps: training loss - 0.41900	, testing loss - 0.87943	
384	 steps: training loss - 0.45044	, testing loss - 0.87971	
385	 steps: training loss - 0.45353	, testing loss - 0.88013	
386	 steps: training loss - 0.43915	, testing loss - 0.88057	
387	 steps: training loss - 0.53452	, testing loss - 0.88117	
388	 steps: training loss - 0.47733	, testing loss - 0.88172	
389	 steps: training loss - 0.43931	, testing loss - 0.88207	
390	 steps: training loss - 0.50060	, testing loss - 0.88242	
391	 steps: training loss - 0.40712	, testing loss - 0.88264	
392	 steps: training loss - 0.55867	, testing loss - 0.88291	
393	 steps: training loss - 0.45810	, testing loss - 0.88309	
394	 steps: training loss - 0.44503	, testing loss - 0.88328	
395	 steps: training loss - 0.49291	, testing loss - 0.88354	
396	 steps: training loss - 0.45663	, testing loss - 0.88382	
397	 steps: training loss - 0.47108	, testing loss - 0.88420	
398	 steps: training loss - 0.42903	, testing loss - 0.88453	
399	 steps: training loss - 0.49657	, testing loss - 0.88494	
400	 steps: training loss - 0.43755	, testing loss - 0.88542	
401	 steps: training loss - 0.44179	, testing loss - 0.88595	
402	 steps: training loss - 0.44054	, testing loss - 0.88646	
403	 steps: training loss - 0.56560	, testing loss - 0.88693	
404	 steps: training loss - 0.44891	, testing loss - 0.88732	
405	 steps: training loss - 0.57213	, testing loss - 0.88781	
406	 steps: training loss - 0.46458	, testing loss - 0.88821	
407	 steps: training loss - 0.45600	, testing loss - 0.88844	
408	 steps: training loss - 0.46755	, testing loss - 0.88875	
409	 steps: training loss - 0.50854	, testing loss - 0.88900	
410	 steps: training loss - 0.52654	, testing loss - 0.88930	
411	 steps: training loss - 0.55021	, testing loss - 0.88967	
412	 steps: training loss - 0.38695	, testing loss - 0.88993	
413	 steps: training loss - 0.43985	, testing loss - 0.89026	
414	 steps: training loss - 0.54339	, testing loss - 0.89065	
415	 steps: training loss - 0.49671	, testing loss - 0.89086	
416	 steps: training loss - 0.46872	, testing loss - 0.89109	
417	 steps: training loss - 0.43100	, testing loss - 0.89136	
418	 steps: training loss - 0.49553	, testing loss - 0.89161	
419	 steps: training loss - 0.37460	, testing loss - 0.89179	
420	 steps: training loss - 0.37511	, testing loss - 0.89201	
421	 steps: training loss - 0.40625	, testing loss - 0.89234	
422	 steps: training loss - 0.46610	, testing loss - 0.89264	
423	 steps: training loss - 0.47022	, testing loss - 0.89302	
424	 steps: training loss - 0.49319	, testing loss - 0.89347	
425	 steps: training loss - 0.43121	, testing loss - 0.89402	
426	 steps: training loss - 0.45118	, testing loss - 0.89461	
427	 steps: training loss - 0.42336	, testing loss - 0.89519	
428	 steps: training loss - 0.44808	, testing loss - 0.89586	
429	 steps: training loss - 0.33750	, testing loss - 0.89655	
430	 steps: training loss - 0.55387	, testing loss - 0.89743	
431	 steps: training loss - 0.44800	, testing loss - 0.89829	
432	 steps: training loss - 0.46109	, testing loss - 0.89893	
433	 steps: training loss - 0.45443	, testing loss - 0.89944	
434	 steps: training loss - 0.43998	, testing loss - 0.89986	
435	 steps: training loss - 0.40583	, testing loss - 0.90033	
436	 steps: training loss - 0.42603	, testing loss - 0.90092	
437	 steps: training loss - 0.42469	, testing loss - 0.90166	
438	 steps: training loss - 0.36401	, testing loss - 0.90242	
439	 steps: training loss - 0.32196	, testing loss - 0.90315	
440	 steps: training loss - 0.45710	, testing loss - 0.90389	
441	 steps: training loss - 0.44054	, testing loss - 0.90461	
442	 steps: training loss - 0.41427	, testing loss - 0.90527	
443	 steps: training loss - 0.47762	, testing loss - 0.90585	
444	 steps: training loss - 0.31163	, testing loss - 0.90646	
445	 steps: training loss - 0.41061	, testing loss - 0.90720	
446	 steps: training loss - 0.44543	, testing loss - 0.90808	
447	 steps: training loss - 0.50822	, testing loss - 0.90888	
448	 steps: training loss - 0.45029	, testing loss - 0.90960	
449	 steps: training loss - 0.49616	, testing loss - 0.91025	
450	 steps: training loss - 0.54313	, testing loss - 0.91095	
451	 steps: training loss - 0.44646	, testing loss - 0.91166	
452	 steps: training loss - 0.44152	, testing loss - 0.91237	
453	 steps: training loss - 0.52173	, testing loss - 0.91317	
454	 steps: training loss - 0.39128	, testing loss - 0.91385	
455	 steps: training loss - 0.46026	, testing loss - 0.91445	
456	 steps: training loss - 0.40226	, testing loss - 0.91511	
457	 steps: training loss - 0.45045	, testing loss - 0.91578	
458	 steps: training loss - 0.42285	, testing loss - 0.91652	
459	 steps: training loss - 0.46704	, testing loss - 0.91738	
460	 steps: training loss - 0.44296	, testing loss - 0.91818	
461	 steps: training loss - 0.46332	, testing loss - 0.91913	
462	 steps: training loss - 0.40125	, testing loss - 0.92023	
463	 steps: training loss - 0.49039	, testing loss - 0.92126	
464	 steps: training loss - 0.53499	, testing loss - 0.92220	
465	 steps: training loss - 0.44808	, testing loss - 0.92308	
466	 steps: training loss - 0.46801	, testing loss - 0.92411	
467	 steps: training loss - 0.52982	, testing loss - 0.92505	
468	 steps: training loss - 0.36457	, testing loss - 0.92575	
469	 steps: training loss - 0.47386	, testing loss - 0.92649	
470	 steps: training loss - 0.41865	, testing loss - 0.92717	
471	 steps: training loss - 0.44979	, testing loss - 0.92785	
472	 steps: training loss - 0.45222	, testing loss - 0.92853	
473	 steps: training loss - 0.46398	, testing loss - 0.92910	
474	 steps: training loss - 0.44824	, testing loss - 0.92955	
475	 steps: training loss - 0.47966	, testing loss - 0.93002	
476	 steps: training loss - 0.43984	, testing loss - 0.93068	
477	 steps: training loss - 0.49501	, testing loss - 0.93120	
478	 steps: training loss - 0.44386	, testing loss - 0.93165	
479	 steps: training loss - 0.39133	, testing loss - 0.93216	
480	 steps: training loss - 0.42420	, testing loss - 0.93266	
481	 steps: training loss - 0.49052	, testing loss - 0.93306	
482	 steps: training loss - 0.50330	, testing loss - 0.93326	
483	 steps: training loss - 0.43322	, testing loss - 0.93322	
484	 steps: training loss - 0.45106	, testing loss - 0.93300	
485	 steps: training loss - 0.46078	, testing loss - 0.93292	
486	 steps: training loss - 0.46188	, testing loss - 0.93299	
487	 steps: training loss - 0.51138	, testing loss - 0.93329	
488	 steps: training loss - 0.46330	, testing loss - 0.93374	
489	 steps: training loss - 0.40042	, testing loss - 0.93403	
490	 steps: training loss - 0.40747	, testing loss - 0.93429	
491	 steps: training loss - 0.34969	, testing loss - 0.93455	
492	 steps: training loss - 0.40029	, testing loss - 0.93494	
493	 steps: training loss - 0.45040	, testing loss - 0.93536	
494	 steps: training loss - 0.43672	, testing loss - 0.93577	
495	 steps: training loss - 0.44150	, testing loss - 0.93624	
496	 steps: training loss - 0.46901	, testing loss - 0.93678	
497	 steps: training loss - 0.36249	, testing loss - 0.93722	
498	 steps: training loss - 0.43402	, testing loss - 0.93753	
499	 steps: training loss - 0.52067	, testing loss - 0.93793	
500	 steps: training loss - 0.45439	, testing loss - 0.93839	
501	 steps: training loss - 0.34502	, testing loss - 0.93887	
502	 steps: training loss - 0.38107	, testing loss - 0.93940	
503	 steps: training loss - 0.42247	, testing loss - 0.93990	
504	 steps: training loss - 0.40180	, testing loss - 0.94034	
505	 steps: training loss - 0.47496	, testing loss - 0.94080	
506	 steps: training loss - 0.38192	, testing loss - 0.94142	
507	 steps: training loss - 0.39328	, testing loss - 0.94209	
508	 steps: training loss - 0.42524	, testing loss - 0.94276	
509	 steps: training loss - 0.51915	, testing loss - 0.94352	
510	 steps: training loss - 0.43915	, testing loss - 0.94433	
511	 steps: training loss - 0.42385	, testing loss - 0.94491	
512	 steps: training loss - 0.43778	, testing loss - 0.94564	
513	 steps: training loss - 0.34285	, testing loss - 0.94632	
514	 steps: training loss - 0.53721	, testing loss - 0.94690	
515	 steps: training loss - 0.37077	, testing loss - 0.94728	
516	 steps: training loss - 0.51687	, testing loss - 0.94773	
517	 steps: training loss - 0.38659	, testing loss - 0.94818	
518	 steps: training loss - 0.42224	, testing loss - 0.94874	
519	 steps: training loss - 0.44589	, testing loss - 0.94955	
520	 steps: training loss - 0.36706	, testing loss - 0.95039	
521	 steps: training loss - 0.49852	, testing loss - 0.95109	
522	 steps: training loss - 0.45330	, testing loss - 0.95136	
523	 steps: training loss - 0.41138	, testing loss - 0.95150	
524	 steps: training loss - 0.45008	, testing loss - 0.95179	
525	 steps: training loss - 0.40136	, testing loss - 0.95199	
526	 steps: training loss - 0.39578	, testing loss - 0.95212	
527	 steps: training loss - 0.37003	, testing loss - 0.95246	
528	 steps: training loss - 0.46508	, testing loss - 0.95273	
529	 steps: training loss - 0.35709	, testing loss - 0.95303	
530	 steps: training loss - 0.46497	, testing loss - 0.95359	
531	 steps: training loss - 0.49395	, testing loss - 0.95422	
532	 steps: training loss - 0.52791	, testing loss - 0.95482	
533	 steps: training loss - 0.42724	, testing loss - 0.95517	
534	 steps: training loss - 0.43031	, testing loss - 0.95555	
535	 steps: training loss - 0.36142	, testing loss - 0.95588	
536	 steps: training loss - 0.39897	, testing loss - 0.95622	
537	 steps: training loss - 0.55223	, testing loss - 0.95653	
538	 steps: training loss - 0.45998	, testing loss - 0.95664	
539	 steps: training loss - 0.39702	, testing loss - 0.95680	
540	 steps: training loss - 0.46276	, testing loss - 0.95691	
541	 steps: training loss - 0.41109	, testing loss - 0.95721	
542	 steps: training loss - 0.41973	, testing loss - 0.95778	
543	 steps: training loss - 0.51889	, testing loss - 0.95853	
544	 steps: training loss - 0.43942	, testing loss - 0.95919	
545	 steps: training loss - 0.40884	, testing loss - 0.95980	
546	 steps: training loss - 0.41662	, testing loss - 0.96054	
547	 steps: training loss - 0.40036	, testing loss - 0.96122	
548	 steps: training loss - 0.56997	, testing loss - 0.96188	
549	 steps: training loss - 0.43129	, testing loss - 0.96244	
550	 steps: training loss - 0.42632	, testing loss - 0.96285	
551	 steps: training loss - 0.35580	, testing loss - 0.96306	
552	 steps: training loss - 0.50861	, testing loss - 0.96314	
553	 steps: training loss - 0.43922	, testing loss - 0.96315	
554	 steps: training loss - 0.48992	, testing loss - 0.96331	
555	 steps: training loss - 0.51476	, testing loss - 0.96335	
556	 steps: training loss - 0.42166	, testing loss - 0.96349	
557	 steps: training loss - 0.36624	, testing loss - 0.96371	
558	 steps: training loss - 0.44645	, testing loss - 0.96395	
559	 steps: training loss - 0.50648	, testing loss - 0.96419	
560	 steps: training loss - 0.38132	, testing loss - 0.96439	
561	 steps: training loss - 0.37917	, testing loss - 0.96468	
562	 steps: training loss - 0.41858	, testing loss - 0.96494	
563	 steps: training loss - 0.49678	, testing loss - 0.96509	
564	 steps: training loss - 0.42764	, testing loss - 0.96522	
565	 steps: training loss - 0.40565	, testing loss - 0.96528	
566	 steps: training loss - 0.45563	, testing loss - 0.96552	
567	 steps: training loss - 0.41281	, testing loss - 0.96568	
568	 steps: training loss - 0.51417	, testing loss - 0.96577	
569	 steps: training loss - 0.42861	, testing loss - 0.96595	
570	 steps: training loss - 0.43572	, testing loss - 0.96614	
571	 steps: training loss - 0.42844	, testing loss - 0.96644	
572	 steps: training loss - 0.32658	, testing loss - 0.96675	
573	 steps: training loss - 0.43934	, testing loss - 0.96708	
574	 steps: training loss - 0.41377	, testing loss - 0.96739	
575	 steps: training loss - 0.39726	, testing loss - 0.96799	
576	 steps: training loss - 0.39539	, testing loss - 0.96878	
577	 steps: training loss - 0.48060	, testing loss - 0.96940	
578	 steps: training loss - 0.47902	, testing loss - 0.96987	
579	 steps: training loss - 0.41234	, testing loss - 0.97030	
580	 steps: training loss - 0.35259	, testing loss - 0.97067	
581	 steps: training loss - 0.44524	, testing loss - 0.97101	
582	 steps: training loss - 0.42689	, testing loss - 0.97138	
583	 steps: training loss - 0.48666	, testing loss - 0.97180	
584	 steps: training loss - 0.45014	, testing loss - 0.97219	
585	 steps: training loss - 0.38985	, testing loss - 0.97251	
586	 steps: training loss - 0.41978	, testing loss - 0.97296	
587	 steps: training loss - 0.37788	, testing loss - 0.97335	
588	 steps: training loss - 0.39554	, testing loss - 0.97385	
589	 steps: training loss - 0.38757	, testing loss - 0.97434	
590	 steps: training loss - 0.41374	, testing loss - 0.97479	
591	 steps: training loss - 0.50596	, testing loss - 0.97514	
592	 steps: training loss - 0.44767	, testing loss - 0.97542	
593	 steps: training loss - 0.39832	, testing loss - 0.97565	
594	 steps: training loss - 0.42853	, testing loss - 0.97604	
595	 steps: training loss - 0.36160	, testing loss - 0.97653	
596	 steps: training loss - 0.31903	, testing loss - 0.97720	
597	 steps: training loss - 0.41801	, testing loss - 0.97805	
598	 steps: training loss - 0.47666	, testing loss - 0.97893	
599	 steps: training loss - 0.33993	, testing loss - 0.97959	
600	 steps: training loss - 0.41762	, testing loss - 0.98003	
601	 steps: training loss - 0.41546	, testing loss - 0.98060	
602	 steps: training loss - 0.41717	, testing loss - 0.98121	
603	 steps: training loss - 0.49214	, testing loss - 0.98159	
604	 steps: training loss - 0.37943	, testing loss - 0.98190	
605	 steps: training loss - 0.39195	, testing loss - 0.98225	
606	 steps: training loss - 0.43250	, testing loss - 0.98281	
607	 steps: training loss - 0.43186	, testing loss - 0.98328	
608	 steps: training loss - 0.49630	, testing loss - 0.98357	
609	 steps: training loss - 0.41031	, testing loss - 0.98384	
610	 steps: training loss - 0.43231	, testing loss - 0.98413	
611	 steps: training loss - 0.47798	, testing loss - 0.98434	
612	 steps: training loss - 0.47303	, testing loss - 0.98458	
613	 steps: training loss - 0.36524	, testing loss - 0.98499	
614	 steps: training loss - 0.37836	, testing loss - 0.98559	
615	 steps: training loss - 0.33984	, testing loss - 0.98608	
616	 steps: training loss - 0.41007	, testing loss - 0.98661	
617	 steps: training loss - 0.45582	, testing loss - 0.98744	
618	 steps: training loss - 0.32101	, testing loss - 0.98824	
619	 steps: training loss - 0.48540	, testing loss - 0.98919	
620	 steps: training loss - 0.36811	, testing loss - 0.99013	
621	 steps: training loss - 0.33826	, testing loss - 0.99091	
622	 steps: training loss - 0.45923	, testing loss - 0.99163	
623	 steps: training loss - 0.34531	, testing loss - 0.99235	
624	 steps: training loss - 0.31675	, testing loss - 0.99319	
625	 steps: training loss - 0.45032	, testing loss - 0.99398	
626	 steps: training loss - 0.40076	, testing loss - 0.99470	
627	 steps: training loss - 0.49575	, testing loss - 0.99553	
628	 steps: training loss - 0.34519	, testing loss - 0.99626	
629	 steps: training loss - 0.32052	, testing loss - 0.99692	
630	 steps: training loss - 0.43158	, testing loss - 0.99765	
631	 steps: training loss - 0.37649	, testing loss - 0.99820	
632	 steps: training loss - 0.33657	, testing loss - 0.99882	
633	 steps: training loss - 0.40170	, testing loss - 0.99930	
634	 steps: training loss - 0.40167	, testing loss - 0.99947	
635	 steps: training loss - 0.42322	, testing loss - 0.99958	
636	 steps: training loss - 0.36634	, testing loss - 0.99975	
637	 steps: training loss - 0.40008	, testing loss - 1.00006	
638	 steps: training loss - 0.49666	, testing loss - 1.00046	
639	 steps: training loss - 0.40819	, testing loss - 1.00095	
640	 steps: training loss - 0.45922	, testing loss - 1.00162	
641	 steps: training loss - 0.42214	, testing loss - 1.00221	
642	 steps: training loss - 0.41683	, testing loss - 1.00285	
643	 steps: training loss - 0.40797	, testing loss - 1.00356	
644	 steps: training loss - 0.49101	, testing loss - 1.00437	
645	 steps: training loss - 0.42960	, testing loss - 1.00494	
646	 steps: training loss - 0.44987	, testing loss - 1.00550	
647	 steps: training loss - 0.42881	, testing loss - 1.00607	
648	 steps: training loss - 0.35462	, testing loss - 1.00673	
649	 steps: training loss - 0.39780	, testing loss - 1.00749	
650	 steps: training loss - 0.47411	, testing loss - 1.00807	
651	 steps: training loss - 0.34937	, testing loss - 1.00855	
652	 steps: training loss - 0.37738	, testing loss - 1.00897	
653	 steps: training loss - 0.40675	, testing loss - 1.00959	
654	 steps: training loss - 0.49635	, testing loss - 1.01012	
655	 steps: training loss - 0.34231	, testing loss - 1.01065	
656	 steps: training loss - 0.28531	, testing loss - 1.01139	
657	 steps: training loss - 0.48976	, testing loss - 1.01230	
658	 steps: training loss - 0.46996	, testing loss - 1.01309	
659	 steps: training loss - 0.39075	, testing loss - 1.01382	
660	 steps: training loss - 0.35738	, testing loss - 1.01447	
661	 steps: training loss - 0.47141	, testing loss - 1.01501	
662	 steps: training loss - 0.27956	, testing loss - 1.01548	
663	 steps: training loss - 0.43352	, testing loss - 1.01601	
664	 steps: training loss - 0.38460	, testing loss - 1.01650	
665	 steps: training loss - 0.47522	, testing loss - 1.01691	
666	 steps: training loss - 0.45537	, testing loss - 1.01711	
667	 steps: training loss - 0.36378	, testing loss - 1.01731	
668	 steps: training loss - 0.48131	, testing loss - 1.01762	
669	 steps: training loss - 0.50142	, testing loss - 1.01787	
670	 steps: training loss - 0.38191	, testing loss - 1.01799	
671	 steps: training loss - 0.37204	, testing loss - 1.01817	
672	 steps: training loss - 0.45535	, testing loss - 1.01862	
673	 steps: training loss - 0.38505	, testing loss - 1.01905	
674	 steps: training loss - 0.37248	, testing loss - 1.01965	
675	 steps: training loss - 0.37678	, testing loss - 1.02021	
676	 steps: training loss - 0.40814	, testing loss - 1.02072	
677	 steps: training loss - 0.56794	, testing loss - 1.02147	
678	 steps: training loss - 0.41517	, testing loss - 1.02216	
679	 steps: training loss - 0.47374	, testing loss - 1.02287	
680	 steps: training loss - 0.41900	, testing loss - 1.02343	
681	 steps: training loss - 0.33568	, testing loss - 1.02374	
682	 steps: training loss - 0.37465	, testing loss - 1.02398	
683	 steps: training loss - 0.45492	, testing loss - 1.02448	
684	 steps: training loss - 0.46157	, testing loss - 1.02500	
685	 steps: training loss - 0.45574	, testing loss - 1.02545	
686	 steps: training loss - 0.46640	, testing loss - 1.02564	
687	 steps: training loss - 0.43259	, testing loss - 1.02556	
688	 steps: training loss - 0.45712	, testing loss - 1.02557	
689	 steps: training loss - 0.41448	, testing loss - 1.02547	
690	 steps: training loss - 0.35112	, testing loss - 1.02545	
691	 steps: training loss - 0.37805	, testing loss - 1.02580	
692	 steps: training loss - 0.38865	, testing loss - 1.02593	
693	 steps: training loss - 0.35422	, testing loss - 1.02601	
694	 steps: training loss - 0.35833	, testing loss - 1.02593	
695	 steps: training loss - 0.42613	, testing loss - 1.02572	
696	 steps: training loss - 0.57405	, testing loss - 1.02579	
697	 steps: training loss - 0.40894	, testing loss - 1.02588	
698	 steps: training loss - 0.44455	, testing loss - 1.02566	
699	 steps: training loss - 0.36255	, testing loss - 1.02523	
700	 steps: training loss - 0.44436	, testing loss - 1.02491	
701	 steps: training loss - 0.35061	, testing loss - 1.02447	
702	 steps: training loss - 0.44541	, testing loss - 1.02420	
703	 steps: training loss - 0.46674	, testing loss - 1.02414	
704	 steps: training loss - 0.42176	, testing loss - 1.02430	
705	 steps: training loss - 0.34508	, testing loss - 1.02463	
706	 steps: training loss - 0.39445	, testing loss - 1.02520	
707	 steps: training loss - 0.36437	, testing loss - 1.02583	
708	 steps: training loss - 0.35550	, testing loss - 1.02638	
709	 steps: training loss - 0.48536	, testing loss - 1.02708	
710	 steps: training loss - 0.35676	, testing loss - 1.02758	
711	 steps: training loss - 0.49934	, testing loss - 1.02813	
712	 steps: training loss - 0.40342	, testing loss - 1.02863	
713	 steps: training loss - 0.35461	, testing loss - 1.02921	
714	 steps: training loss - 0.44720	, testing loss - 1.02974	
715	 steps: training loss - 0.31887	, testing loss - 1.03029	
716	 steps: training loss - 0.33488	, testing loss - 1.03102	
717	 steps: training loss - 0.28807	, testing loss - 1.03172	
718	 steps: training loss - 0.42304	, testing loss - 1.03228	
719	 steps: training loss - 0.36423	, testing loss - 1.03268	
720	 steps: training loss - 0.41524	, testing loss - 1.03294	
721	 steps: training loss - 0.41196	, testing loss - 1.03311	
722	 steps: training loss - 0.52039	, testing loss - 1.03341	
723	 steps: training loss - 0.40338	, testing loss - 1.03362	
724	 steps: training loss - 0.47982	, testing loss - 1.03372	
725	 steps: training loss - 0.35921	, testing loss - 1.03384	
726	 steps: training loss - 0.42549	, testing loss - 1.03382	
727	 steps: training loss - 0.47864	, testing loss - 1.03390	
728	 steps: training loss - 0.34163	, testing loss - 1.03402	
729	 steps: training loss - 0.38349	, testing loss - 1.03401	
730	 steps: training loss - 0.31210	, testing loss - 1.03411	
731	 steps: training loss - 0.50684	, testing loss - 1.03452	
732	 steps: training loss - 0.44902	, testing loss - 1.03506	
733	 steps: training loss - 0.25968	, testing loss - 1.03550	
734	 steps: training loss - 0.31072	, testing loss - 1.03611	
735	 steps: training loss - 0.38971	, testing loss - 1.03680	
736	 steps: training loss - 0.30350	, testing loss - 1.03759	
737	 steps: training loss - 0.46715	, testing loss - 1.03843	
738	 steps: training loss - 0.41488	, testing loss - 1.03913	
739	 steps: training loss - 0.37307	, testing loss - 1.03962	
740	 steps: training loss - 0.29344	, testing loss - 1.04025	
741	 steps: training loss - 0.41017	, testing loss - 1.04093	
742	 steps: training loss - 0.32996	, testing loss - 1.04139	
743	 steps: training loss - 0.26703	, testing loss - 1.04190	
744	 steps: training loss - 0.35768	, testing loss - 1.04261	
745	 steps: training loss - 0.39429	, testing loss - 1.04342	
746	 steps: training loss - 0.45977	, testing loss - 1.04410	
747	 steps: training loss - 0.30644	, testing loss - 1.04470	
748	 steps: training loss - 0.23828	, testing loss - 1.04536	
749	 steps: training loss - 0.41745	, testing loss - 1.04611	
750	 steps: training loss - 0.30767	, testing loss - 1.04676	
751	 steps: training loss - 0.42303	, testing loss - 1.04740	
752	 steps: training loss - 0.44433	, testing loss - 1.04792	
753	 steps: training loss - 0.36284	, testing loss - 1.04858	
754	 steps: training loss - 0.29791	, testing loss - 1.04941	
755	 steps: training loss - 0.32826	, testing loss - 1.05024	
756	 steps: training loss - 0.40242	, testing loss - 1.05102	
757	 steps: training loss - 0.39046	, testing loss - 1.05179	
758	 steps: training loss - 0.43838	, testing loss - 1.05260	
759	 steps: training loss - 0.31900	, testing loss - 1.05360	
760	 steps: training loss - 0.34403	, testing loss - 1.05439	
761	 steps: training loss - 0.23150	, testing loss - 1.05505	
762	 steps: training loss - 0.41151	, testing loss - 1.05594	
763	 steps: training loss - 0.27856	, testing loss - 1.05674	
764	 steps: training loss - 0.47840	, testing loss - 1.05740	
765	 steps: training loss - 0.39881	, testing loss - 1.05779	
766	 steps: training loss - 0.49396	, testing loss - 1.05799	
767	 steps: training loss - 0.35158	, testing loss - 1.05797	
768	 steps: training loss - 0.38581	, testing loss - 1.05770	
769	 steps: training loss - 0.30101	, testing loss - 1.05765	
770	 steps: training loss - 0.39657	, testing loss - 1.05779	
771	 steps: training loss - 0.39839	, testing loss - 1.05811	
772	 steps: training loss - 0.42300	, testing loss - 1.05835	
773	 steps: training loss - 0.39411	, testing loss - 1.05833	
774	 steps: training loss - 0.47681	, testing loss - 1.05813	
775	 steps: training loss - 0.35819	, testing loss - 1.05813	
776	 steps: training loss - 0.44822	, testing loss - 1.05834	
777	 steps: training loss - 0.38325	, testing loss - 1.05866	
778	 steps: training loss - 0.47599	, testing loss - 1.05901	
779	 steps: training loss - 0.39755	, testing loss - 1.05934	
780	 steps: training loss - 0.37294	, testing loss - 1.05971	
781	 steps: training loss - 0.34692	, testing loss - 1.06020	
782	 steps: training loss - 0.43732	, testing loss - 1.06048	
783	 steps: training loss - 0.49454	, testing loss - 1.06054	
784	 steps: training loss - 0.35492	, testing loss - 1.06072	
785	 steps: training loss - 0.33037	, testing loss - 1.06110	
786	 steps: training loss - 0.23454	, testing loss - 1.06156	
787	 steps: training loss - 0.44963	, testing loss - 1.06209	
788	 steps: training loss - 0.32575	, testing loss - 1.06265	
789	 steps: training loss - 0.33994	, testing loss - 1.06302	
790	 steps: training loss - 0.42587	, testing loss - 1.06345	
791	 steps: training loss - 0.36333	, testing loss - 1.06402	
792	 steps: training loss - 0.53991	, testing loss - 1.06456	
793	 steps: training loss - 0.40471	, testing loss - 1.06505	
794	 steps: training loss - 0.28547	, testing loss - 1.06553	
795	 steps: training loss - 0.44536	, testing loss - 1.06605	
796	 steps: training loss - 0.44132	, testing loss - 1.06630	
797	 steps: training loss - 0.49669	, testing loss - 1.06651	
798	 steps: training loss - 0.31750	, testing loss - 1.06686	
799	 steps: training loss - 0.34373	, testing loss - 1.06718	
800	 steps: training loss - 0.48978	, testing loss - 1.06724	
801	 steps: training loss - 0.45955	, testing loss - 1.06726	
802	 steps: training loss - 0.44110	, testing loss - 1.06749	
803	 steps: training loss - 0.27539	, testing loss - 1.06779	
804	 steps: training loss - 0.30046	, testing loss - 1.06824	
805	 steps: training loss - 0.41474	, testing loss - 1.06867	
806	 steps: training loss - 0.35517	, testing loss - 1.06916	
807	 steps: training loss - 0.41711	, testing loss - 1.06955	
808	 steps: training loss - 0.41022	, testing loss - 1.07003	
809	 steps: training loss - 0.30939	, testing loss - 1.07048	
810	 steps: training loss - 0.30944	, testing loss - 1.07109	
811	 steps: training loss - 0.43488	, testing loss - 1.07190	
812	 steps: training loss - 0.44341	, testing loss - 1.07255	
813	 steps: training loss - 0.50662	, testing loss - 1.07313	
814	 steps: training loss - 0.28258	, testing loss - 1.07336	
815	 steps: training loss - 0.36803	, testing loss - 1.07341	
816	 steps: training loss - 0.28759	, testing loss - 1.07349	
817	 steps: training loss - 0.44652	, testing loss - 1.07356	
818	 steps: training loss - 0.55624	, testing loss - 1.07370	
819	 steps: training loss - 0.44509	, testing loss - 1.07370	
820	 steps: training loss - 0.32102	, testing loss - 1.07369	
821	 steps: training loss - 0.33516	, testing loss - 1.07361	
822	 steps: training loss - 0.34408	, testing loss - 1.07350	
823	 steps: training loss - 0.40287	, testing loss - 1.07363	
824	 steps: training loss - 0.36213	, testing loss - 1.07377	
825	 steps: training loss - 0.47592	, testing loss - 1.07390	
826	 steps: training loss - 0.41476	, testing loss - 1.07405	
827	 steps: training loss - 0.39793	, testing loss - 1.07408	
828	 steps: training loss - 0.29877	, testing loss - 1.07403	
829	 steps: training loss - 0.29761	, testing loss - 1.07412	
830	 steps: training loss - 0.29270	, testing loss - 1.07423	
831	 steps: training loss - 0.51917	, testing loss - 1.07446	
832	 steps: training loss - 0.35929	, testing loss - 1.07464	
833	 steps: training loss - 0.31781	, testing loss - 1.07501	
834	 steps: training loss - 0.49536	, testing loss - 1.07552	
835	 steps: training loss - 0.27574	, testing loss - 1.07594	
836	 steps: training loss - 0.34717	, testing loss - 1.07645	
837	 steps: training loss - 0.38784	, testing loss - 1.07700	
838	 steps: training loss - 0.34721	, testing loss - 1.07745	
839	 steps: training loss - 0.34754	, testing loss - 1.07784	
840	 steps: training loss - 0.50090	, testing loss - 1.07820	
841	 steps: training loss - 0.36841	, testing loss - 1.07849	
842	 steps: training loss - 0.49571	, testing loss - 1.07894	
843	 steps: training loss - 0.30214	, testing loss - 1.07950	
844	 steps: training loss - 0.35207	, testing loss - 1.07992	
845	 steps: training loss - 0.36244	, testing loss - 1.08026	
846	 steps: training loss - 0.46907	, testing loss - 1.08052	
847	 steps: training loss - 0.35215	, testing loss - 1.08066	
848	 steps: training loss - 0.53922	, testing loss - 1.08080	
849	 steps: training loss - 0.31218	, testing loss - 1.08098	
850	 steps: training loss - 0.42734	, testing loss - 1.08106	
851	 steps: training loss - 0.37441	, testing loss - 1.08104	
852	 steps: training loss - 0.43837	, testing loss - 1.08110	
853	 steps: training loss - 0.42511	, testing loss - 1.08146	
854	 steps: training loss - 0.37568	, testing loss - 1.08192	
855	 steps: training loss - 0.29309	, testing loss - 1.08236	
856	 steps: training loss - 0.47135	, testing loss - 1.08283	
857	 steps: training loss - 0.45576	, testing loss - 1.08330	
858	 steps: training loss - 0.34665	, testing loss - 1.08394	
859	 steps: training loss - 0.35563	, testing loss - 1.08459	
860	 steps: training loss - 0.37597	, testing loss - 1.08526	
861	 steps: training loss - 0.35846	, testing loss - 1.08578	
862	 steps: training loss - 0.39928	, testing loss - 1.08605	
863	 steps: training loss - 0.25333	, testing loss - 1.08619	
864	 steps: training loss - 0.40291	, testing loss - 1.08632	
865	 steps: training loss - 0.34088	, testing loss - 1.08638	
866	 steps: training loss - 0.34638	, testing loss - 1.08657	
867	 steps: training loss - 0.41863	, testing loss - 1.08681	
868	 steps: training loss - 0.45235	, testing loss - 1.08719	
869	 steps: training loss - 0.41475	, testing loss - 1.08762	
870	 steps: training loss - 0.44729	, testing loss - 1.08801	
871	 steps: training loss - 0.29822	, testing loss - 1.08838	
872	 steps: training loss - 0.40731	, testing loss - 1.08860	
873	 steps: training loss - 0.36600	, testing loss - 1.08868	
874	 steps: training loss - 0.33239	, testing loss - 1.08895	
875	 steps: training loss - 0.37237	, testing loss - 1.08933	
876	 steps: training loss - 0.33616	, testing loss - 1.08988	
877	 steps: training loss - 0.26651	, testing loss - 1.09026	
878	 steps: training loss - 0.46116	, testing loss - 1.09055	
879	 steps: training loss - 0.45276	, testing loss - 1.09087	
880	 steps: training loss - 0.33559	, testing loss - 1.09121	
881	 steps: training loss - 0.34167	, testing loss - 1.09177	
882	 steps: training loss - 0.29936	, testing loss - 1.09228	
883	 steps: training loss - 0.43847	, testing loss - 1.09273	
884	 steps: training loss - 0.38274	, testing loss - 1.09316	
885	 steps: training loss - 0.27930	, testing loss - 1.09372	
886	 steps: training loss - 0.45713	, testing loss - 1.09443	
887	 steps: training loss - 0.45809	, testing loss - 1.09505	
888	 steps: training loss - 0.29143	, testing loss - 1.09578	
889	 steps: training loss - 0.32808	, testing loss - 1.09672	
890	 steps: training loss - 0.38634	, testing loss - 1.09776	
891	 steps: training loss - 0.39087	, testing loss - 1.09865	
892	 steps: training loss - 0.44681	, testing loss - 1.09959	
893	 steps: training loss - 0.46518	, testing loss - 1.10050	
894	 steps: training loss - 0.33622	, testing loss - 1.10112	
895	 steps: training loss - 0.49715	, testing loss - 1.10139	
896	 steps: training loss - 0.38363	, testing loss - 1.10170	
897	 steps: training loss - 0.38027	, testing loss - 1.10194	
898	 steps: training loss - 0.31141	, testing loss - 1.10228	
899	 steps: training loss - 0.34492	, testing loss - 1.10270	
900	 steps: training loss - 0.34067	, testing loss - 1.10298	
901	 steps: training loss - 0.32057	, testing loss - 1.10344	
902	 steps: training loss - 0.43855	, testing loss - 1.10399	
903	 steps: training loss - 0.37902	, testing loss - 1.10456	
904	 steps: training loss - 0.46652	, testing loss - 1.10514	
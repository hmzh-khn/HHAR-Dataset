TRAINING INFO - is_lstm? False
-------------
actions ['bike', 'sit']
devices ['samsungold_1', 'samsungold_2']
step size 10
num_epochs 1000
batch size 20
feature size (x,y,z) 3
hidden size 20
number recurrent layers 2
number of classes 2
writing pickle to run8_model.p
writing accuracy + pickle to acc_run8_model.p
------------------------------------------------
0	 steps: training loss - 0.73961	, testing loss - 0.69364	
1	 steps: training loss - 0.73544	, testing loss - 0.69082	
2	 steps: training loss - 0.73143	, testing loss - 0.68815	
3	 steps: training loss - 0.72759	, testing loss - 0.68561	
4	 steps: training loss - 0.72390	, testing loss - 0.68319	
5	 steps: training loss - 0.72036	, testing loss - 0.68087	
6	 steps: training loss - 0.71695	, testing loss - 0.67865	
7	 steps: training loss - 0.71368	, testing loss - 0.67654	
8	 steps: training loss - 0.71054	, testing loss - 0.67452	
9	 steps: training loss - 0.70753	, testing loss - 0.67259	
10	 steps: training loss - 0.70463	, testing loss - 0.67074	
11	 steps: training loss - 0.70186	, testing loss - 0.66896	
12	 steps: training loss - 0.69918	, testing loss - 0.66724	
13	 steps: training loss - 0.69661	, testing loss - 0.66557	
14	 steps: training loss - 0.69411	, testing loss - 0.66394	
15	 steps: training loss - 0.69170	, testing loss - 0.66233	
16	 steps: training loss - 0.68935	, testing loss - 0.66074	
17	 steps: training loss - 0.68705	, testing loss - 0.65915	
18	 steps: training loss - 0.68480	, testing loss - 0.65756	
19	 steps: training loss - 0.68259	, testing loss - 0.65596	
20	 steps: training loss - 0.68042	, testing loss - 0.65435	
21	 steps: training loss - 0.67827	, testing loss - 0.65273	
22	 steps: training loss - 0.67615	, testing loss - 0.65109	
23	 steps: training loss - 0.67405	, testing loss - 0.64943	
24	 steps: training loss - 0.67197	, testing loss - 0.64777	
25	 steps: training loss - 0.66991	, testing loss - 0.64609	
26	 steps: training loss - 0.66787	, testing loss - 0.64440	
27	 steps: training loss - 0.66585	, testing loss - 0.64272	
28	 steps: training loss - 0.66385	, testing loss - 0.64103	
29	 steps: training loss - 0.66186	, testing loss - 0.63934	
30	 steps: training loss - 0.65990	, testing loss - 0.63766	
31	 steps: training loss - 0.65795	, testing loss - 0.63599	
32	 steps: training loss - 0.65602	, testing loss - 0.63433	
33	 steps: training loss - 0.65411	, testing loss - 0.63268	
34	 steps: training loss - 0.65222	, testing loss - 0.63104	
35	 steps: training loss - 0.65034	, testing loss - 0.62942	
36	 steps: training loss - 0.64848	, testing loss - 0.62782	
37	 steps: training loss - 0.64665	, testing loss - 0.62624	
38	 steps: training loss - 0.64483	, testing loss - 0.62467	
39	 steps: training loss - 0.64302	, testing loss - 0.62313	
40	 steps: training loss - 0.64124	, testing loss - 0.62160	
41	 steps: training loss - 0.63947	, testing loss - 0.62009	
42	 steps: training loss - 0.63772	, testing loss - 0.61860	
43	 steps: training loss - 0.63599	, testing loss - 0.61713	
44	 steps: training loss - 0.63427	, testing loss - 0.61568	
45	 steps: training loss - 0.63257	, testing loss - 0.61424	
46	 steps: training loss - 0.63088	, testing loss - 0.61282	
47	 steps: training loss - 0.62922	, testing loss - 0.61142	
48	 steps: training loss - 0.62757	, testing loss - 0.61004	
49	 steps: training loss - 0.62593	, testing loss - 0.60867	
50	 steps: training loss - 0.62431	, testing loss - 0.60732	
51	 steps: training loss - 0.62271	, testing loss - 0.60599	
52	 steps: training loss - 0.62112	, testing loss - 0.60467	
53	 steps: training loss - 0.61955	, testing loss - 0.60336	
54	 steps: training loss - 0.61800	, testing loss - 0.60207	
55	 steps: training loss - 0.61646	, testing loss - 0.60079	
56	 steps: training loss - 0.61494	, testing loss - 0.59952	
57	 steps: training loss - 0.61343	, testing loss - 0.59827	
58	 steps: training loss - 0.61193	, testing loss - 0.59703	
59	 steps: training loss - 0.61045	, testing loss - 0.59580	
60	 steps: training loss - 0.60899	, testing loss - 0.59459	
61	 steps: training loss - 0.60754	, testing loss - 0.59339	
62	 steps: training loss - 0.60611	, testing loss - 0.59220	
63	 steps: training loss - 0.60469	, testing loss - 0.59102	
64	 steps: training loss - 0.60328	, testing loss - 0.58986	
65	 steps: training loss - 0.60189	, testing loss - 0.58871	
66	 steps: training loss - 0.60052	, testing loss - 0.58757	
67	 steps: training loss - 0.59916	, testing loss - 0.58644	
68	 steps: training loss - 0.59781	, testing loss - 0.58532	
69	 steps: training loss - 0.59647	, testing loss - 0.58422	
70	 steps: training loss - 0.59515	, testing loss - 0.58313	
71	 steps: training loss - 0.59384	, testing loss - 0.58205	
72	 steps: training loss - 0.59255	, testing loss - 0.58098	
73	 steps: training loss - 0.59127	, testing loss - 0.57992	
74	 steps: training loss - 0.59000	, testing loss - 0.57887	
75	 steps: training loss - 0.58875	, testing loss - 0.57784	
76	 steps: training loss - 0.58751	, testing loss - 0.57682	
77	 steps: training loss - 0.58628	, testing loss - 0.57581	
78	 steps: training loss - 0.58506	, testing loss - 0.57481	
79	 steps: training loss - 0.58386	, testing loss - 0.57382	
80	 steps: training loss - 0.58267	, testing loss - 0.57284	
81	 steps: training loss - 0.58149	, testing loss - 0.57187	
82	 steps: training loss - 0.58032	, testing loss - 0.57092	
83	 steps: training loss - 0.57917	, testing loss - 0.56997	
84	 steps: training loss - 0.57802	, testing loss - 0.56904	
85	 steps: training loss - 0.57689	, testing loss - 0.56811	
86	 steps: training loss - 0.57577	, testing loss - 0.56720	
87	 steps: training loss - 0.57467	, testing loss - 0.56630	
88	 steps: training loss - 0.57357	, testing loss - 0.56540	
89	 steps: training loss - 0.57249	, testing loss - 0.56452	
90	 steps: training loss - 0.57141	, testing loss - 0.56365	
91	 steps: training loss - 0.57035	, testing loss - 0.56278	
92	 steps: training loss - 0.56930	, testing loss - 0.56193	
93	 steps: training loss - 0.56826	, testing loss - 0.56109	
94	 steps: training loss - 0.56723	, testing loss - 0.56025	
95	 steps: training loss - 0.56621	, testing loss - 0.55943	
96	 steps: training loss - 0.56520	, testing loss - 0.55861	
97	 steps: training loss - 0.56420	, testing loss - 0.55781	
98	 steps: training loss - 0.56321	, testing loss - 0.55701	
99	 steps: training loss - 0.56223	, testing loss - 0.55622	
100	 steps: training loss - 0.56126	, testing loss - 0.55544	
101	 steps: training loss - 0.56031	, testing loss - 0.55467	
102	 steps: training loss - 0.55936	, testing loss - 0.55391	
103	 steps: training loss - 0.55842	, testing loss - 0.55316	
104	 steps: training loss - 0.55749	, testing loss - 0.55242	
105	 steps: training loss - 0.55657	, testing loss - 0.55168	
106	 steps: training loss - 0.55566	, testing loss - 0.55095	
107	 steps: training loss - 0.55476	, testing loss - 0.55024	
108	 steps: training loss - 0.55386	, testing loss - 0.54953	
109	 steps: training loss - 0.55298	, testing loss - 0.54882	
110	 steps: training loss - 0.55211	, testing loss - 0.54813	
111	 steps: training loss - 0.55124	, testing loss - 0.54744	
112	 steps: training loss - 0.55039	, testing loss - 0.54677	
113	 steps: training loss - 0.54954	, testing loss - 0.54610	
114	 steps: training loss - 0.54870	, testing loss - 0.54543	
115	 steps: training loss - 0.54787	, testing loss - 0.54478	
116	 steps: training loss - 0.54705	, testing loss - 0.54413	
117	 steps: training loss - 0.54623	, testing loss - 0.54349	
118	 steps: training loss - 0.54542	, testing loss - 0.54286	
119	 steps: training loss - 0.54463	, testing loss - 0.54224	
120	 steps: training loss - 0.54384	, testing loss - 0.54162	
121	 steps: training loss - 0.54305	, testing loss - 0.54101	
122	 steps: training loss - 0.54228	, testing loss - 0.54041	
123	 steps: training loss - 0.54151	, testing loss - 0.53981	
124	 steps: training loss - 0.54075	, testing loss - 0.53922	
125	 steps: training loss - 0.54000	, testing loss - 0.53864	
126	 steps: training loss - 0.53926	, testing loss - 0.53806	
127	 steps: training loss - 0.53852	, testing loss - 0.53749	
128	 steps: training loss - 0.53779	, testing loss - 0.53693	
129	 steps: training loss - 0.53707	, testing loss - 0.53638	
130	 steps: training loss - 0.53635	, testing loss - 0.53583	
131	 steps: training loss - 0.53565	, testing loss - 0.53528	
132	 steps: training loss - 0.53495	, testing loss - 0.53475	
133	 steps: training loss - 0.53425	, testing loss - 0.53422	
134	 steps: training loss - 0.53356	, testing loss - 0.53369	
135	 steps: training loss - 0.53288	, testing loss - 0.53318	
136	 steps: training loss - 0.53221	, testing loss - 0.53266	
137	 steps: training loss - 0.53154	, testing loss - 0.53216	
138	 steps: training loss - 0.53088	, testing loss - 0.53166	
139	 steps: training loss - 0.53022	, testing loss - 0.53117	
140	 steps: training loss - 0.52957	, testing loss - 0.53068	
141	 steps: training loss - 0.52893	, testing loss - 0.53020	
142	 steps: training loss - 0.52829	, testing loss - 0.52972	
143	 steps: training loss - 0.52766	, testing loss - 0.52925	
144	 steps: training loss - 0.52704	, testing loss - 0.52878	
145	 steps: training loss - 0.52642	, testing loss - 0.52832	
146	 steps: training loss - 0.52581	, testing loss - 0.52787	
147	 steps: training loss - 0.52520	, testing loss - 0.52742	
148	 steps: training loss - 0.52460	, testing loss - 0.52698	
149	 steps: training loss - 0.52400	, testing loss - 0.52654	
150	 steps: training loss - 0.52341	, testing loss - 0.52611	
151	 steps: training loss - 0.52283	, testing loss - 0.52568	
152	 steps: training loss - 0.52225	, testing loss - 0.52526	
153	 steps: training loss - 0.52168	, testing loss - 0.52484	
154	 steps: training loss - 0.52111	, testing loss - 0.52443	
155	 steps: training loss - 0.52054	, testing loss - 0.52402	
156	 steps: training loss - 0.51999	, testing loss - 0.52362	
157	 steps: training loss - 0.51943	, testing loss - 0.52322	
158	 steps: training loss - 0.51889	, testing loss - 0.52282	
159	 steps: training loss - 0.51834	, testing loss - 0.52244	
160	 steps: training loss - 0.51780	, testing loss - 0.52205	
161	 steps: training loss - 0.51727	, testing loss - 0.52167	
162	 steps: training loss - 0.51674	, testing loss - 0.52130	
163	 steps: training loss - 0.51622	, testing loss - 0.52093	
164	 steps: training loss - 0.51570	, testing loss - 0.52056	
165	 steps: training loss - 0.51519	, testing loss - 0.52020	
166	 steps: training loss - 0.51468	, testing loss - 0.51984	
167	 steps: training loss - 0.51417	, testing loss - 0.51949	
168	 steps: training loss - 0.51367	, testing loss - 0.51914	
169	 steps: training loss - 0.51318	, testing loss - 0.51880	
170	 steps: training loss - 0.51268	, testing loss - 0.51846	
171	 steps: training loss - 0.51220	, testing loss - 0.51812	
172	 steps: training loss - 0.51171	, testing loss - 0.51779	
173	 steps: training loss - 0.51123	, testing loss - 0.51746	
174	 steps: training loss - 0.51076	, testing loss - 0.51714	
175	 steps: training loss - 0.51029	, testing loss - 0.51682	
176	 steps: training loss - 0.50982	, testing loss - 0.51650	
177	 steps: training loss - 0.50936	, testing loss - 0.51619	
178	 steps: training loss - 0.50890	, testing loss - 0.51588	
179	 steps: training loss - 0.50845	, testing loss - 0.51558	
180	 steps: training loss - 0.50800	, testing loss - 0.51527	
181	 steps: training loss - 0.50755	, testing loss - 0.51498	
182	 steps: training loss - 0.50711	, testing loss - 0.51468	
183	 steps: training loss - 0.50667	, testing loss - 0.51439	
184	 steps: training loss - 0.50623	, testing loss - 0.51411	
185	 steps: training loss - 0.50580	, testing loss - 0.51382	
186	 steps: training loss - 0.50537	, testing loss - 0.51355	
187	 steps: training loss - 0.50494	, testing loss - 0.51327	
188	 steps: training loss - 0.50452	, testing loss - 0.51300	
189	 steps: training loss - 0.50411	, testing loss - 0.51273	
190	 steps: training loss - 0.50369	, testing loss - 0.51246	
191	 steps: training loss - 0.50328	, testing loss - 0.51220	
192	 steps: training loss - 0.50287	, testing loss - 0.51194	
193	 steps: training loss - 0.50247	, testing loss - 0.51168	
194	 steps: training loss - 0.50207	, testing loss - 0.51143	
195	 steps: training loss - 0.50167	, testing loss - 0.51118	
196	 steps: training loss - 0.50127	, testing loss - 0.51093	
197	 steps: training loss - 0.50088	, testing loss - 0.51069	
198	 steps: training loss - 0.50049	, testing loss - 0.51045	
199	 steps: training loss - 0.50011	, testing loss - 0.51021	
200	 steps: training loss - 0.49973	, testing loss - 0.50998	
201	 steps: training loss - 0.49935	, testing loss - 0.50975	
202	 steps: training loss - 0.49897	, testing loss - 0.50952	
203	 steps: training loss - 0.49860	, testing loss - 0.50929	
204	 steps: training loss - 0.49823	, testing loss - 0.50907	
205	 steps: training loss - 0.49786	, testing loss - 0.50885	
206	 steps: training loss - 0.49750	, testing loss - 0.50863	
207	 steps: training loss - 0.49713	, testing loss - 0.50842	
208	 steps: training loss - 0.49678	, testing loss - 0.50820	
209	 steps: training loss - 0.49642	, testing loss - 0.50799	
210	 steps: training loss - 0.49607	, testing loss - 0.50779	
211	 steps: training loss - 0.49571	, testing loss - 0.50758	
212	 steps: training loss - 0.49537	, testing loss - 0.50738	
213	 steps: training loss - 0.49502	, testing loss - 0.50718	
214	 steps: training loss - 0.49468	, testing loss - 0.50699	
215	 steps: training loss - 0.49434	, testing loss - 0.50679	
216	 steps: training loss - 0.49400	, testing loss - 0.50660	
217	 steps: training loss - 0.49366	, testing loss - 0.50641	
218	 steps: training loss - 0.49333	, testing loss - 0.50623	
219	 steps: training loss - 0.49300	, testing loss - 0.50604	
220	 steps: training loss - 0.49267	, testing loss - 0.50586	
221	 steps: training loss - 0.49235	, testing loss - 0.50568	
222	 steps: training loss - 0.49202	, testing loss - 0.50551	
223	 steps: training loss - 0.49170	, testing loss - 0.50533	
224	 steps: training loss - 0.49138	, testing loss - 0.50516	
225	 steps: training loss - 0.49107	, testing loss - 0.50499	
226	 steps: training loss - 0.49075	, testing loss - 0.50482	
227	 steps: training loss - 0.49044	, testing loss - 0.50466	
228	 steps: training loss - 0.49013	, testing loss - 0.50449	
229	 steps: training loss - 0.48982	, testing loss - 0.50433	
230	 steps: training loss - 0.48952	, testing loss - 0.50417	
231	 steps: training loss - 0.48922	, testing loss - 0.50402	
232	 steps: training loss - 0.48891	, testing loss - 0.50386	
233	 steps: training loss - 0.48861	, testing loss - 0.50371	
234	 steps: training loss - 0.48832	, testing loss - 0.50356	
235	 steps: training loss - 0.48802	, testing loss - 0.50341	
236	 steps: training loss - 0.48773	, testing loss - 0.50326	
237	 steps: training loss - 0.48744	, testing loss - 0.50312	
238	 steps: training loss - 0.48715	, testing loss - 0.50297	
239	 steps: training loss - 0.48686	, testing loss - 0.50283	
240	 steps: training loss - 0.48658	, testing loss - 0.50269	
241	 steps: training loss - 0.48629	, testing loss - 0.50256	
242	 steps: training loss - 0.48601	, testing loss - 0.50242	
243	 steps: training loss - 0.48573	, testing loss - 0.50229	
244	 steps: training loss - 0.48546	, testing loss - 0.50215	
245	 steps: training loss - 0.48518	, testing loss - 0.50202	
246	 steps: training loss - 0.48491	, testing loss - 0.50190	
247	 steps: training loss - 0.48463	, testing loss - 0.50177	
248	 steps: training loss - 0.48436	, testing loss - 0.50164	
249	 steps: training loss - 0.48410	, testing loss - 0.50152	
250	 steps: training loss - 0.48383	, testing loss - 0.50140	
251	 steps: training loss - 0.48356	, testing loss - 0.50128	
252	 steps: training loss - 0.48330	, testing loss - 0.50116	
253	 steps: training loss - 0.48304	, testing loss - 0.50105	
254	 steps: training loss - 0.48278	, testing loss - 0.50093	
255	 steps: training loss - 0.48252	, testing loss - 0.50082	
256	 steps: training loss - 0.48226	, testing loss - 0.50071	
257	 steps: training loss - 0.48201	, testing loss - 0.50060	
258	 steps: training loss - 0.48175	, testing loss - 0.50049	
259	 steps: training loss - 0.48150	, testing loss - 0.50038	
260	 steps: training loss - 0.48125	, testing loss - 0.50028	
261	 steps: training loss - 0.48100	, testing loss - 0.50017	
262	 steps: training loss - 0.48075	, testing loss - 0.50007	
263	 steps: training loss - 0.48051	, testing loss - 0.49997	
264	 steps: training loss - 0.48026	, testing loss - 0.49987	
265	 steps: training loss - 0.48002	, testing loss - 0.49977	
266	 steps: training loss - 0.47978	, testing loss - 0.49968	
267	 steps: training loss - 0.47954	, testing loss - 0.49958	
268	 steps: training loss - 0.47930	, testing loss - 0.49949	
269	 steps: training loss - 0.47906	, testing loss - 0.49939	
270	 steps: training loss - 0.47882	, testing loss - 0.49930	
271	 steps: training loss - 0.47859	, testing loss - 0.49921	
272	 steps: training loss - 0.47835	, testing loss - 0.49912	
273	 steps: training loss - 0.47812	, testing loss - 0.49904	
274	 steps: training loss - 0.47789	, testing loss - 0.49895	
275	 steps: training loss - 0.47766	, testing loss - 0.49887	
276	 steps: training loss - 0.47743	, testing loss - 0.49878	
277	 steps: training loss - 0.47721	, testing loss - 0.49870	
278	 steps: training loss - 0.47698	, testing loss - 0.49862	
279	 steps: training loss - 0.47676	, testing loss - 0.49854	
280	 steps: training loss - 0.47653	, testing loss - 0.49846	
281	 steps: training loss - 0.47631	, testing loss - 0.49839	
282	 steps: training loss - 0.47609	, testing loss - 0.49831	
283	 steps: training loss - 0.47587	, testing loss - 0.49824	
284	 steps: training loss - 0.47565	, testing loss - 0.49816	
285	 steps: training loss - 0.47544	, testing loss - 0.49809	
286	 steps: training loss - 0.47522	, testing loss - 0.49802	
287	 steps: training loss - 0.47501	, testing loss - 0.49795	
288	 steps: training loss - 0.47479	, testing loss - 0.49788	
289	 steps: training loss - 0.47458	, testing loss - 0.49781	
290	 steps: training loss - 0.47437	, testing loss - 0.49774	
291	 steps: training loss - 0.47416	, testing loss - 0.49768	
292	 steps: training loss - 0.47395	, testing loss - 0.49761	
293	 steps: training loss - 0.47374	, testing loss - 0.49755	
294	 steps: training loss - 0.47353	, testing loss - 0.49749	
295	 steps: training loss - 0.47333	, testing loss - 0.49742	
296	 steps: training loss - 0.47312	, testing loss - 0.49736	
297	 steps: training loss - 0.47292	, testing loss - 0.49730	
298	 steps: training loss - 0.47272	, testing loss - 0.49724	
299	 steps: training loss - 0.47252	, testing loss - 0.49719	
300	 steps: training loss - 0.47231	, testing loss - 0.49713	
301	 steps: training loss - 0.47211	, testing loss - 0.49707	
302	 steps: training loss - 0.47192	, testing loss - 0.49702	
303	 steps: training loss - 0.47172	, testing loss - 0.49696	
304	 steps: training loss - 0.47152	, testing loss - 0.49691	
305	 steps: training loss - 0.47133	, testing loss - 0.49686	
306	 steps: training loss - 0.47113	, testing loss - 0.49681	
307	 steps: training loss - 0.47094	, testing loss - 0.49676	
308	 steps: training loss - 0.47074	, testing loss - 0.49671	
309	 steps: training loss - 0.47055	, testing loss - 0.49666	
310	 steps: training loss - 0.47036	, testing loss - 0.49661	
311	 steps: training loss - 0.47017	, testing loss - 0.49656	
312	 steps: training loss - 0.46998	, testing loss - 0.49652	
313	 steps: training loss - 0.46979	, testing loss - 0.49647	
314	 steps: training loss - 0.46961	, testing loss - 0.49643	
315	 steps: training loss - 0.46942	, testing loss - 0.49638	
316	 steps: training loss - 0.46923	, testing loss - 0.49634	
317	 steps: training loss - 0.46905	, testing loss - 0.49630	
318	 steps: training loss - 0.46887	, testing loss - 0.49626	
319	 steps: training loss - 0.46868	, testing loss - 0.49621	
320	 steps: training loss - 0.46850	, testing loss - 0.49617	
321	 steps: training loss - 0.46832	, testing loss - 0.49614	
322	 steps: training loss - 0.46814	, testing loss - 0.49610	
323	 steps: training loss - 0.46796	, testing loss - 0.49606	
324	 steps: training loss - 0.46778	, testing loss - 0.49602	
325	 steps: training loss - 0.46760	, testing loss - 0.49599	
326	 steps: training loss - 0.46742	, testing loss - 0.49595	
327	 steps: training loss - 0.46725	, testing loss - 0.49592	
328	 steps: training loss - 0.46707	, testing loss - 0.49588	
329	 steps: training loss - 0.46690	, testing loss - 0.49585	
330	 steps: training loss - 0.46672	, testing loss - 0.49582	
331	 steps: training loss - 0.46655	, testing loss - 0.49578	
332	 steps: training loss - 0.46638	, testing loss - 0.49575	
333	 steps: training loss - 0.46620	, testing loss - 0.49572	
334	 steps: training loss - 0.46603	, testing loss - 0.49569	
335	 steps: training loss - 0.46586	, testing loss - 0.49566	
336	 steps: training loss - 0.46569	, testing loss - 0.49563	
337	 steps: training loss - 0.46552	, testing loss - 0.49560	
338	 steps: training loss - 0.46536	, testing loss - 0.49558	
339	 steps: training loss - 0.46519	, testing loss - 0.49555	
340	 steps: training loss - 0.46502	, testing loss - 0.49552	
341	 steps: training loss - 0.46485	, testing loss - 0.49550	
342	 steps: training loss - 0.46469	, testing loss - 0.49547	
343	 steps: training loss - 0.46452	, testing loss - 0.49545	
344	 steps: training loss - 0.46436	, testing loss - 0.49542	
345	 steps: training loss - 0.46420	, testing loss - 0.49540	
346	 steps: training loss - 0.46403	, testing loss - 0.49538	
347	 steps: training loss - 0.46387	, testing loss - 0.49535	
348	 steps: training loss - 0.46371	, testing loss - 0.49533	
349	 steps: training loss - 0.46355	, testing loss - 0.49531	
350	 steps: training loss - 0.46339	, testing loss - 0.49529	
351	 steps: training loss - 0.46323	, testing loss - 0.49527	
352	 steps: training loss - 0.46307	, testing loss - 0.49525	
353	 steps: training loss - 0.46291	, testing loss - 0.49523	
354	 steps: training loss - 0.46275	, testing loss - 0.49521	
355	 steps: training loss - 0.46260	, testing loss - 0.49519	
356	 steps: training loss - 0.46244	, testing loss - 0.49517	
357	 steps: training loss - 0.46228	, testing loss - 0.49516	
358	 steps: training loss - 0.46213	, testing loss - 0.49514	
359	 steps: training loss - 0.46197	, testing loss - 0.49512	
360	 steps: training loss - 0.46182	, testing loss - 0.49511	
361	 steps: training loss - 0.46167	, testing loss - 0.49509	
362	 steps: training loss - 0.46151	, testing loss - 0.49508	
363	 steps: training loss - 0.46136	, testing loss - 0.49506	
364	 steps: training loss - 0.46121	, testing loss - 0.49505	
365	 steps: training loss - 0.46106	, testing loss - 0.49504	
366	 steps: training loss - 0.46091	, testing loss - 0.49502	
367	 steps: training loss - 0.46076	, testing loss - 0.49501	
368	 steps: training loss - 0.46061	, testing loss - 0.49500	
369	 steps: training loss - 0.46046	, testing loss - 0.49499	
370	 steps: training loss - 0.46031	, testing loss - 0.49497	
371	 steps: training loss - 0.46016	, testing loss - 0.49496	
372	 steps: training loss - 0.46001	, testing loss - 0.49495	
373	 steps: training loss - 0.45987	, testing loss - 0.49494	
374	 steps: training loss - 0.45972	, testing loss - 0.49493	
375	 steps: training loss - 0.45957	, testing loss - 0.49492	
376	 steps: training loss - 0.45943	, testing loss - 0.49491	
377	 steps: training loss - 0.45928	, testing loss - 0.49490	
378	 steps: training loss - 0.45914	, testing loss - 0.49490	
379	 steps: training loss - 0.45899	, testing loss - 0.49489	
380	 steps: training loss - 0.45885	, testing loss - 0.49488	
381	 steps: training loss - 0.45871	, testing loss - 0.49487	
382	 steps: training loss - 0.45857	, testing loss - 0.49487	
383	 steps: training loss - 0.45842	, testing loss - 0.49486	
384	 steps: training loss - 0.45828	, testing loss - 0.49485	
385	 steps: training loss - 0.45814	, testing loss - 0.49485	
386	 steps: training loss - 0.45800	, testing loss - 0.49484	
387	 steps: training loss - 0.45786	, testing loss - 0.49484	
388	 steps: training loss - 0.45772	, testing loss - 0.49483	
389	 steps: training loss - 0.45758	, testing loss - 0.49483	
390	 steps: training loss - 0.45744	, testing loss - 0.49483	
391	 steps: training loss - 0.45731	, testing loss - 0.49482	
392	 steps: training loss - 0.45717	, testing loss - 0.49482	
393	 steps: training loss - 0.45703	, testing loss - 0.49481	
394	 steps: training loss - 0.45689	, testing loss - 0.49481	
395	 steps: training loss - 0.45676	, testing loss - 0.49481	
396	 steps: training loss - 0.45662	, testing loss - 0.49481	
397	 steps: training loss - 0.45649	, testing loss - 0.49481	
398	 steps: training loss - 0.45635	, testing loss - 0.49480	
399	 steps: training loss - 0.45622	, testing loss - 0.49480	
400	 steps: training loss - 0.45608	, testing loss - 0.49480	
401	 steps: training loss - 0.45595	, testing loss - 0.49480	
402	 steps: training loss - 0.45582	, testing loss - 0.49480	
403	 steps: training loss - 0.45568	, testing loss - 0.49480	
404	 steps: training loss - 0.45555	, testing loss - 0.49480	
405	 steps: training loss - 0.45542	, testing loss - 0.49480	
406	 steps: training loss - 0.45529	, testing loss - 0.49480	
407	 steps: training loss - 0.45516	, testing loss - 0.49480	
408	 steps: training loss - 0.45503	, testing loss - 0.49480	
409	 steps: training loss - 0.45489	, testing loss - 0.49481	
410	 steps: training loss - 0.45476	, testing loss - 0.49481	
411	 steps: training loss - 0.45464	, testing loss - 0.49481	
412	 steps: training loss - 0.45451	, testing loss - 0.49481	
413	 steps: training loss - 0.45438	, testing loss - 0.49481	
414	 steps: training loss - 0.45425	, testing loss - 0.49482	
415	 steps: training loss - 0.45412	, testing loss - 0.49482	
416	 steps: training loss - 0.45399	, testing loss - 0.49482	
417	 steps: training loss - 0.45387	, testing loss - 0.49483	
418	 steps: training loss - 0.45374	, testing loss - 0.49483	
419	 steps: training loss - 0.45361	, testing loss - 0.49483	
420	 steps: training loss - 0.45349	, testing loss - 0.49484	
421	 steps: training loss - 0.45336	, testing loss - 0.49484	
422	 steps: training loss - 0.45324	, testing loss - 0.49485	
423	 steps: training loss - 0.45311	, testing loss - 0.49485	
424	 steps: training loss - 0.45299	, testing loss - 0.49486	
425	 steps: training loss - 0.45286	, testing loss - 0.49486	
426	 steps: training loss - 0.45274	, testing loss - 0.49487	
427	 steps: training loss - 0.45261	, testing loss - 0.49487	
428	 steps: training loss - 0.45249	, testing loss - 0.49488	
429	 steps: training loss - 0.45237	, testing loss - 0.49488	
430	 steps: training loss - 0.45225	, testing loss - 0.49489	
431	 steps: training loss - 0.45212	, testing loss - 0.49490	
432	 steps: training loss - 0.45200	, testing loss - 0.49490	
433	 steps: training loss - 0.45188	, testing loss - 0.49491	
434	 steps: training loss - 0.45176	, testing loss - 0.49492	
435	 steps: training loss - 0.45164	, testing loss - 0.49493	
436	 steps: training loss - 0.45152	, testing loss - 0.49493	
437	 steps: training loss - 0.45140	, testing loss - 0.49494	
438	 steps: training loss - 0.45128	, testing loss - 0.49495	
439	 steps: training loss - 0.45116	, testing loss - 0.49496	
440	 steps: training loss - 0.45104	, testing loss - 0.49496	
441	 steps: training loss - 0.45092	, testing loss - 0.49497	
442	 steps: training loss - 0.45080	, testing loss - 0.49498	
443	 steps: training loss - 0.45069	, testing loss - 0.49499	
444	 steps: training loss - 0.45057	, testing loss - 0.49500	
445	 steps: training loss - 0.45045	, testing loss - 0.49501	
446	 steps: training loss - 0.45033	, testing loss - 0.49502	
447	 steps: training loss - 0.45022	, testing loss - 0.49503	
448	 steps: training loss - 0.45010	, testing loss - 0.49504	
449	 steps: training loss - 0.44998	, testing loss - 0.49505	
450	 steps: training loss - 0.44987	, testing loss - 0.49506	
451	 steps: training loss - 0.44975	, testing loss - 0.49507	
452	 steps: training loss - 0.44964	, testing loss - 0.49508	
453	 steps: training loss - 0.44952	, testing loss - 0.49509	
454	 steps: training loss - 0.44941	, testing loss - 0.49510	
455	 steps: training loss - 0.44929	, testing loss - 0.49511	
456	 steps: training loss - 0.44918	, testing loss - 0.49512	
457	 steps: training loss - 0.44907	, testing loss - 0.49513	
458	 steps: training loss - 0.44895	, testing loss - 0.49514	
459	 steps: training loss - 0.44884	, testing loss - 0.49515	
460	 steps: training loss - 0.44873	, testing loss - 0.49516	
461	 steps: training loss - 0.44862	, testing loss - 0.49517	
462	 steps: training loss - 0.44850	, testing loss - 0.49518	
463	 steps: training loss - 0.44839	, testing loss - 0.49520	
464	 steps: training loss - 0.44828	, testing loss - 0.49521	
465	 steps: training loss - 0.44817	, testing loss - 0.49522	
466	 steps: training loss - 0.44806	, testing loss - 0.49523	
467	 steps: training loss - 0.44795	, testing loss - 0.49524	
468	 steps: training loss - 0.44784	, testing loss - 0.49526	
469	 steps: training loss - 0.44773	, testing loss - 0.49527	
470	 steps: training loss - 0.44762	, testing loss - 0.49528	
471	 steps: training loss - 0.44751	, testing loss - 0.49529	
472	 steps: training loss - 0.44740	, testing loss - 0.49531	
473	 steps: training loss - 0.44729	, testing loss - 0.49532	
474	 steps: training loss - 0.44718	, testing loss - 0.49533	
475	 steps: training loss - 0.44707	, testing loss - 0.49535	
476	 steps: training loss - 0.44696	, testing loss - 0.49536	
477	 steps: training loss - 0.44685	, testing loss - 0.49537	
478	 steps: training loss - 0.44675	, testing loss - 0.49539	
479	 steps: training loss - 0.44664	, testing loss - 0.49540	
480	 steps: training loss - 0.44653	, testing loss - 0.49541	
481	 steps: training loss - 0.44643	, testing loss - 0.49543	
482	 steps: training loss - 0.44632	, testing loss - 0.49544	
483	 steps: training loss - 0.44621	, testing loss - 0.49546	
484	 steps: training loss - 0.44611	, testing loss - 0.49547	
485	 steps: training loss - 0.44600	, testing loss - 0.49548	
486	 steps: training loss - 0.44590	, testing loss - 0.49550	
487	 steps: training loss - 0.44579	, testing loss - 0.49551	
488	 steps: training loss - 0.44568	, testing loss - 0.49553	
489	 steps: training loss - 0.44558	, testing loss - 0.49554	
490	 steps: training loss - 0.44548	, testing loss - 0.49556	
491	 steps: training loss - 0.44537	, testing loss - 0.49557	
492	 steps: training loss - 0.44527	, testing loss - 0.49559	
493	 steps: training loss - 0.44516	, testing loss - 0.49560	
494	 steps: training loss - 0.44506	, testing loss - 0.49562	
495	 steps: training loss - 0.44496	, testing loss - 0.49563	
496	 steps: training loss - 0.44485	, testing loss - 0.49565	
497	 steps: training loss - 0.44475	, testing loss - 0.49566	
498	 steps: training loss - 0.44465	, testing loss - 0.49568	
499	 steps: training loss - 0.44455	, testing loss - 0.49569	
500	 steps: training loss - 0.44444	, testing loss - 0.49571	
501	 steps: training loss - 0.44434	, testing loss - 0.49573	
502	 steps: training loss - 0.44424	, testing loss - 0.49574	
503	 steps: training loss - 0.44414	, testing loss - 0.49576	
504	 steps: training loss - 0.44404	, testing loss - 0.49577	
505	 steps: training loss - 0.44394	, testing loss - 0.49579	
506	 steps: training loss - 0.44384	, testing loss - 0.49581	
507	 steps: training loss - 0.44374	, testing loss - 0.49582	
508	 steps: training loss - 0.44364	, testing loss - 0.49584	
509	 steps: training loss - 0.44354	, testing loss - 0.49586	
510	 steps: training loss - 0.44344	, testing loss - 0.49587	
511	 steps: training loss - 0.44334	, testing loss - 0.49589	
512	 steps: training loss - 0.44324	, testing loss - 0.49591	
513	 steps: training loss - 0.44314	, testing loss - 0.49592	
514	 steps: training loss - 0.44304	, testing loss - 0.49594	
515	 steps: training loss - 0.44294	, testing loss - 0.49596	
516	 steps: training loss - 0.44284	, testing loss - 0.49597	
517	 steps: training loss - 0.44274	, testing loss - 0.49599	
518	 steps: training loss - 0.44265	, testing loss - 0.49601	
519	 steps: training loss - 0.44255	, testing loss - 0.49602	
520	 steps: training loss - 0.44245	, testing loss - 0.49604	
521	 steps: training loss - 0.44235	, testing loss - 0.49606	
522	 steps: training loss - 0.44226	, testing loss - 0.49608	
523	 steps: training loss - 0.44216	, testing loss - 0.49609	
524	 steps: training loss - 0.44206	, testing loss - 0.49611	
525	 steps: training loss - 0.44197	, testing loss - 0.49613	
526	 steps: training loss - 0.44187	, testing loss - 0.49615	
527	 steps: training loss - 0.44178	, testing loss - 0.49616	
528	 steps: training loss - 0.44168	, testing loss - 0.49618	
529	 steps: training loss - 0.44158	, testing loss - 0.49620	
530	 steps: training loss - 0.44149	, testing loss - 0.49622	
531	 steps: training loss - 0.44139	, testing loss - 0.49623	
532	 steps: training loss - 0.44130	, testing loss - 0.49625	
533	 steps: training loss - 0.44120	, testing loss - 0.49627	
534	 steps: training loss - 0.44111	, testing loss - 0.49629	
535	 steps: training loss - 0.44102	, testing loss - 0.49631	
536	 steps: training loss - 0.44092	, testing loss - 0.49633	
537	 steps: training loss - 0.44083	, testing loss - 0.49634	
538	 steps: training loss - 0.44073	, testing loss - 0.49636	
539	 steps: training loss - 0.44064	, testing loss - 0.49638	
540	 steps: training loss - 0.44055	, testing loss - 0.49640	
541	 steps: training loss - 0.44045	, testing loss - 0.49642	
542	 steps: training loss - 0.44036	, testing loss - 0.49644	
543	 steps: training loss - 0.44027	, testing loss - 0.49645	
544	 steps: training loss - 0.44018	, testing loss - 0.49647	
545	 steps: training loss - 0.44008	, testing loss - 0.49649	
546	 steps: training loss - 0.43999	, testing loss - 0.49651	
547	 steps: training loss - 0.43990	, testing loss - 0.49653	
548	 steps: training loss - 0.43981	, testing loss - 0.49655	
549	 steps: training loss - 0.43972	, testing loss - 0.49657	
550	 steps: training loss - 0.43963	, testing loss - 0.49659	
551	 steps: training loss - 0.43954	, testing loss - 0.49660	
552	 steps: training loss - 0.43944	, testing loss - 0.49662	
553	 steps: training loss - 0.43935	, testing loss - 0.49664	
554	 steps: training loss - 0.43926	, testing loss - 0.49666	
555	 steps: training loss - 0.43917	, testing loss - 0.49668	
556	 steps: training loss - 0.43908	, testing loss - 0.49670	
557	 steps: training loss - 0.43899	, testing loss - 0.49672	
558	 steps: training loss - 0.43890	, testing loss - 0.49674	
559	 steps: training loss - 0.43881	, testing loss - 0.49676	
560	 steps: training loss - 0.43872	, testing loss - 0.49678	
561	 steps: training loss - 0.43864	, testing loss - 0.49680	
562	 steps: training loss - 0.43855	, testing loss - 0.49682	
563	 steps: training loss - 0.43846	, testing loss - 0.49684	
564	 steps: training loss - 0.43837	, testing loss - 0.49686	
565	 steps: training loss - 0.43828	, testing loss - 0.49688	
566	 steps: training loss - 0.43819	, testing loss - 0.49689	
567	 steps: training loss - 0.43811	, testing loss - 0.49691	
568	 steps: training loss - 0.43802	, testing loss - 0.49693	
569	 steps: training loss - 0.43793	, testing loss - 0.49695	
570	 steps: training loss - 0.43784	, testing loss - 0.49697	
571	 steps: training loss - 0.43776	, testing loss - 0.49699	
572	 steps: training loss - 0.43767	, testing loss - 0.49701	
573	 steps: training loss - 0.43758	, testing loss - 0.49703	
574	 steps: training loss - 0.43749	, testing loss - 0.49705	
575	 steps: training loss - 0.43741	, testing loss - 0.49707	
576	 steps: training loss - 0.43732	, testing loss - 0.49709	
577	 steps: training loss - 0.43724	, testing loss - 0.49711	
578	 steps: training loss - 0.43715	, testing loss - 0.49713	
579	 steps: training loss - 0.43706	, testing loss - 0.49715	
580	 steps: training loss - 0.43698	, testing loss - 0.49717	
581	 steps: training loss - 0.43689	, testing loss - 0.49719	
582	 steps: training loss - 0.43681	, testing loss - 0.49721	
583	 steps: training loss - 0.43672	, testing loss - 0.49724	
584	 steps: training loss - 0.43664	, testing loss - 0.49726	
585	 steps: training loss - 0.43655	, testing loss - 0.49728	
586	 steps: training loss - 0.43647	, testing loss - 0.49730	
587	 steps: training loss - 0.43638	, testing loss - 0.49732	
588	 steps: training loss - 0.43630	, testing loss - 0.49734	
589	 steps: training loss - 0.43622	, testing loss - 0.49736	
590	 steps: training loss - 0.43613	, testing loss - 0.49738	
591	 steps: training loss - 0.43605	, testing loss - 0.49740	
592	 steps: training loss - 0.43596	, testing loss - 0.49742	
593	 steps: training loss - 0.43588	, testing loss - 0.49744	
594	 steps: training loss - 0.43580	, testing loss - 0.49746	
595	 steps: training loss - 0.43572	, testing loss - 0.49748	
596	 steps: training loss - 0.43563	, testing loss - 0.49750	
597	 steps: training loss - 0.43555	, testing loss - 0.49752	
598	 steps: training loss - 0.43547	, testing loss - 0.49754	
599	 steps: training loss - 0.43538	, testing loss - 0.49756	
600	 steps: training loss - 0.43530	, testing loss - 0.49759	
601	 steps: training loss - 0.43522	, testing loss - 0.49761	
602	 steps: training loss - 0.43514	, testing loss - 0.49763	
603	 steps: training loss - 0.43506	, testing loss - 0.49765	
604	 steps: training loss - 0.43498	, testing loss - 0.49767	
605	 steps: training loss - 0.43489	, testing loss - 0.49769	
606	 steps: training loss - 0.43481	, testing loss - 0.49771	
607	 steps: training loss - 0.43473	, testing loss - 0.49773	
608	 steps: training loss - 0.43465	, testing loss - 0.49775	
609	 steps: training loss - 0.43457	, testing loss - 0.49777	
610	 steps: training loss - 0.43449	, testing loss - 0.49780	
611	 steps: training loss - 0.43441	, testing loss - 0.49782	
612	 steps: training loss - 0.43433	, testing loss - 0.49784	
613	 steps: training loss - 0.43425	, testing loss - 0.49786	
614	 steps: training loss - 0.43417	, testing loss - 0.49788	
615	 steps: training loss - 0.43409	, testing loss - 0.49790	
616	 steps: training loss - 0.43401	, testing loss - 0.49792	
617	 steps: training loss - 0.43393	, testing loss - 0.49794	
618	 steps: training loss - 0.43385	, testing loss - 0.49797	
619	 steps: training loss - 0.43377	, testing loss - 0.49799	
620	 steps: training loss - 0.43369	, testing loss - 0.49801	
621	 steps: training loss - 0.43361	, testing loss - 0.49803	
622	 steps: training loss - 0.43353	, testing loss - 0.49805	
623	 steps: training loss - 0.43346	, testing loss - 0.49807	
624	 steps: training loss - 0.43338	, testing loss - 0.49809	
625	 steps: training loss - 0.43330	, testing loss - 0.49812	
626	 steps: training loss - 0.43322	, testing loss - 0.49814	
627	 steps: training loss - 0.43314	, testing loss - 0.49816	
628	 steps: training loss - 0.43307	, testing loss - 0.49818	
629	 steps: training loss - 0.43299	, testing loss - 0.49820	
630	 steps: training loss - 0.43291	, testing loss - 0.49822	
631	 steps: training loss - 0.43283	, testing loss - 0.49825	
632	 steps: training loss - 0.43276	, testing loss - 0.49827	
633	 steps: training loss - 0.43268	, testing loss - 0.49829	
634	 steps: training loss - 0.43260	, testing loss - 0.49831	
635	 steps: training loss - 0.43253	, testing loss - 0.49833	
636	 steps: training loss - 0.43245	, testing loss - 0.49835	
637	 steps: training loss - 0.43237	, testing loss - 0.49838	
638	 steps: training loss - 0.43230	, testing loss - 0.49840	
639	 steps: training loss - 0.43222	, testing loss - 0.49842	
640	 steps: training loss - 0.43214	, testing loss - 0.49844	
641	 steps: training loss - 0.43207	, testing loss - 0.49846	
642	 steps: training loss - 0.43199	, testing loss - 0.49849	
643	 steps: training loss - 0.43192	, testing loss - 0.49851	
644	 steps: training loss - 0.43184	, testing loss - 0.49853	
645	 steps: training loss - 0.43177	, testing loss - 0.49855	
646	 steps: training loss - 0.43169	, testing loss - 0.49857	
647	 steps: training loss - 0.43162	, testing loss - 0.49860	
648	 steps: training loss - 0.43154	, testing loss - 0.49862	
649	 steps: training loss - 0.43147	, testing loss - 0.49864	
650	 steps: training loss - 0.43139	, testing loss - 0.49866	
651	 steps: training loss - 0.43132	, testing loss - 0.49868	
652	 steps: training loss - 0.43124	, testing loss - 0.49871	
653	 steps: training loss - 0.43117	, testing loss - 0.49873	
654	 steps: training loss - 0.43110	, testing loss - 0.49875	
655	 steps: training loss - 0.43102	, testing loss - 0.49877	
656	 steps: training loss - 0.43095	, testing loss - 0.49880	
657	 steps: training loss - 0.43087	, testing loss - 0.49882	
658	 steps: training loss - 0.43080	, testing loss - 0.49884	
659	 steps: training loss - 0.43073	, testing loss - 0.49886	
660	 steps: training loss - 0.43065	, testing loss - 0.49888	
661	 steps: training loss - 0.43058	, testing loss - 0.49891	
662	 steps: training loss - 0.43051	, testing loss - 0.49893	
663	 steps: training loss - 0.43044	, testing loss - 0.49895	
664	 steps: training loss - 0.43036	, testing loss - 0.49897	
665	 steps: training loss - 0.43029	, testing loss - 0.49900	
666	 steps: training loss - 0.43022	, testing loss - 0.49902	
667	 steps: training loss - 0.43015	, testing loss - 0.49904	
668	 steps: training loss - 0.43007	, testing loss - 0.49906	
669	 steps: training loss - 0.43000	, testing loss - 0.49909	
670	 steps: training loss - 0.42993	, testing loss - 0.49911	
671	 steps: training loss - 0.42986	, testing loss - 0.49913	
672	 steps: training loss - 0.42979	, testing loss - 0.49915	
673	 steps: training loss - 0.42972	, testing loss - 0.49918	
674	 steps: training loss - 0.42964	, testing loss - 0.49920	
675	 steps: training loss - 0.42957	, testing loss - 0.49922	
676	 steps: training loss - 0.42950	, testing loss - 0.49924	
677	 steps: training loss - 0.42943	, testing loss - 0.49927	
678	 steps: training loss - 0.42936	, testing loss - 0.49929	
679	 steps: training loss - 0.42929	, testing loss - 0.49931	
680	 steps: training loss - 0.42922	, testing loss - 0.49934	
681	 steps: training loss - 0.42915	, testing loss - 0.49936	
682	 steps: training loss - 0.42908	, testing loss - 0.49938	
683	 steps: training loss - 0.42901	, testing loss - 0.49940	
684	 steps: training loss - 0.42894	, testing loss - 0.49943	
685	 steps: training loss - 0.42887	, testing loss - 0.49945	
686	 steps: training loss - 0.42880	, testing loss - 0.49947	
687	 steps: training loss - 0.42873	, testing loss - 0.49949	
688	 steps: training loss - 0.42866	, testing loss - 0.49952	
689	 steps: training loss - 0.42859	, testing loss - 0.49954	
690	 steps: training loss - 0.42852	, testing loss - 0.49956	
691	 steps: training loss - 0.42845	, testing loss - 0.49959	
692	 steps: training loss - 0.42838	, testing loss - 0.49961	
693	 steps: training loss - 0.42831	, testing loss - 0.49963	
694	 steps: training loss - 0.42825	, testing loss - 0.49965	
695	 steps: training loss - 0.42818	, testing loss - 0.49968	
696	 steps: training loss - 0.42811	, testing loss - 0.49970	
697	 steps: training loss - 0.42804	, testing loss - 0.49972	
698	 steps: training loss - 0.42797	, testing loss - 0.49975	
699	 steps: training loss - 0.42790	, testing loss - 0.49977	
700	 steps: training loss - 0.42784	, testing loss - 0.49979	
701	 steps: training loss - 0.42777	, testing loss - 0.49982	
702	 steps: training loss - 0.42770	, testing loss - 0.49984	
703	 steps: training loss - 0.42763	, testing loss - 0.49986	
704	 steps: training loss - 0.42757	, testing loss - 0.49988	
705	 steps: training loss - 0.42750	, testing loss - 0.49991	
706	 steps: training loss - 0.42743	, testing loss - 0.49993	
707	 steps: training loss - 0.42736	, testing loss - 0.49995	
708	 steps: training loss - 0.42730	, testing loss - 0.49998	
709	 steps: training loss - 0.42723	, testing loss - 0.50000	
710	 steps: training loss - 0.42716	, testing loss - 0.50002	
711	 steps: training loss - 0.42710	, testing loss - 0.50005	
712	 steps: training loss - 0.42703	, testing loss - 0.50007	
713	 steps: training loss - 0.42696	, testing loss - 0.50009	
714	 steps: training loss - 0.42690	, testing loss - 0.50012	
715	 steps: training loss - 0.42683	, testing loss - 0.50014	
716	 steps: training loss - 0.42677	, testing loss - 0.50016	
717	 steps: training loss - 0.42670	, testing loss - 0.50019	
718	 steps: training loss - 0.42663	, testing loss - 0.50021	
719	 steps: training loss - 0.42657	, testing loss - 0.50023	
720	 steps: training loss - 0.42650	, testing loss - 0.50026	
721	 steps: training loss - 0.42644	, testing loss - 0.50028	
722	 steps: training loss - 0.42637	, testing loss - 0.50030	
723	 steps: training loss - 0.42631	, testing loss - 0.50033	
724	 steps: training loss - 0.42624	, testing loss - 0.50035	
725	 steps: training loss - 0.42618	, testing loss - 0.50037	
726	 steps: training loss - 0.42611	, testing loss - 0.50040	
727	 steps: training loss - 0.42605	, testing loss - 0.50042	
728	 steps: training loss - 0.42598	, testing loss - 0.50044	
729	 steps: training loss - 0.42592	, testing loss - 0.50047	
730	 steps: training loss - 0.42585	, testing loss - 0.50049	
731	 steps: training loss - 0.42579	, testing loss - 0.50051	
732	 steps: training loss - 0.42573	, testing loss - 0.50054	
733	 steps: training loss - 0.42566	, testing loss - 0.50056	
734	 steps: training loss - 0.42560	, testing loss - 0.50059	
735	 steps: training loss - 0.42553	, testing loss - 0.50061	
736	 steps: training loss - 0.42547	, testing loss - 0.50063	
737	 steps: training loss - 0.42541	, testing loss - 0.50066	
738	 steps: training loss - 0.42534	, testing loss - 0.50068	
739	 steps: training loss - 0.42528	, testing loss - 0.50070	
740	 steps: training loss - 0.42522	, testing loss - 0.50073	
741	 steps: training loss - 0.42515	, testing loss - 0.50075	
742	 steps: training loss - 0.42509	, testing loss - 0.50077	
743	 steps: training loss - 0.42503	, testing loss - 0.50080	
744	 steps: training loss - 0.42496	, testing loss - 0.50082	
745	 steps: training loss - 0.42490	, testing loss - 0.50085	
746	 steps: training loss - 0.42484	, testing loss - 0.50087	
747	 steps: training loss - 0.42478	, testing loss - 0.50089	
748	 steps: training loss - 0.42471	, testing loss - 0.50092	
749	 steps: training loss - 0.42465	, testing loss - 0.50094	
750	 steps: training loss - 0.42459	, testing loss - 0.50096	
751	 steps: training loss - 0.42453	, testing loss - 0.50099	
752	 steps: training loss - 0.42447	, testing loss - 0.50101	
753	 steps: training loss - 0.42440	, testing loss - 0.50104	
754	 steps: training loss - 0.42434	, testing loss - 0.50106	
755	 steps: training loss - 0.42428	, testing loss - 0.50108	
756	 steps: training loss - 0.42422	, testing loss - 0.50111	
757	 steps: training loss - 0.42416	, testing loss - 0.50113	
758	 steps: training loss - 0.42410	, testing loss - 0.50115	
759	 steps: training loss - 0.42403	, testing loss - 0.50118	
760	 steps: training loss - 0.42397	, testing loss - 0.50120	
761	 steps: training loss - 0.42391	, testing loss - 0.50123	
762	 steps: training loss - 0.42385	, testing loss - 0.50125	
763	 steps: training loss - 0.42379	, testing loss - 0.50127	
764	 steps: training loss - 0.42373	, testing loss - 0.50130	
765	 steps: training loss - 0.42367	, testing loss - 0.50132	
766	 steps: training loss - 0.42361	, testing loss - 0.50135	
767	 steps: training loss - 0.42355	, testing loss - 0.50137	
768	 steps: training loss - 0.42349	, testing loss - 0.50139	
769	 steps: training loss - 0.42343	, testing loss - 0.50142	
770	 steps: training loss - 0.42337	, testing loss - 0.50144	
771	 steps: training loss - 0.42331	, testing loss - 0.50147	
772	 steps: training loss - 0.42325	, testing loss - 0.50149	
773	 steps: training loss - 0.42319	, testing loss - 0.50151	
774	 steps: training loss - 0.42313	, testing loss - 0.50154	
775	 steps: training loss - 0.42307	, testing loss - 0.50156	
776	 steps: training loss - 0.42301	, testing loss - 0.50159	
777	 steps: training loss - 0.42295	, testing loss - 0.50161	
778	 steps: training loss - 0.42289	, testing loss - 0.50164	
779	 steps: training loss - 0.42283	, testing loss - 0.50166	
780	 steps: training loss - 0.42277	, testing loss - 0.50168	
781	 steps: training loss - 0.42271	, testing loss - 0.50171	
782	 steps: training loss - 0.42266	, testing loss - 0.50173	
783	 steps: training loss - 0.42260	, testing loss - 0.50176	
784	 steps: training loss - 0.42254	, testing loss - 0.50178	
785	 steps: training loss - 0.42248	, testing loss - 0.50180	
786	 steps: training loss - 0.42242	, testing loss - 0.50183	
787	 steps: training loss - 0.42236	, testing loss - 0.50185	
788	 steps: training loss - 0.42230	, testing loss - 0.50188	
789	 steps: training loss - 0.42225	, testing loss - 0.50190	
790	 steps: training loss - 0.42219	, testing loss - 0.50193	
791	 steps: training loss - 0.42213	, testing loss - 0.50195	
792	 steps: training loss - 0.42207	, testing loss - 0.50197	
793	 steps: training loss - 0.42201	, testing loss - 0.50200	
794	 steps: training loss - 0.42196	, testing loss - 0.50202	
795	 steps: training loss - 0.42190	, testing loss - 0.50205	
796	 steps: training loss - 0.42184	, testing loss - 0.50207	
797	 steps: training loss - 0.42178	, testing loss - 0.50210	
798	 steps: training loss - 0.42173	, testing loss - 0.50212	
799	 steps: training loss - 0.42167	, testing loss - 0.50215	
800	 steps: training loss - 0.42161	, testing loss - 0.50217	
801	 steps: training loss - 0.42156	, testing loss - 0.50219	
802	 steps: training loss - 0.42150	, testing loss - 0.50222	
803	 steps: training loss - 0.42144	, testing loss - 0.50224	
804	 steps: training loss - 0.42139	, testing loss - 0.50227	
805	 steps: training loss - 0.42133	, testing loss - 0.50229	
806	 steps: training loss - 0.42127	, testing loss - 0.50232	
807	 steps: training loss - 0.42122	, testing loss - 0.50234	
808	 steps: training loss - 0.42116	, testing loss - 0.50237	
809	 steps: training loss - 0.42110	, testing loss - 0.50239	
810	 steps: training loss - 0.42105	, testing loss - 0.50241	
811	 steps: training loss - 0.42099	, testing loss - 0.50244	
812	 steps: training loss - 0.42093	, testing loss - 0.50246	
813	 steps: training loss - 0.42088	, testing loss - 0.50249	
814	 steps: training loss - 0.42082	, testing loss - 0.50251	
815	 steps: training loss - 0.42077	, testing loss - 0.50254	
816	 steps: training loss - 0.42071	, testing loss - 0.50256	
817	 steps: training loss - 0.42066	, testing loss - 0.50259	
818	 steps: training loss - 0.42060	, testing loss - 0.50261	
819	 steps: training loss - 0.42055	, testing loss - 0.50264	
820	 steps: training loss - 0.42049	, testing loss - 0.50266	
821	 steps: training loss - 0.42043	, testing loss - 0.50269	
822	 steps: training loss - 0.42038	, testing loss - 0.50271	
823	 steps: training loss - 0.42032	, testing loss - 0.50273	
824	 steps: training loss - 0.42027	, testing loss - 0.50276	
825	 steps: training loss - 0.42021	, testing loss - 0.50278	
826	 steps: training loss - 0.42016	, testing loss - 0.50281	
827	 steps: training loss - 0.42011	, testing loss - 0.50283	
828	 steps: training loss - 0.42005	, testing loss - 0.50286	
829	 steps: training loss - 0.42000	, testing loss - 0.50288	
830	 steps: training loss - 0.41994	, testing loss - 0.50291	
831	 steps: training loss - 0.41989	, testing loss - 0.50293	
832	 steps: training loss - 0.41983	, testing loss - 0.50296	
833	 steps: training loss - 0.41978	, testing loss - 0.50298	
834	 steps: training loss - 0.41973	, testing loss - 0.50301	
835	 steps: training loss - 0.41967	, testing loss - 0.50303	
836	 steps: training loss - 0.41962	, testing loss - 0.50306	
837	 steps: training loss - 0.41956	, testing loss - 0.50308	
838	 steps: training loss - 0.41951	, testing loss - 0.50311	
839	 steps: training loss - 0.41946	, testing loss - 0.50313	
840	 steps: training loss - 0.41940	, testing loss - 0.50316	
841	 steps: training loss - 0.41935	, testing loss - 0.50318	
842	 steps: training loss - 0.41930	, testing loss - 0.50321	
843	 steps: training loss - 0.41924	, testing loss - 0.50323	
844	 steps: training loss - 0.41919	, testing loss - 0.50326	
845	 steps: training loss - 0.41914	, testing loss - 0.50328	
846	 steps: training loss - 0.41908	, testing loss - 0.50331	
847	 steps: training loss - 0.41903	, testing loss - 0.50333	
848	 steps: training loss - 0.41898	, testing loss - 0.50336	
849	 steps: training loss - 0.41893	, testing loss - 0.50338	
850	 steps: training loss - 0.41887	, testing loss - 0.50341	
851	 steps: training loss - 0.41882	, testing loss - 0.50343	
852	 steps: training loss - 0.41877	, testing loss - 0.50346	
853	 steps: training loss - 0.41872	, testing loss - 0.50348	
854	 steps: training loss - 0.41866	, testing loss - 0.50351	
855	 steps: training loss - 0.41861	, testing loss - 0.50353	
856	 steps: training loss - 0.41856	, testing loss - 0.50356	
857	 steps: training loss - 0.41851	, testing loss - 0.50358	
858	 steps: training loss - 0.41845	, testing loss - 0.50361	
859	 steps: training loss - 0.41840	, testing loss - 0.50363	
860	 steps: training loss - 0.41835	, testing loss - 0.50366	
861	 steps: training loss - 0.41830	, testing loss - 0.50368	
862	 steps: training loss - 0.41825	, testing loss - 0.50371	
863	 steps: training loss - 0.41820	, testing loss - 0.50373	
864	 steps: training loss - 0.41814	, testing loss - 0.50376	
865	 steps: training loss - 0.41809	, testing loss - 0.50378	
866	 steps: training loss - 0.41804	, testing loss - 0.50381	
867	 steps: training loss - 0.41799	, testing loss - 0.50383	
868	 steps: training loss - 0.41794	, testing loss - 0.50386	
869	 steps: training loss - 0.41789	, testing loss - 0.50388	
870	 steps: training loss - 0.41784	, testing loss - 0.50391	
871	 steps: training loss - 0.41779	, testing loss - 0.50393	
872	 steps: training loss - 0.41773	, testing loss - 0.50396	
873	 steps: training loss - 0.41768	, testing loss - 0.50398	
874	 steps: training loss - 0.41763	, testing loss - 0.50401	
875	 steps: training loss - 0.41758	, testing loss - 0.50403	
876	 steps: training loss - 0.41753	, testing loss - 0.50406	
877	 steps: training loss - 0.41748	, testing loss - 0.50408	
878	 steps: training loss - 0.41743	, testing loss - 0.50411	
879	 steps: training loss - 0.41738	, testing loss - 0.50413	
880	 steps: training loss - 0.41733	, testing loss - 0.50416	
881	 steps: training loss - 0.41728	, testing loss - 0.50418	
882	 steps: training loss - 0.41723	, testing loss - 0.50421	
883	 steps: training loss - 0.41718	, testing loss - 0.50423	
884	 steps: training loss - 0.41713	, testing loss - 0.50426	
885	 steps: training loss - 0.41708	, testing loss - 0.50429	
886	 steps: training loss - 0.41703	, testing loss - 0.50431	
887	 steps: training loss - 0.41698	, testing loss - 0.50434	
888	 steps: training loss - 0.41693	, testing loss - 0.50436	
889	 steps: training loss - 0.41688	, testing loss - 0.50439	
890	 steps: training loss - 0.41683	, testing loss - 0.50441	
891	 steps: training loss - 0.41678	, testing loss - 0.50444	
892	 steps: training loss - 0.41673	, testing loss - 0.50446	
893	 steps: training loss - 0.41668	, testing loss - 0.50449	
894	 steps: training loss - 0.41664	, testing loss - 0.50451	
895	 steps: training loss - 0.41659	, testing loss - 0.50454	
896	 steps: training loss - 0.41654	, testing loss - 0.50456	
897	 steps: training loss - 0.41649	, testing loss - 0.50459	
898	 steps: training loss - 0.41644	, testing loss - 0.50461	
899	 steps: training loss - 0.41639	, testing loss - 0.50464	
900	 steps: training loss - 0.41634	, testing loss - 0.50467	
901	 steps: training loss - 0.41629	, testing loss - 0.50469	
902	 steps: training loss - 0.41624	, testing loss - 0.50472	
903	 steps: training loss - 0.41620	, testing loss - 0.50474	
904	 steps: training loss - 0.41615	, testing loss - 0.50477	
905	 steps: training loss - 0.41610	, testing loss - 0.50479	
906	 steps: training loss - 0.41605	, testing loss - 0.50482	
907	 steps: training loss - 0.41600	, testing loss - 0.50484	
908	 steps: training loss - 0.41595	, testing loss - 0.50487	
909	 steps: training loss - 0.41591	, testing loss - 0.50489	
910	 steps: training loss - 0.41586	, testing loss - 0.50492	
911	 steps: training loss - 0.41581	, testing loss - 0.50495	
912	 steps: training loss - 0.41576	, testing loss - 0.50497	
913	 steps: training loss - 0.41572	, testing loss - 0.50500	
914	 steps: training loss - 0.41567	, testing loss - 0.50502	
915	 steps: training loss - 0.41562	, testing loss - 0.50505	
916	 steps: training loss - 0.41557	, testing loss - 0.50507	
917	 steps: training loss - 0.41552	, testing loss - 0.50510	
918	 steps: training loss - 0.41548	, testing loss - 0.50512	
919	 steps: training loss - 0.41543	, testing loss - 0.50515	
920	 steps: training loss - 0.41538	, testing loss - 0.50518	
921	 steps: training loss - 0.41534	, testing loss - 0.50520	
922	 steps: training loss - 0.41529	, testing loss - 0.50523	
923	 steps: training loss - 0.41524	, testing loss - 0.50525	
924	 steps: training loss - 0.41519	, testing loss - 0.50528	
925	 steps: training loss - 0.41515	, testing loss - 0.50530	
926	 steps: training loss - 0.41510	, testing loss - 0.50533	
927	 steps: training loss - 0.41505	, testing loss - 0.50535	
928	 steps: training loss - 0.41501	, testing loss - 0.50538	
929	 steps: training loss - 0.41496	, testing loss - 0.50541	
930	 steps: training loss - 0.41491	, testing loss - 0.50543	
931	 steps: training loss - 0.41487	, testing loss - 0.50546	
932	 steps: training loss - 0.41482	, testing loss - 0.50548	
933	 steps: training loss - 0.41477	, testing loss - 0.50551	
934	 steps: training loss - 0.41473	, testing loss - 0.50553	
935	 steps: training loss - 0.41468	, testing loss - 0.50556	
936	 steps: training loss - 0.41464	, testing loss - 0.50559	
937	 steps: training loss - 0.41459	, testing loss - 0.50561	
938	 steps: training loss - 0.41454	, testing loss - 0.50564	
939	 steps: training loss - 0.41450	, testing loss - 0.50566	
940	 steps: training loss - 0.41445	, testing loss - 0.50569	
941	 steps: training loss - 0.41441	, testing loss - 0.50571	
942	 steps: training loss - 0.41436	, testing loss - 0.50574	
943	 steps: training loss - 0.41431	, testing loss - 0.50577	
944	 steps: training loss - 0.41427	, testing loss - 0.50579	
945	 steps: training loss - 0.41422	, testing loss - 0.50582	
946	 steps: training loss - 0.41418	, testing loss - 0.50584	
947	 steps: training loss - 0.41413	, testing loss - 0.50587	
948	 steps: training loss - 0.41409	, testing loss - 0.50589	
949	 steps: training loss - 0.41404	, testing loss - 0.50592	
950	 steps: training loss - 0.41400	, testing loss - 0.50595	
951	 steps: training loss - 0.41395	, testing loss - 0.50597	
952	 steps: training loss - 0.41391	, testing loss - 0.50600	
953	 steps: training loss - 0.41386	, testing loss - 0.50602	
954	 steps: training loss - 0.41382	, testing loss - 0.50605	
955	 steps: training loss - 0.41377	, testing loss - 0.50608	
956	 steps: training loss - 0.41373	, testing loss - 0.50610	
957	 steps: training loss - 0.41368	, testing loss - 0.50613	
958	 steps: training loss - 0.41364	, testing loss - 0.50615	
959	 steps: training loss - 0.41359	, testing loss - 0.50618	
960	 steps: training loss - 0.41355	, testing loss - 0.50620	
961	 steps: training loss - 0.41350	, testing loss - 0.50623	
962	 steps: training loss - 0.41346	, testing loss - 0.50626	
963	 steps: training loss - 0.41341	, testing loss - 0.50628	
964	 steps: training loss - 0.41337	, testing loss - 0.50631	
965	 steps: training loss - 0.41333	, testing loss - 0.50633	
966	 steps: training loss - 0.41328	, testing loss - 0.50636	
967	 steps: training loss - 0.41324	, testing loss - 0.50639	
968	 steps: training loss - 0.41319	, testing loss - 0.50641	
969	 steps: training loss - 0.41315	, testing loss - 0.50644	
970	 steps: training loss - 0.41310	, testing loss - 0.50646	
971	 steps: training loss - 0.41306	, testing loss - 0.50649	
972	 steps: training loss - 0.41302	, testing loss - 0.50651	
973	 steps: training loss - 0.41297	, testing loss - 0.50654	
974	 steps: training loss - 0.41293	, testing loss - 0.50657	
975	 steps: training loss - 0.41289	, testing loss - 0.50659	
976	 steps: training loss - 0.41284	, testing loss - 0.50662	
977	 steps: training loss - 0.41280	, testing loss - 0.50664	
978	 steps: training loss - 0.41275	, testing loss - 0.50667	
979	 steps: training loss - 0.41271	, testing loss - 0.50670	
980	 steps: training loss - 0.41267	, testing loss - 0.50672	
981	 steps: training loss - 0.41262	, testing loss - 0.50675	
982	 steps: training loss - 0.41258	, testing loss - 0.50677	
983	 steps: training loss - 0.41254	, testing loss - 0.50680	
984	 steps: training loss - 0.41250	, testing loss - 0.50683	
985	 steps: training loss - 0.41245	, testing loss - 0.50685	
986	 steps: training loss - 0.41241	, testing loss - 0.50688	
987	 steps: training loss - 0.41237	, testing loss - 0.50690	
988	 steps: training loss - 0.41232	, testing loss - 0.50693	
989	 steps: training loss - 0.41228	, testing loss - 0.50696	
990	 steps: training loss - 0.41224	, testing loss - 0.50698	
991	 steps: training loss - 0.41220	, testing loss - 0.50701	
992	 steps: training loss - 0.41215	, testing loss - 0.50703	
993	 steps: training loss - 0.41211	, testing loss - 0.50706	
994	 steps: training loss - 0.41207	, testing loss - 0.50709	
995	 steps: training loss - 0.41202	, testing loss - 0.50711	
996	 steps: training loss - 0.41198	, testing loss - 0.50714	
997	 steps: training loss - 0.41194	, testing loss - 0.50716	
998	 steps: training loss - 0.41190	, testing loss - 0.50719	
999	 steps: training loss - 0.41186	, testing loss - 0.50721	
EVALUATION
----------
Test loss: 0.40871
MIMO Accuracies: 0.97331
